{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##all coding is runing on env theano_3\n",
    "# set plots inline for ipython\n",
    "# %matplotlib inline\n",
    "\n",
    "# import system & ipython utils\n",
    "import os\n",
    "import subprocess\n",
    "from IPython.display import Image\n",
    "import time\n",
    "\n",
    "# numpy and visualization libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import data preprocessing, cross validaiton\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# performance measures\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss, hamming_loss, mean_squared_error\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# # personal libs\n",
    "# from mwutil.ml import run_training_range\n",
    "# from mwutil.ml import compare_classifiers\n",
    "\n",
    "# set random seed\n",
    "rand_state = np.random.RandomState(32)\n",
    "# set the scoring function\n",
    "# set the scoring function\n",
    "scoring_function_label = 'f1_weighted'\n",
    "scoring_function = f1_score\n",
    "\n",
    "# set number of folds for cross validation\n",
    "n_folds = 3\n",
    "\n",
    "# init an empty dict to hold all models for final analysis & comparison\n",
    "classifiers = {}\n",
    "\n",
    "# define the number of 'stages' aka bins to map the UPDRS scores\n",
    "n_stages = 55\n",
    "\n",
    "# set updrs metric (total or motor)\n",
    "#updrs_measure = 'total_UPDRS'\n",
    "\n",
    "# subsampling (-1 for no subsampling)\n",
    "subsample_n = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "## Plotting libraries\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "## Sklearn Libraries\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, confusion_matrix, roc_curve, auc, \\\n",
    "            classification_report, recall_score, precision_recall_curve, roc_auc_score, precision_score, accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import get_scorer\n",
    "\n",
    "## XGBoost Librarires\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# pickle library\n",
    "import pickle\n",
    "\n",
    "## Scipy Libraries\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy.stats import f\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import yeojohnson\n",
    "\n",
    "#statistics\n",
    "from statistics import stdev \n",
    "\n",
    "#itertools\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "#mlxtend\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "#math\n",
    "import math\n",
    "\n",
    "# Define random state\n",
    "random_state = 2020\n",
    "np.random.seed(random_state)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (soft): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMLP(\\n  (linear): Linear(in_features=3, out_features=10, bias=True)\\n)\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear = nn.Linear(3, 2)\n",
    "        self.soft = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        out = self.soft(self.linear(x))\n",
    "        # out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = MLP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "'''\n",
    "MLP(\n",
    "  (linear): Linear(in_features=3, out_features=10, bias=True)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, data,y):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind]\n",
    "        y = self.y[ind]\n",
    "        return x, y\n",
    "class TestDataset(TrainDataset):\n",
    "    def __getitem__(self, ind):\n",
    "        x = self.data[ind]\n",
    "        return x\n",
    "\n",
    "train_set = TrainDataset(X_train.values, y_train.to_numpy())\n",
    "test_set  = TestDataset(X_test.values, y_test.to_numpy())\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_set,  shuffle=True, batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_set,  shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 | Batch 0 | Loss   0.60\n",
      "\tEpoch 0 | Batch 0 | Acc   0.77\n",
      "Epoch 0 | Loss   0.60\n",
      "\tEpoch 1 | Batch 0 | Loss   0.62\n",
      "\tEpoch 1 | Batch 0 | Acc   0.75\n",
      "Epoch 1 | Loss   0.59\n",
      "\tEpoch 2 | Batch 0 | Loss   0.55\n",
      "\tEpoch 2 | Batch 0 | Acc   0.83\n",
      "Epoch 2 | Loss   0.59\n",
      "\tEpoch 3 | Batch 0 | Loss   0.54\n",
      "\tEpoch 3 | Batch 0 | Acc   0.88\n",
      "Epoch 3 | Loss   0.58\n",
      "\tEpoch 4 | Batch 0 | Loss   0.57\n",
      "\tEpoch 4 | Batch 0 | Acc   0.86\n",
      "Epoch 4 | Loss   0.58\n",
      "\tEpoch 5 | Batch 0 | Loss   0.57\n",
      "\tEpoch 5 | Batch 0 | Acc   0.83\n",
      "Epoch 5 | Loss   0.58\n",
      "\tEpoch 6 | Batch 0 | Loss   0.59\n",
      "\tEpoch 6 | Batch 0 | Acc   0.80\n",
      "Epoch 6 | Loss   0.57\n",
      "\tEpoch 7 | Batch 0 | Loss   0.60\n",
      "\tEpoch 7 | Batch 0 | Acc   0.80\n",
      "Epoch 7 | Loss   0.56\n",
      "\tEpoch 8 | Batch 0 | Loss   0.58\n",
      "\tEpoch 8 | Batch 0 | Acc   0.80\n",
      "Epoch 8 | Loss   0.57\n",
      "\tEpoch 9 | Batch 0 | Loss   0.52\n",
      "\tEpoch 9 | Batch 0 | Acc   0.89\n",
      "Epoch 9 | Loss   0.58\n",
      "\tEpoch 10 | Batch 0 | Loss   0.58\n",
      "\tEpoch 10 | Batch 0 | Acc   0.81\n",
      "Epoch 10 | Loss   0.57\n",
      "\tEpoch 11 | Batch 0 | Loss   0.59\n",
      "\tEpoch 11 | Batch 0 | Acc   0.78\n",
      "Epoch 11 | Loss   0.57\n",
      "\tEpoch 12 | Batch 0 | Loss   0.51\n",
      "\tEpoch 12 | Batch 0 | Acc   0.86\n",
      "Epoch 12 | Loss   0.56\n",
      "\tEpoch 13 | Batch 0 | Loss   0.59\n",
      "\tEpoch 13 | Batch 0 | Acc   0.75\n",
      "Epoch 13 | Loss   0.57\n",
      "\tEpoch 14 | Batch 0 | Loss   0.60\n",
      "\tEpoch 14 | Batch 0 | Acc   0.73\n",
      "Epoch 14 | Loss   0.55\n",
      "\tEpoch 15 | Batch 0 | Loss   0.54\n",
      "\tEpoch 15 | Batch 0 | Acc   0.83\n",
      "Epoch 15 | Loss   0.56\n",
      "\tEpoch 16 | Batch 0 | Loss   0.52\n",
      "\tEpoch 16 | Batch 0 | Acc   0.88\n",
      "Epoch 16 | Loss   0.55\n",
      "\tEpoch 17 | Batch 0 | Loss   0.54\n",
      "\tEpoch 17 | Batch 0 | Acc   0.81\n",
      "Epoch 17 | Loss   0.55\n",
      "\tEpoch 18 | Batch 0 | Loss   0.57\n",
      "\tEpoch 18 | Batch 0 | Acc   0.80\n",
      "Epoch 18 | Loss   0.56\n",
      "\tEpoch 19 | Batch 0 | Loss   0.55\n",
      "\tEpoch 19 | Batch 0 | Acc   0.80\n",
      "Epoch 19 | Loss   0.56\n",
      "\tEpoch 20 | Batch 0 | Loss   0.61\n",
      "\tEpoch 20 | Batch 0 | Acc   0.69\n",
      "Epoch 20 | Loss   0.54\n",
      "\tEpoch 21 | Batch 0 | Loss   0.51\n",
      "\tEpoch 21 | Batch 0 | Acc   0.88\n",
      "Epoch 21 | Loss   0.54\n",
      "\tEpoch 22 | Batch 0 | Loss   0.52\n",
      "\tEpoch 22 | Batch 0 | Acc   0.84\n",
      "Epoch 22 | Loss   0.55\n",
      "\tEpoch 23 | Batch 0 | Loss   0.52\n",
      "\tEpoch 23 | Batch 0 | Acc   0.84\n",
      "Epoch 23 | Loss   0.54\n",
      "\tEpoch 24 | Batch 0 | Loss   0.51\n",
      "\tEpoch 24 | Batch 0 | Acc   0.84\n",
      "Epoch 24 | Loss   0.54\n",
      "\tEpoch 25 | Batch 0 | Loss   0.54\n",
      "\tEpoch 25 | Batch 0 | Acc   0.81\n",
      "Epoch 25 | Loss   0.54\n",
      "\tEpoch 26 | Batch 0 | Loss   0.53\n",
      "\tEpoch 26 | Batch 0 | Acc   0.84\n",
      "Epoch 26 | Loss   0.54\n",
      "\tEpoch 27 | Batch 0 | Loss   0.54\n",
      "\tEpoch 27 | Batch 0 | Acc   0.83\n",
      "Epoch 27 | Loss   0.52\n",
      "\tEpoch 28 | Batch 0 | Loss   0.52\n",
      "\tEpoch 28 | Batch 0 | Acc   0.88\n",
      "Epoch 28 | Loss   0.52\n",
      "\tEpoch 29 | Batch 0 | Loss   0.53\n",
      "\tEpoch 29 | Batch 0 | Acc   0.81\n",
      "Epoch 29 | Loss   0.53\n",
      "\tEpoch 30 | Batch 0 | Loss   0.53\n",
      "\tEpoch 30 | Batch 0 | Acc   0.78\n",
      "Epoch 30 | Loss   0.52\n",
      "\tEpoch 31 | Batch 0 | Loss   0.51\n",
      "\tEpoch 31 | Batch 0 | Acc   0.83\n",
      "Epoch 31 | Loss   0.52\n",
      "\tEpoch 32 | Batch 0 | Loss   0.47\n",
      "\tEpoch 32 | Batch 0 | Acc   0.92\n",
      "Epoch 32 | Loss   0.51\n",
      "\tEpoch 33 | Batch 0 | Loss   0.51\n",
      "\tEpoch 33 | Batch 0 | Acc   0.88\n",
      "Epoch 33 | Loss   0.52\n",
      "\tEpoch 34 | Batch 0 | Loss   0.55\n",
      "\tEpoch 34 | Batch 0 | Acc   0.81\n",
      "Epoch 34 | Loss   0.52\n",
      "\tEpoch 35 | Batch 0 | Loss   0.47\n",
      "\tEpoch 35 | Batch 0 | Acc   0.89\n",
      "Epoch 35 | Loss   0.52\n",
      "\tEpoch 36 | Batch 0 | Loss   0.48\n",
      "\tEpoch 36 | Batch 0 | Acc   0.89\n",
      "Epoch 36 | Loss   0.51\n",
      "\tEpoch 37 | Batch 0 | Loss   0.55\n",
      "\tEpoch 37 | Batch 0 | Acc   0.77\n",
      "Epoch 37 | Loss   0.51\n",
      "\tEpoch 38 | Batch 0 | Loss   0.52\n",
      "\tEpoch 38 | Batch 0 | Acc   0.83\n",
      "Epoch 38 | Loss   0.50\n",
      "\tEpoch 39 | Batch 0 | Loss   0.55\n",
      "\tEpoch 39 | Batch 0 | Acc   0.78\n",
      "Epoch 39 | Loss   0.50\n",
      "\tEpoch 40 | Batch 0 | Loss   0.48\n",
      "\tEpoch 40 | Batch 0 | Acc   0.91\n",
      "Epoch 40 | Loss   0.51\n",
      "\tEpoch 41 | Batch 0 | Loss   0.49\n",
      "\tEpoch 41 | Batch 0 | Acc   0.84\n",
      "Epoch 41 | Loss   0.52\n",
      "\tEpoch 42 | Batch 0 | Loss   0.47\n",
      "\tEpoch 42 | Batch 0 | Acc   0.88\n",
      "Epoch 42 | Loss   0.50\n",
      "\tEpoch 43 | Batch 0 | Loss   0.52\n",
      "\tEpoch 43 | Batch 0 | Acc   0.84\n",
      "Epoch 43 | Loss   0.51\n",
      "\tEpoch 44 | Batch 0 | Loss   0.44\n",
      "\tEpoch 44 | Batch 0 | Acc   0.94\n",
      "Epoch 44 | Loss   0.51\n",
      "\tEpoch 45 | Batch 0 | Loss   0.51\n",
      "\tEpoch 45 | Batch 0 | Acc   0.86\n",
      "Epoch 45 | Loss   0.50\n",
      "\tEpoch 46 | Batch 0 | Loss   0.48\n",
      "\tEpoch 46 | Batch 0 | Acc   0.88\n",
      "Epoch 46 | Loss   0.50\n",
      "\tEpoch 47 | Batch 0 | Loss   0.51\n",
      "\tEpoch 47 | Batch 0 | Acc   0.86\n",
      "Epoch 47 | Loss   0.50\n",
      "\tEpoch 48 | Batch 0 | Loss   0.49\n",
      "\tEpoch 48 | Batch 0 | Acc   0.89\n",
      "Epoch 48 | Loss   0.49\n",
      "\tEpoch 49 | Batch 0 | Loss   0.50\n",
      "\tEpoch 49 | Batch 0 | Acc   0.84\n",
      "Epoch 49 | Loss   0.49\n",
      "\tEpoch 50 | Batch 0 | Loss   0.47\n",
      "\tEpoch 50 | Batch 0 | Acc   0.92\n",
      "Epoch 50 | Loss   0.49\n",
      "\tEpoch 51 | Batch 0 | Loss   0.46\n",
      "\tEpoch 51 | Batch 0 | Acc   0.91\n",
      "Epoch 51 | Loss   0.50\n",
      "\tEpoch 52 | Batch 0 | Loss   0.46\n",
      "\tEpoch 52 | Batch 0 | Acc   0.91\n",
      "Epoch 52 | Loss   0.50\n",
      "\tEpoch 53 | Batch 0 | Loss   0.47\n",
      "\tEpoch 53 | Batch 0 | Acc   0.89\n",
      "Epoch 53 | Loss   0.49\n",
      "\tEpoch 54 | Batch 0 | Loss   0.51\n",
      "\tEpoch 54 | Batch 0 | Acc   0.83\n",
      "Epoch 54 | Loss   0.49\n",
      "\tEpoch 55 | Batch 0 | Loss   0.53\n",
      "\tEpoch 55 | Batch 0 | Acc   0.81\n",
      "Epoch 55 | Loss   0.48\n",
      "\tEpoch 56 | Batch 0 | Loss   0.48\n",
      "\tEpoch 56 | Batch 0 | Acc   0.88\n",
      "Epoch 56 | Loss   0.49\n",
      "\tEpoch 57 | Batch 0 | Loss   0.49\n",
      "\tEpoch 57 | Batch 0 | Acc   0.89\n",
      "Epoch 57 | Loss   0.50\n",
      "\tEpoch 58 | Batch 0 | Loss   0.48\n",
      "\tEpoch 58 | Batch 0 | Acc   0.88\n",
      "Epoch 58 | Loss   0.48\n",
      "\tEpoch 59 | Batch 0 | Loss   0.49\n",
      "\tEpoch 59 | Batch 0 | Acc   0.84\n",
      "Epoch 59 | Loss   0.49\n",
      "\tEpoch 60 | Batch 0 | Loss   0.56\n",
      "\tEpoch 60 | Batch 0 | Acc   0.75\n",
      "Epoch 60 | Loss   0.49\n",
      "\tEpoch 61 | Batch 0 | Loss   0.44\n",
      "\tEpoch 61 | Batch 0 | Acc   0.94\n",
      "Epoch 61 | Loss   0.48\n",
      "\tEpoch 62 | Batch 0 | Loss   0.53\n",
      "\tEpoch 62 | Batch 0 | Acc   0.84\n",
      "Epoch 62 | Loss   0.48\n",
      "\tEpoch 63 | Batch 0 | Loss   0.45\n",
      "\tEpoch 63 | Batch 0 | Acc   0.92\n",
      "Epoch 63 | Loss   0.48\n",
      "\tEpoch 64 | Batch 0 | Loss   0.45\n",
      "\tEpoch 64 | Batch 0 | Acc   0.91\n",
      "Epoch 64 | Loss   0.48\n",
      "\tEpoch 65 | Batch 0 | Loss   0.49\n",
      "\tEpoch 65 | Batch 0 | Acc   0.84\n",
      "Epoch 65 | Loss   0.47\n",
      "\tEpoch 66 | Batch 0 | Loss   0.49\n",
      "\tEpoch 66 | Batch 0 | Acc   0.88\n",
      "Epoch 66 | Loss   0.48\n",
      "\tEpoch 67 | Batch 0 | Loss   0.50\n",
      "\tEpoch 67 | Batch 0 | Acc   0.81\n",
      "Epoch 67 | Loss   0.46\n",
      "\tEpoch 68 | Batch 0 | Loss   0.45\n",
      "\tEpoch 68 | Batch 0 | Acc   0.92\n",
      "Epoch 68 | Loss   0.48\n",
      "\tEpoch 69 | Batch 0 | Loss   0.46\n",
      "\tEpoch 69 | Batch 0 | Acc   0.91\n",
      "Epoch 69 | Loss   0.47\n",
      "\tEpoch 70 | Batch 0 | Loss   0.47\n",
      "\tEpoch 70 | Batch 0 | Acc   0.89\n",
      "Epoch 70 | Loss   0.47\n",
      "\tEpoch 71 | Batch 0 | Loss   0.47\n",
      "\tEpoch 71 | Batch 0 | Acc   0.91\n",
      "Epoch 71 | Loss   0.46\n",
      "\tEpoch 72 | Batch 0 | Loss   0.47\n",
      "\tEpoch 72 | Batch 0 | Acc   0.89\n",
      "Epoch 72 | Loss   0.47\n",
      "\tEpoch 73 | Batch 0 | Loss   0.42\n",
      "\tEpoch 73 | Batch 0 | Acc   0.94\n",
      "Epoch 73 | Loss   0.47\n",
      "\tEpoch 74 | Batch 0 | Loss   0.44\n",
      "\tEpoch 74 | Batch 0 | Acc   0.92\n",
      "Epoch 74 | Loss   0.46\n",
      "\tEpoch 75 | Batch 0 | Loss   0.47\n",
      "\tEpoch 75 | Batch 0 | Acc   0.89\n",
      "Epoch 75 | Loss   0.45\n",
      "\tEpoch 76 | Batch 0 | Loss   0.44\n",
      "\tEpoch 76 | Batch 0 | Acc   0.92\n",
      "Epoch 76 | Loss   0.46\n",
      "\tEpoch 77 | Batch 0 | Loss   0.41\n",
      "\tEpoch 77 | Batch 0 | Acc   0.97\n",
      "Epoch 77 | Loss   0.45\n",
      "\tEpoch 78 | Batch 0 | Loss   0.49\n",
      "\tEpoch 78 | Batch 0 | Acc   0.86\n",
      "Epoch 78 | Loss   0.45\n",
      "\tEpoch 79 | Batch 0 | Loss   0.45\n",
      "\tEpoch 79 | Batch 0 | Acc   0.91\n",
      "Epoch 79 | Loss   0.45\n",
      "\tEpoch 80 | Batch 0 | Loss   0.47\n",
      "\tEpoch 80 | Batch 0 | Acc   0.89\n",
      "Epoch 80 | Loss   0.45\n",
      "\tEpoch 81 | Batch 0 | Loss   0.45\n",
      "\tEpoch 81 | Batch 0 | Acc   0.92\n",
      "Epoch 81 | Loss   0.46\n",
      "\tEpoch 82 | Batch 0 | Loss   0.41\n",
      "\tEpoch 82 | Batch 0 | Acc   0.97\n",
      "Epoch 82 | Loss   0.45\n",
      "\tEpoch 83 | Batch 0 | Loss   0.45\n",
      "\tEpoch 83 | Batch 0 | Acc   0.89\n",
      "Epoch 83 | Loss   0.45\n",
      "\tEpoch 84 | Batch 0 | Loss   0.42\n",
      "\tEpoch 84 | Batch 0 | Acc   0.97\n",
      "Epoch 84 | Loss   0.44\n",
      "\tEpoch 85 | Batch 0 | Loss   0.48\n",
      "\tEpoch 85 | Batch 0 | Acc   0.88\n",
      "Epoch 85 | Loss   0.45\n",
      "\tEpoch 86 | Batch 0 | Loss   0.50\n",
      "\tEpoch 86 | Batch 0 | Acc   0.86\n",
      "Epoch 86 | Loss   0.44\n",
      "\tEpoch 87 | Batch 0 | Loss   0.42\n",
      "\tEpoch 87 | Batch 0 | Acc   0.97\n",
      "Epoch 87 | Loss   0.46\n",
      "\tEpoch 88 | Batch 0 | Loss   0.43\n",
      "\tEpoch 88 | Batch 0 | Acc   0.95\n",
      "Epoch 88 | Loss   0.45\n",
      "\tEpoch 89 | Batch 0 | Loss   0.40\n",
      "\tEpoch 89 | Batch 0 | Acc   0.97\n",
      "Epoch 89 | Loss   0.45\n",
      "\tEpoch 90 | Batch 0 | Loss   0.44\n",
      "\tEpoch 90 | Batch 0 | Acc   0.94\n",
      "Epoch 90 | Loss   0.44\n",
      "\tEpoch 91 | Batch 0 | Loss   0.45\n",
      "\tEpoch 91 | Batch 0 | Acc   0.89\n",
      "Epoch 91 | Loss   0.44\n",
      "\tEpoch 92 | Batch 0 | Loss   0.45\n",
      "\tEpoch 92 | Batch 0 | Acc   0.92\n",
      "Epoch 92 | Loss   0.44\n",
      "\tEpoch 93 | Batch 0 | Loss   0.44\n",
      "\tEpoch 93 | Batch 0 | Acc   0.91\n",
      "Epoch 93 | Loss   0.43\n",
      "\tEpoch 94 | Batch 0 | Loss   0.42\n",
      "\tEpoch 94 | Batch 0 | Acc   0.92\n",
      "Epoch 94 | Loss   0.44\n",
      "\tEpoch 95 | Batch 0 | Loss   0.45\n",
      "\tEpoch 95 | Batch 0 | Acc   0.91\n",
      "Epoch 95 | Loss   0.44\n",
      "\tEpoch 96 | Batch 0 | Loss   0.44\n",
      "\tEpoch 96 | Batch 0 | Acc   0.95\n",
      "Epoch 96 | Loss   0.43\n",
      "\tEpoch 97 | Batch 0 | Loss   0.42\n",
      "\tEpoch 97 | Batch 0 | Acc   0.94\n",
      "Epoch 97 | Loss   0.44\n",
      "\tEpoch 98 | Batch 0 | Loss   0.43\n",
      "\tEpoch 98 | Batch 0 | Acc   0.95\n",
      "Epoch 98 | Loss   0.43\n",
      "\tEpoch 99 | Batch 0 | Loss   0.41\n",
      "\tEpoch 99 | Batch 0 | Acc   0.97\n",
      "Epoch 99 | Loss   0.43\n",
      "\tEpoch 100 | Batch 0 | Loss   0.43\n",
      "\tEpoch 100 | Batch 0 | Acc   0.94\n",
      "Epoch 100 | Loss   0.43\n",
      "\tEpoch 101 | Batch 0 | Loss   0.48\n",
      "\tEpoch 101 | Batch 0 | Acc   0.88\n",
      "Epoch 101 | Loss   0.43\n",
      "\tEpoch 102 | Batch 0 | Loss   0.42\n",
      "\tEpoch 102 | Batch 0 | Acc   0.95\n",
      "Epoch 102 | Loss   0.44\n",
      "\tEpoch 103 | Batch 0 | Loss   0.43\n",
      "\tEpoch 103 | Batch 0 | Acc   0.95\n",
      "Epoch 103 | Loss   0.43\n",
      "\tEpoch 104 | Batch 0 | Loss   0.45\n",
      "\tEpoch 104 | Batch 0 | Acc   0.95\n",
      "Epoch 104 | Loss   0.42\n",
      "\tEpoch 105 | Batch 0 | Loss   0.45\n",
      "\tEpoch 105 | Batch 0 | Acc   0.92\n",
      "Epoch 105 | Loss   0.42\n",
      "\tEpoch 106 | Batch 0 | Loss   0.42\n",
      "\tEpoch 106 | Batch 0 | Acc   0.95\n",
      "Epoch 106 | Loss   0.43\n",
      "\tEpoch 107 | Batch 0 | Loss   0.44\n",
      "\tEpoch 107 | Batch 0 | Acc   0.94\n",
      "Epoch 107 | Loss   0.43\n",
      "\tEpoch 108 | Batch 0 | Loss   0.46\n",
      "\tEpoch 108 | Batch 0 | Acc   0.88\n",
      "Epoch 108 | Loss   0.42\n",
      "\tEpoch 109 | Batch 0 | Loss   0.42\n",
      "\tEpoch 109 | Batch 0 | Acc   0.94\n",
      "Epoch 109 | Loss   0.42\n",
      "\tEpoch 110 | Batch 0 | Loss   0.41\n",
      "\tEpoch 110 | Batch 0 | Acc   0.97\n",
      "Epoch 110 | Loss   0.42\n",
      "\tEpoch 111 | Batch 0 | Loss   0.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 111 | Batch 0 | Acc   0.97\n",
      "Epoch 111 | Loss   0.42\n",
      "\tEpoch 112 | Batch 0 | Loss   0.42\n",
      "\tEpoch 112 | Batch 0 | Acc   0.95\n",
      "Epoch 112 | Loss   0.42\n",
      "\tEpoch 113 | Batch 0 | Loss   0.44\n",
      "\tEpoch 113 | Batch 0 | Acc   0.94\n",
      "Epoch 113 | Loss   0.42\n",
      "\tEpoch 114 | Batch 0 | Loss   0.40\n",
      "\tEpoch 114 | Batch 0 | Acc   0.97\n",
      "Epoch 114 | Loss   0.42\n",
      "\tEpoch 115 | Batch 0 | Loss   0.44\n",
      "\tEpoch 115 | Batch 0 | Acc   0.94\n",
      "Epoch 115 | Loss   0.42\n",
      "\tEpoch 116 | Batch 0 | Loss   0.40\n",
      "\tEpoch 116 | Batch 0 | Acc   0.97\n",
      "Epoch 116 | Loss   0.43\n",
      "\tEpoch 117 | Batch 0 | Loss   0.41\n",
      "\tEpoch 117 | Batch 0 | Acc   0.95\n",
      "Epoch 117 | Loss   0.42\n",
      "\tEpoch 118 | Batch 0 | Loss   0.41\n",
      "\tEpoch 118 | Batch 0 | Acc   0.94\n",
      "Epoch 118 | Loss   0.41\n",
      "\tEpoch 119 | Batch 0 | Loss   0.44\n",
      "\tEpoch 119 | Batch 0 | Acc   0.91\n",
      "Epoch 119 | Loss   0.41\n",
      "\tEpoch 120 | Batch 0 | Loss   0.43\n",
      "\tEpoch 120 | Batch 0 | Acc   0.94\n",
      "Epoch 120 | Loss   0.41\n",
      "\tEpoch 121 | Batch 0 | Loss   0.42\n",
      "\tEpoch 121 | Batch 0 | Acc   0.94\n",
      "Epoch 121 | Loss   0.41\n",
      "\tEpoch 122 | Batch 0 | Loss   0.40\n",
      "\tEpoch 122 | Batch 0 | Acc   0.98\n",
      "Epoch 122 | Loss   0.41\n",
      "\tEpoch 123 | Batch 0 | Loss   0.40\n",
      "\tEpoch 123 | Batch 0 | Acc   0.97\n",
      "Epoch 123 | Loss   0.41\n",
      "\tEpoch 124 | Batch 0 | Loss   0.40\n",
      "\tEpoch 124 | Batch 0 | Acc   0.97\n",
      "Epoch 124 | Loss   0.42\n",
      "\tEpoch 125 | Batch 0 | Loss   0.41\n",
      "\tEpoch 125 | Batch 0 | Acc   0.97\n",
      "Epoch 125 | Loss   0.41\n",
      "\tEpoch 126 | Batch 0 | Loss   0.43\n",
      "\tEpoch 126 | Batch 0 | Acc   0.94\n",
      "Epoch 126 | Loss   0.42\n",
      "\tEpoch 127 | Batch 0 | Loss   0.40\n",
      "\tEpoch 127 | Batch 0 | Acc   0.97\n",
      "Epoch 127 | Loss   0.41\n",
      "\tEpoch 128 | Batch 0 | Loss   0.39\n",
      "\tEpoch 128 | Batch 0 | Acc   0.98\n",
      "Epoch 128 | Loss   0.41\n",
      "\tEpoch 129 | Batch 0 | Loss   0.48\n",
      "\tEpoch 129 | Batch 0 | Acc   0.86\n",
      "Epoch 129 | Loss   0.41\n",
      "\tEpoch 130 | Batch 0 | Loss   0.44\n",
      "\tEpoch 130 | Batch 0 | Acc   0.91\n",
      "Epoch 130 | Loss   0.41\n",
      "\tEpoch 131 | Batch 0 | Loss   0.41\n",
      "\tEpoch 131 | Batch 0 | Acc   0.97\n",
      "Epoch 131 | Loss   0.41\n",
      "\tEpoch 132 | Batch 0 | Loss   0.40\n",
      "\tEpoch 132 | Batch 0 | Acc   0.97\n",
      "Epoch 132 | Loss   0.41\n",
      "\tEpoch 133 | Batch 0 | Loss   0.41\n",
      "\tEpoch 133 | Batch 0 | Acc   0.95\n",
      "Epoch 133 | Loss   0.41\n",
      "\tEpoch 134 | Batch 0 | Loss   0.43\n",
      "\tEpoch 134 | Batch 0 | Acc   0.92\n",
      "Epoch 134 | Loss   0.41\n",
      "\tEpoch 135 | Batch 0 | Loss   0.43\n",
      "\tEpoch 135 | Batch 0 | Acc   0.94\n",
      "Epoch 135 | Loss   0.41\n",
      "\tEpoch 136 | Batch 0 | Loss   0.43\n",
      "\tEpoch 136 | Batch 0 | Acc   0.94\n",
      "Epoch 136 | Loss   0.41\n",
      "\tEpoch 137 | Batch 0 | Loss   0.41\n",
      "\tEpoch 137 | Batch 0 | Acc   0.94\n",
      "Epoch 137 | Loss   0.41\n",
      "\tEpoch 138 | Batch 0 | Loss   0.44\n",
      "\tEpoch 138 | Batch 0 | Acc   0.94\n",
      "Epoch 138 | Loss   0.41\n",
      "\tEpoch 139 | Batch 0 | Loss   0.40\n",
      "\tEpoch 139 | Batch 0 | Acc   0.97\n",
      "Epoch 139 | Loss   0.41\n",
      "\tEpoch 140 | Batch 0 | Loss   0.43\n",
      "\tEpoch 140 | Batch 0 | Acc   0.92\n",
      "Epoch 140 | Loss   0.40\n",
      "\tEpoch 141 | Batch 0 | Loss   0.41\n",
      "\tEpoch 141 | Batch 0 | Acc   0.95\n",
      "Epoch 141 | Loss   0.40\n",
      "\tEpoch 142 | Batch 0 | Loss   0.39\n",
      "\tEpoch 142 | Batch 0 | Acc   0.98\n",
      "Epoch 142 | Loss   0.41\n",
      "\tEpoch 143 | Batch 0 | Loss   0.38\n",
      "\tEpoch 143 | Batch 0 | Acc   0.98\n",
      "Epoch 143 | Loss   0.41\n",
      "\tEpoch 144 | Batch 0 | Loss   0.38\n",
      "\tEpoch 144 | Batch 0 | Acc   0.97\n",
      "Epoch 144 | Loss   0.40\n",
      "\tEpoch 145 | Batch 0 | Loss   0.41\n",
      "\tEpoch 145 | Batch 0 | Acc   0.95\n",
      "Epoch 145 | Loss   0.40\n",
      "\tEpoch 146 | Batch 0 | Loss   0.40\n",
      "\tEpoch 146 | Batch 0 | Acc   0.95\n",
      "Epoch 146 | Loss   0.41\n",
      "\tEpoch 147 | Batch 0 | Loss   0.39\n",
      "\tEpoch 147 | Batch 0 | Acc   0.98\n",
      "Epoch 147 | Loss   0.41\n",
      "\tEpoch 148 | Batch 0 | Loss   0.39\n",
      "\tEpoch 148 | Batch 0 | Acc   0.97\n",
      "Epoch 148 | Loss   0.40\n",
      "\tEpoch 149 | Batch 0 | Loss   0.38\n",
      "\tEpoch 149 | Batch 0 | Acc   0.98\n",
      "Epoch 149 | Loss   0.40\n",
      "\tEpoch 150 | Batch 0 | Loss   0.38\n",
      "\tEpoch 150 | Batch 0 | Acc   0.98\n",
      "Epoch 150 | Loss   0.42\n",
      "\tEpoch 151 | Batch 0 | Loss   0.40\n",
      "\tEpoch 151 | Batch 0 | Acc   0.95\n",
      "Epoch 151 | Loss   0.40\n",
      "\tEpoch 152 | Batch 0 | Loss   0.37\n",
      "\tEpoch 152 | Batch 0 | Acc   1.00\n",
      "Epoch 152 | Loss   0.40\n",
      "\tEpoch 153 | Batch 0 | Loss   0.40\n",
      "\tEpoch 153 | Batch 0 | Acc   0.97\n",
      "Epoch 153 | Loss   0.40\n",
      "\tEpoch 154 | Batch 0 | Loss   0.39\n",
      "\tEpoch 154 | Batch 0 | Acc   0.98\n",
      "Epoch 154 | Loss   0.41\n",
      "\tEpoch 155 | Batch 0 | Loss   0.38\n",
      "\tEpoch 155 | Batch 0 | Acc   0.97\n",
      "Epoch 155 | Loss   0.40\n",
      "\tEpoch 156 | Batch 0 | Loss   0.41\n",
      "\tEpoch 156 | Batch 0 | Acc   0.94\n",
      "Epoch 156 | Loss   0.40\n",
      "\tEpoch 157 | Batch 0 | Loss   0.40\n",
      "\tEpoch 157 | Batch 0 | Acc   0.97\n",
      "Epoch 157 | Loss   0.40\n",
      "\tEpoch 158 | Batch 0 | Loss   0.42\n",
      "\tEpoch 158 | Batch 0 | Acc   0.94\n",
      "Epoch 158 | Loss   0.40\n",
      "\tEpoch 159 | Batch 0 | Loss   0.45\n",
      "\tEpoch 159 | Batch 0 | Acc   0.89\n",
      "Epoch 159 | Loss   0.40\n",
      "\tEpoch 160 | Batch 0 | Loss   0.42\n",
      "\tEpoch 160 | Batch 0 | Acc   0.92\n",
      "Epoch 160 | Loss   0.40\n",
      "\tEpoch 161 | Batch 0 | Loss   0.38\n",
      "\tEpoch 161 | Batch 0 | Acc   0.97\n",
      "Epoch 161 | Loss   0.40\n",
      "\tEpoch 162 | Batch 0 | Loss   0.40\n",
      "\tEpoch 162 | Batch 0 | Acc   0.95\n",
      "Epoch 162 | Loss   0.41\n",
      "\tEpoch 163 | Batch 0 | Loss   0.40\n",
      "\tEpoch 163 | Batch 0 | Acc   0.94\n",
      "Epoch 163 | Loss   0.40\n",
      "\tEpoch 164 | Batch 0 | Loss   0.37\n",
      "\tEpoch 164 | Batch 0 | Acc   0.98\n",
      "Epoch 164 | Loss   0.40\n",
      "\tEpoch 165 | Batch 0 | Loss   0.38\n",
      "\tEpoch 165 | Batch 0 | Acc   0.97\n",
      "Epoch 165 | Loss   0.39\n",
      "\tEpoch 166 | Batch 0 | Loss   0.39\n",
      "\tEpoch 166 | Batch 0 | Acc   0.97\n",
      "Epoch 166 | Loss   0.40\n",
      "\tEpoch 167 | Batch 0 | Loss   0.42\n",
      "\tEpoch 167 | Batch 0 | Acc   0.94\n",
      "Epoch 167 | Loss   0.39\n",
      "\tEpoch 168 | Batch 0 | Loss   0.39\n",
      "\tEpoch 168 | Batch 0 | Acc   0.95\n",
      "Epoch 168 | Loss   0.41\n",
      "\tEpoch 169 | Batch 0 | Loss   0.37\n",
      "\tEpoch 169 | Batch 0 | Acc   1.00\n",
      "Epoch 169 | Loss   0.39\n",
      "\tEpoch 170 | Batch 0 | Loss   0.40\n",
      "\tEpoch 170 | Batch 0 | Acc   0.94\n",
      "Epoch 170 | Loss   0.40\n",
      "\tEpoch 171 | Batch 0 | Loss   0.36\n",
      "\tEpoch 171 | Batch 0 | Acc   1.00\n",
      "Epoch 171 | Loss   0.39\n",
      "\tEpoch 172 | Batch 0 | Loss   0.40\n",
      "\tEpoch 172 | Batch 0 | Acc   0.95\n",
      "Epoch 172 | Loss   0.39\n",
      "\tEpoch 173 | Batch 0 | Loss   0.40\n",
      "\tEpoch 173 | Batch 0 | Acc   0.94\n",
      "Epoch 173 | Loss   0.39\n",
      "\tEpoch 174 | Batch 0 | Loss   0.40\n",
      "\tEpoch 174 | Batch 0 | Acc   0.94\n",
      "Epoch 174 | Loss   0.39\n",
      "\tEpoch 175 | Batch 0 | Loss   0.40\n",
      "\tEpoch 175 | Batch 0 | Acc   0.95\n",
      "Epoch 175 | Loss   0.39\n",
      "\tEpoch 176 | Batch 0 | Loss   0.39\n",
      "\tEpoch 176 | Batch 0 | Acc   0.97\n",
      "Epoch 176 | Loss   0.40\n",
      "\tEpoch 177 | Batch 0 | Loss   0.38\n",
      "\tEpoch 177 | Batch 0 | Acc   0.97\n",
      "Epoch 177 | Loss   0.39\n",
      "\tEpoch 178 | Batch 0 | Loss   0.38\n",
      "\tEpoch 178 | Batch 0 | Acc   0.97\n",
      "Epoch 178 | Loss   0.39\n",
      "\tEpoch 179 | Batch 0 | Loss   0.39\n",
      "\tEpoch 179 | Batch 0 | Acc   0.95\n",
      "Epoch 179 | Loss   0.40\n",
      "\tEpoch 180 | Batch 0 | Loss   0.41\n",
      "\tEpoch 180 | Batch 0 | Acc   0.94\n",
      "Epoch 180 | Loss   0.39\n",
      "\tEpoch 181 | Batch 0 | Loss   0.39\n",
      "\tEpoch 181 | Batch 0 | Acc   0.95\n",
      "Epoch 181 | Loss   0.39\n",
      "\tEpoch 182 | Batch 0 | Loss   0.39\n",
      "\tEpoch 182 | Batch 0 | Acc   0.97\n",
      "Epoch 182 | Loss   0.39\n",
      "\tEpoch 183 | Batch 0 | Loss   0.38\n",
      "\tEpoch 183 | Batch 0 | Acc   0.97\n",
      "Epoch 183 | Loss   0.40\n",
      "\tEpoch 184 | Batch 0 | Loss   0.40\n",
      "\tEpoch 184 | Batch 0 | Acc   0.95\n",
      "Epoch 184 | Loss   0.39\n",
      "\tEpoch 185 | Batch 0 | Loss   0.41\n",
      "\tEpoch 185 | Batch 0 | Acc   0.92\n",
      "Epoch 185 | Loss   0.39\n",
      "\tEpoch 186 | Batch 0 | Loss   0.37\n",
      "\tEpoch 186 | Batch 0 | Acc   0.98\n",
      "Epoch 186 | Loss   0.39\n",
      "\tEpoch 187 | Batch 0 | Loss   0.40\n",
      "\tEpoch 187 | Batch 0 | Acc   0.94\n",
      "Epoch 187 | Loss   0.39\n",
      "\tEpoch 188 | Batch 0 | Loss   0.42\n",
      "\tEpoch 188 | Batch 0 | Acc   0.92\n",
      "Epoch 188 | Loss   0.39\n",
      "\tEpoch 189 | Batch 0 | Loss   0.37\n",
      "\tEpoch 189 | Batch 0 | Acc   0.98\n",
      "Epoch 189 | Loss   0.39\n",
      "\tEpoch 190 | Batch 0 | Loss   0.39\n",
      "\tEpoch 190 | Batch 0 | Acc   0.95\n",
      "Epoch 190 | Loss   0.39\n",
      "\tEpoch 191 | Batch 0 | Loss   0.38\n",
      "\tEpoch 191 | Batch 0 | Acc   0.97\n",
      "Epoch 191 | Loss   0.39\n",
      "\tEpoch 192 | Batch 0 | Loss   0.42\n",
      "\tEpoch 192 | Batch 0 | Acc   0.91\n",
      "Epoch 192 | Loss   0.40\n",
      "\tEpoch 193 | Batch 0 | Loss   0.41\n",
      "\tEpoch 193 | Batch 0 | Acc   0.94\n",
      "Epoch 193 | Loss   0.40\n",
      "\tEpoch 194 | Batch 0 | Loss   0.40\n",
      "\tEpoch 194 | Batch 0 | Acc   0.94\n",
      "Epoch 194 | Loss   0.39\n",
      "\tEpoch 195 | Batch 0 | Loss   0.37\n",
      "\tEpoch 195 | Batch 0 | Acc   0.98\n",
      "Epoch 195 | Loss   0.39\n",
      "\tEpoch 196 | Batch 0 | Loss   0.39\n",
      "\tEpoch 196 | Batch 0 | Acc   0.95\n",
      "Epoch 196 | Loss   0.39\n",
      "\tEpoch 197 | Batch 0 | Loss   0.41\n",
      "\tEpoch 197 | Batch 0 | Acc   0.91\n",
      "Epoch 197 | Loss   0.39\n",
      "\tEpoch 198 | Batch 0 | Loss   0.41\n",
      "\tEpoch 198 | Batch 0 | Acc   0.92\n",
      "Epoch 198 | Loss   0.39\n",
      "\tEpoch 199 | Batch 0 | Loss   0.39\n",
      "\tEpoch 199 | Batch 0 | Acc   0.97\n",
      "Epoch 199 | Loss   0.39\n",
      "\tEpoch 200 | Batch 0 | Loss   0.36\n",
      "\tEpoch 200 | Batch 0 | Acc   0.97\n",
      "Epoch 200 | Loss   0.38\n",
      "\tEpoch 201 | Batch 0 | Loss   0.39\n",
      "\tEpoch 201 | Batch 0 | Acc   0.95\n",
      "Epoch 201 | Loss   0.39\n",
      "\tEpoch 202 | Batch 0 | Loss   0.40\n",
      "\tEpoch 202 | Batch 0 | Acc   0.95\n",
      "Epoch 202 | Loss   0.39\n",
      "\tEpoch 203 | Batch 0 | Loss   0.37\n",
      "\tEpoch 203 | Batch 0 | Acc   0.97\n",
      "Epoch 203 | Loss   0.38\n",
      "\tEpoch 204 | Batch 0 | Loss   0.40\n",
      "\tEpoch 204 | Batch 0 | Acc   0.94\n",
      "Epoch 204 | Loss   0.38\n",
      "\tEpoch 205 | Batch 0 | Loss   0.37\n",
      "\tEpoch 205 | Batch 0 | Acc   0.98\n",
      "Epoch 205 | Loss   0.38\n",
      "\tEpoch 206 | Batch 0 | Loss   0.38\n",
      "\tEpoch 206 | Batch 0 | Acc   0.95\n",
      "Epoch 206 | Loss   0.38\n",
      "\tEpoch 207 | Batch 0 | Loss   0.42\n",
      "\tEpoch 207 | Batch 0 | Acc   0.91\n",
      "Epoch 207 | Loss   0.38\n",
      "\tEpoch 208 | Batch 0 | Loss   0.40\n",
      "\tEpoch 208 | Batch 0 | Acc   0.94\n",
      "Epoch 208 | Loss   0.39\n",
      "\tEpoch 209 | Batch 0 | Loss   0.39\n",
      "\tEpoch 209 | Batch 0 | Acc   0.95\n",
      "Epoch 209 | Loss   0.39\n",
      "\tEpoch 210 | Batch 0 | Loss   0.38\n",
      "\tEpoch 210 | Batch 0 | Acc   0.95\n",
      "Epoch 210 | Loss   0.38\n",
      "\tEpoch 211 | Batch 0 | Loss   0.42\n",
      "\tEpoch 211 | Batch 0 | Acc   0.92\n",
      "Epoch 211 | Loss   0.38\n",
      "\tEpoch 212 | Batch 0 | Loss   0.35\n",
      "\tEpoch 212 | Batch 0 | Acc   1.00\n",
      "Epoch 212 | Loss   0.38\n",
      "\tEpoch 213 | Batch 0 | Loss   0.40\n",
      "\tEpoch 213 | Batch 0 | Acc   0.94\n",
      "Epoch 213 | Loss   0.39\n",
      "\tEpoch 214 | Batch 0 | Loss   0.38\n",
      "\tEpoch 214 | Batch 0 | Acc   0.95\n",
      "Epoch 214 | Loss   0.39\n",
      "\tEpoch 215 | Batch 0 | Loss   0.39\n",
      "\tEpoch 215 | Batch 0 | Acc   0.95\n",
      "Epoch 215 | Loss   0.38\n",
      "\tEpoch 216 | Batch 0 | Loss   0.40\n",
      "\tEpoch 216 | Batch 0 | Acc   0.95\n",
      "Epoch 216 | Loss   0.38\n",
      "\tEpoch 217 | Batch 0 | Loss   0.40\n",
      "\tEpoch 217 | Batch 0 | Acc   0.92\n",
      "Epoch 217 | Loss   0.39\n",
      "\tEpoch 218 | Batch 0 | Loss   0.36\n",
      "\tEpoch 218 | Batch 0 | Acc   0.97\n",
      "Epoch 218 | Loss   0.40\n",
      "\tEpoch 219 | Batch 0 | Loss   0.42\n",
      "\tEpoch 219 | Batch 0 | Acc   0.91\n",
      "Epoch 219 | Loss   0.38\n",
      "\tEpoch 220 | Batch 0 | Loss   0.39\n",
      "\tEpoch 220 | Batch 0 | Acc   0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220 | Loss   0.38\n",
      "\tEpoch 221 | Batch 0 | Loss   0.38\n",
      "\tEpoch 221 | Batch 0 | Acc   0.95\n",
      "Epoch 221 | Loss   0.39\n",
      "\tEpoch 222 | Batch 0 | Loss   0.42\n",
      "\tEpoch 222 | Batch 0 | Acc   0.91\n",
      "Epoch 222 | Loss   0.38\n",
      "\tEpoch 223 | Batch 0 | Loss   0.39\n",
      "\tEpoch 223 | Batch 0 | Acc   0.95\n",
      "Epoch 223 | Loss   0.38\n",
      "\tEpoch 224 | Batch 0 | Loss   0.43\n",
      "\tEpoch 224 | Batch 0 | Acc   0.88\n",
      "Epoch 224 | Loss   0.38\n",
      "\tEpoch 225 | Batch 0 | Loss   0.37\n",
      "\tEpoch 225 | Batch 0 | Acc   0.98\n",
      "Epoch 225 | Loss   0.39\n",
      "\tEpoch 226 | Batch 0 | Loss   0.36\n",
      "\tEpoch 226 | Batch 0 | Acc   1.00\n",
      "Epoch 226 | Loss   0.38\n",
      "\tEpoch 227 | Batch 0 | Loss   0.36\n",
      "\tEpoch 227 | Batch 0 | Acc   0.98\n",
      "Epoch 227 | Loss   0.38\n",
      "\tEpoch 228 | Batch 0 | Loss   0.39\n",
      "\tEpoch 228 | Batch 0 | Acc   0.95\n",
      "Epoch 228 | Loss   0.38\n",
      "\tEpoch 229 | Batch 0 | Loss   0.41\n",
      "\tEpoch 229 | Batch 0 | Acc   0.91\n",
      "Epoch 229 | Loss   0.39\n",
      "\tEpoch 230 | Batch 0 | Loss   0.43\n",
      "\tEpoch 230 | Batch 0 | Acc   0.91\n",
      "Epoch 230 | Loss   0.38\n",
      "\tEpoch 231 | Batch 0 | Loss   0.37\n",
      "\tEpoch 231 | Batch 0 | Acc   0.97\n",
      "Epoch 231 | Loss   0.39\n",
      "\tEpoch 232 | Batch 0 | Loss   0.38\n",
      "\tEpoch 232 | Batch 0 | Acc   0.95\n",
      "Epoch 232 | Loss   0.38\n",
      "\tEpoch 233 | Batch 0 | Loss   0.39\n",
      "\tEpoch 233 | Batch 0 | Acc   0.95\n",
      "Epoch 233 | Loss   0.38\n",
      "\tEpoch 234 | Batch 0 | Loss   0.38\n",
      "\tEpoch 234 | Batch 0 | Acc   0.95\n",
      "Epoch 234 | Loss   0.38\n",
      "\tEpoch 235 | Batch 0 | Loss   0.38\n",
      "\tEpoch 235 | Batch 0 | Acc   0.97\n",
      "Epoch 235 | Loss   0.39\n",
      "\tEpoch 236 | Batch 0 | Loss   0.39\n",
      "\tEpoch 236 | Batch 0 | Acc   0.94\n",
      "Epoch 236 | Loss   0.39\n",
      "\tEpoch 237 | Batch 0 | Loss   0.38\n",
      "\tEpoch 237 | Batch 0 | Acc   0.95\n",
      "Epoch 237 | Loss   0.39\n",
      "\tEpoch 238 | Batch 0 | Loss   0.42\n",
      "\tEpoch 238 | Batch 0 | Acc   0.89\n",
      "Epoch 238 | Loss   0.38\n",
      "\tEpoch 239 | Batch 0 | Loss   0.37\n",
      "\tEpoch 239 | Batch 0 | Acc   0.98\n",
      "Epoch 239 | Loss   0.38\n",
      "\tEpoch 240 | Batch 0 | Loss   0.38\n",
      "\tEpoch 240 | Batch 0 | Acc   0.95\n",
      "Epoch 240 | Loss   0.38\n",
      "\tEpoch 241 | Batch 0 | Loss   0.38\n",
      "\tEpoch 241 | Batch 0 | Acc   0.94\n",
      "Epoch 241 | Loss   0.38\n",
      "\tEpoch 242 | Batch 0 | Loss   0.39\n",
      "\tEpoch 242 | Batch 0 | Acc   0.94\n",
      "Epoch 242 | Loss   0.38\n",
      "\tEpoch 243 | Batch 0 | Loss   0.39\n",
      "\tEpoch 243 | Batch 0 | Acc   0.94\n",
      "Epoch 243 | Loss   0.39\n",
      "\tEpoch 244 | Batch 0 | Loss   0.39\n",
      "\tEpoch 244 | Batch 0 | Acc   0.94\n",
      "Epoch 244 | Loss   0.38\n",
      "\tEpoch 245 | Batch 0 | Loss   0.37\n",
      "\tEpoch 245 | Batch 0 | Acc   0.95\n",
      "Epoch 245 | Loss   0.38\n",
      "\tEpoch 246 | Batch 0 | Loss   0.38\n",
      "\tEpoch 246 | Batch 0 | Acc   0.94\n",
      "Epoch 246 | Loss   0.38\n",
      "\tEpoch 247 | Batch 0 | Loss   0.38\n",
      "\tEpoch 247 | Batch 0 | Acc   0.95\n",
      "Epoch 247 | Loss   0.39\n",
      "\tEpoch 248 | Batch 0 | Loss   0.40\n",
      "\tEpoch 248 | Batch 0 | Acc   0.94\n",
      "Epoch 248 | Loss   0.38\n",
      "\tEpoch 249 | Batch 0 | Loss   0.39\n",
      "\tEpoch 249 | Batch 0 | Acc   0.92\n",
      "Epoch 249 | Loss   0.38\n",
      "\tEpoch 250 | Batch 0 | Loss   0.40\n",
      "\tEpoch 250 | Batch 0 | Acc   0.92\n",
      "Epoch 250 | Loss   0.39\n",
      "\tEpoch 251 | Batch 0 | Loss   0.37\n",
      "\tEpoch 251 | Batch 0 | Acc   0.95\n",
      "Epoch 251 | Loss   0.38\n",
      "\tEpoch 252 | Batch 0 | Loss   0.39\n",
      "\tEpoch 252 | Batch 0 | Acc   0.94\n",
      "Epoch 252 | Loss   0.38\n",
      "\tEpoch 253 | Batch 0 | Loss   0.41\n",
      "\tEpoch 253 | Batch 0 | Acc   0.92\n",
      "Epoch 253 | Loss   0.38\n",
      "\tEpoch 254 | Batch 0 | Loss   0.40\n",
      "\tEpoch 254 | Batch 0 | Acc   0.94\n",
      "Epoch 254 | Loss   0.38\n",
      "\tEpoch 255 | Batch 0 | Loss   0.38\n",
      "\tEpoch 255 | Batch 0 | Acc   0.92\n",
      "Epoch 255 | Loss   0.39\n",
      "\tEpoch 256 | Batch 0 | Loss   0.40\n",
      "\tEpoch 256 | Batch 0 | Acc   0.95\n",
      "Epoch 256 | Loss   0.38\n",
      "\tEpoch 257 | Batch 0 | Loss   0.36\n",
      "\tEpoch 257 | Batch 0 | Acc   0.98\n",
      "Epoch 257 | Loss   0.38\n",
      "\tEpoch 258 | Batch 0 | Loss   0.35\n",
      "\tEpoch 258 | Batch 0 | Acc   0.97\n",
      "Epoch 258 | Loss   0.39\n",
      "\tEpoch 259 | Batch 0 | Loss   0.41\n",
      "\tEpoch 259 | Batch 0 | Acc   0.91\n",
      "Epoch 259 | Loss   0.38\n",
      "\tEpoch 260 | Batch 0 | Loss   0.38\n",
      "\tEpoch 260 | Batch 0 | Acc   0.94\n",
      "Epoch 260 | Loss   0.39\n",
      "\tEpoch 261 | Batch 0 | Loss   0.39\n",
      "\tEpoch 261 | Batch 0 | Acc   0.94\n",
      "Epoch 261 | Loss   0.38\n",
      "\tEpoch 262 | Batch 0 | Loss   0.40\n",
      "\tEpoch 262 | Batch 0 | Acc   0.92\n",
      "Epoch 262 | Loss   0.37\n",
      "\tEpoch 263 | Batch 0 | Loss   0.38\n",
      "\tEpoch 263 | Batch 0 | Acc   0.94\n",
      "Epoch 263 | Loss   0.38\n",
      "\tEpoch 264 | Batch 0 | Loss   0.37\n",
      "\tEpoch 264 | Batch 0 | Acc   0.95\n",
      "Epoch 264 | Loss   0.39\n",
      "\tEpoch 265 | Batch 0 | Loss   0.37\n",
      "\tEpoch 265 | Batch 0 | Acc   0.95\n",
      "Epoch 265 | Loss   0.38\n",
      "\tEpoch 266 | Batch 0 | Loss   0.38\n",
      "\tEpoch 266 | Batch 0 | Acc   0.94\n",
      "Epoch 266 | Loss   0.38\n",
      "\tEpoch 267 | Batch 0 | Loss   0.37\n",
      "\tEpoch 267 | Batch 0 | Acc   0.95\n",
      "Epoch 267 | Loss   0.38\n",
      "\tEpoch 268 | Batch 0 | Loss   0.37\n",
      "\tEpoch 268 | Batch 0 | Acc   0.95\n",
      "Epoch 268 | Loss   0.38\n",
      "\tEpoch 269 | Batch 0 | Loss   0.39\n",
      "\tEpoch 269 | Batch 0 | Acc   0.95\n",
      "Epoch 269 | Loss   0.39\n",
      "\tEpoch 270 | Batch 0 | Loss   0.35\n",
      "\tEpoch 270 | Batch 0 | Acc   0.97\n",
      "Epoch 270 | Loss   0.38\n",
      "\tEpoch 271 | Batch 0 | Loss   0.41\n",
      "\tEpoch 271 | Batch 0 | Acc   0.92\n",
      "Epoch 271 | Loss   0.38\n",
      "\tEpoch 272 | Batch 0 | Loss   0.37\n",
      "\tEpoch 272 | Batch 0 | Acc   0.95\n",
      "Epoch 272 | Loss   0.38\n",
      "\tEpoch 273 | Batch 0 | Loss   0.39\n",
      "\tEpoch 273 | Batch 0 | Acc   0.95\n",
      "Epoch 273 | Loss   0.38\n",
      "\tEpoch 274 | Batch 0 | Loss   0.35\n",
      "\tEpoch 274 | Batch 0 | Acc   0.98\n",
      "Epoch 274 | Loss   0.37\n",
      "\tEpoch 275 | Batch 0 | Loss   0.40\n",
      "\tEpoch 275 | Batch 0 | Acc   0.94\n",
      "Epoch 275 | Loss   0.38\n",
      "\tEpoch 276 | Batch 0 | Loss   0.36\n",
      "\tEpoch 276 | Batch 0 | Acc   0.97\n",
      "Epoch 276 | Loss   0.38\n",
      "\tEpoch 277 | Batch 0 | Loss   0.40\n",
      "\tEpoch 277 | Batch 0 | Acc   0.94\n",
      "Epoch 277 | Loss   0.37\n",
      "\tEpoch 278 | Batch 0 | Loss   0.39\n",
      "\tEpoch 278 | Batch 0 | Acc   0.94\n",
      "Epoch 278 | Loss   0.37\n",
      "\tEpoch 279 | Batch 0 | Loss   0.40\n",
      "\tEpoch 279 | Batch 0 | Acc   0.94\n",
      "Epoch 279 | Loss   0.39\n",
      "\tEpoch 280 | Batch 0 | Loss   0.38\n",
      "\tEpoch 280 | Batch 0 | Acc   0.97\n",
      "Epoch 280 | Loss   0.37\n",
      "\tEpoch 281 | Batch 0 | Loss   0.36\n",
      "\tEpoch 281 | Batch 0 | Acc   0.98\n",
      "Epoch 281 | Loss   0.38\n",
      "\tEpoch 282 | Batch 0 | Loss   0.39\n",
      "\tEpoch 282 | Batch 0 | Acc   0.92\n",
      "Epoch 282 | Loss   0.38\n",
      "\tEpoch 283 | Batch 0 | Loss   0.36\n",
      "\tEpoch 283 | Batch 0 | Acc   0.97\n",
      "Epoch 283 | Loss   0.37\n",
      "\tEpoch 284 | Batch 0 | Loss   0.38\n",
      "\tEpoch 284 | Batch 0 | Acc   0.94\n",
      "Epoch 284 | Loss   0.37\n",
      "\tEpoch 285 | Batch 0 | Loss   0.38\n",
      "\tEpoch 285 | Batch 0 | Acc   0.95\n",
      "Epoch 285 | Loss   0.38\n",
      "\tEpoch 286 | Batch 0 | Loss   0.38\n",
      "\tEpoch 286 | Batch 0 | Acc   0.97\n",
      "Epoch 286 | Loss   0.37\n",
      "\tEpoch 287 | Batch 0 | Loss   0.38\n",
      "\tEpoch 287 | Batch 0 | Acc   0.95\n",
      "Epoch 287 | Loss   0.37\n",
      "\tEpoch 288 | Batch 0 | Loss   0.35\n",
      "\tEpoch 288 | Batch 0 | Acc   0.98\n",
      "Epoch 288 | Loss   0.38\n",
      "\tEpoch 289 | Batch 0 | Loss   0.35\n",
      "\tEpoch 289 | Batch 0 | Acc   0.98\n",
      "Epoch 289 | Loss   0.38\n",
      "\tEpoch 290 | Batch 0 | Loss   0.35\n",
      "\tEpoch 290 | Batch 0 | Acc   0.98\n",
      "Epoch 290 | Loss   0.37\n",
      "\tEpoch 291 | Batch 0 | Loss   0.37\n",
      "\tEpoch 291 | Batch 0 | Acc   0.95\n",
      "Epoch 291 | Loss   0.37\n",
      "\tEpoch 292 | Batch 0 | Loss   0.36\n",
      "\tEpoch 292 | Batch 0 | Acc   0.97\n",
      "Epoch 292 | Loss   0.37\n",
      "\tEpoch 293 | Batch 0 | Loss   0.36\n",
      "\tEpoch 293 | Batch 0 | Acc   0.97\n",
      "Epoch 293 | Loss   0.37\n",
      "\tEpoch 294 | Batch 0 | Loss   0.39\n",
      "\tEpoch 294 | Batch 0 | Acc   0.94\n",
      "Epoch 294 | Loss   0.37\n",
      "\tEpoch 295 | Batch 0 | Loss   0.36\n",
      "\tEpoch 295 | Batch 0 | Acc   0.98\n",
      "Epoch 295 | Loss   0.37\n",
      "\tEpoch 296 | Batch 0 | Loss   0.38\n",
      "\tEpoch 296 | Batch 0 | Acc   0.95\n",
      "Epoch 296 | Loss   0.38\n",
      "\tEpoch 297 | Batch 0 | Loss   0.38\n",
      "\tEpoch 297 | Batch 0 | Acc   0.97\n",
      "Epoch 297 | Loss   0.37\n",
      "\tEpoch 298 | Batch 0 | Loss   0.37\n",
      "\tEpoch 298 | Batch 0 | Acc   0.95\n",
      "Epoch 298 | Loss   0.38\n",
      "\tEpoch 299 | Batch 0 | Loss   0.35\n",
      "\tEpoch 299 | Batch 0 | Acc   0.98\n",
      "Epoch 299 | Loss   0.38\n",
      "\tEpoch 300 | Batch 0 | Loss   0.37\n",
      "\tEpoch 300 | Batch 0 | Acc   0.95\n",
      "Epoch 300 | Loss   0.37\n",
      "\tEpoch 301 | Batch 0 | Loss   0.39\n",
      "\tEpoch 301 | Batch 0 | Acc   0.95\n",
      "Epoch 301 | Loss   0.37\n",
      "\tEpoch 302 | Batch 0 | Loss   0.38\n",
      "\tEpoch 302 | Batch 0 | Acc   0.95\n",
      "Epoch 302 | Loss   0.37\n",
      "\tEpoch 303 | Batch 0 | Loss   0.37\n",
      "\tEpoch 303 | Batch 0 | Acc   0.95\n",
      "Epoch 303 | Loss   0.40\n",
      "\tEpoch 304 | Batch 0 | Loss   0.35\n",
      "\tEpoch 304 | Batch 0 | Acc   0.98\n",
      "Epoch 304 | Loss   0.38\n",
      "\tEpoch 305 | Batch 0 | Loss   0.38\n",
      "\tEpoch 305 | Batch 0 | Acc   0.94\n",
      "Epoch 305 | Loss   0.38\n",
      "\tEpoch 306 | Batch 0 | Loss   0.36\n",
      "\tEpoch 306 | Batch 0 | Acc   0.97\n",
      "Epoch 306 | Loss   0.37\n",
      "\tEpoch 307 | Batch 0 | Loss   0.39\n",
      "\tEpoch 307 | Batch 0 | Acc   0.94\n",
      "Epoch 307 | Loss   0.37\n",
      "\tEpoch 308 | Batch 0 | Loss   0.36\n",
      "\tEpoch 308 | Batch 0 | Acc   0.95\n",
      "Epoch 308 | Loss   0.38\n",
      "\tEpoch 309 | Batch 0 | Loss   0.39\n",
      "\tEpoch 309 | Batch 0 | Acc   0.95\n",
      "Epoch 309 | Loss   0.38\n",
      "\tEpoch 310 | Batch 0 | Loss   0.38\n",
      "\tEpoch 310 | Batch 0 | Acc   0.94\n",
      "Epoch 310 | Loss   0.38\n",
      "\tEpoch 311 | Batch 0 | Loss   0.37\n",
      "\tEpoch 311 | Batch 0 | Acc   0.95\n",
      "Epoch 311 | Loss   0.37\n",
      "\tEpoch 312 | Batch 0 | Loss   0.36\n",
      "\tEpoch 312 | Batch 0 | Acc   0.97\n",
      "Epoch 312 | Loss   0.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 313 | Batch 0 | Loss   0.40\n",
      "\tEpoch 313 | Batch 0 | Acc   0.92\n",
      "Epoch 313 | Loss   0.37\n",
      "\tEpoch 314 | Batch 0 | Loss   0.40\n",
      "\tEpoch 314 | Batch 0 | Acc   0.92\n",
      "Epoch 314 | Loss   0.38\n",
      "\tEpoch 315 | Batch 0 | Loss   0.37\n",
      "\tEpoch 315 | Batch 0 | Acc   0.95\n",
      "Epoch 315 | Loss   0.37\n",
      "\tEpoch 316 | Batch 0 | Loss   0.35\n",
      "\tEpoch 316 | Batch 0 | Acc   0.98\n",
      "Epoch 316 | Loss   0.37\n",
      "\tEpoch 317 | Batch 0 | Loss   0.36\n",
      "\tEpoch 317 | Batch 0 | Acc   0.97\n",
      "Epoch 317 | Loss   0.38\n",
      "\tEpoch 318 | Batch 0 | Loss   0.36\n",
      "\tEpoch 318 | Batch 0 | Acc   0.97\n",
      "Epoch 318 | Loss   0.37\n",
      "\tEpoch 319 | Batch 0 | Loss   0.41\n",
      "\tEpoch 319 | Batch 0 | Acc   0.89\n",
      "Epoch 319 | Loss   0.37\n",
      "\tEpoch 320 | Batch 0 | Loss   0.38\n",
      "\tEpoch 320 | Batch 0 | Acc   0.95\n",
      "Epoch 320 | Loss   0.37\n",
      "\tEpoch 321 | Batch 0 | Loss   0.37\n",
      "\tEpoch 321 | Batch 0 | Acc   0.95\n",
      "Epoch 321 | Loss   0.38\n",
      "\tEpoch 322 | Batch 0 | Loss   0.36\n",
      "\tEpoch 322 | Batch 0 | Acc   0.97\n",
      "Epoch 322 | Loss   0.37\n",
      "\tEpoch 323 | Batch 0 | Loss   0.35\n",
      "\tEpoch 323 | Batch 0 | Acc   0.98\n",
      "Epoch 323 | Loss   0.38\n",
      "\tEpoch 324 | Batch 0 | Loss   0.37\n",
      "\tEpoch 324 | Batch 0 | Acc   0.94\n",
      "Epoch 324 | Loss   0.37\n",
      "\tEpoch 325 | Batch 0 | Loss   0.39\n",
      "\tEpoch 325 | Batch 0 | Acc   0.95\n",
      "Epoch 325 | Loss   0.37\n",
      "\tEpoch 326 | Batch 0 | Loss   0.36\n",
      "\tEpoch 326 | Batch 0 | Acc   0.95\n",
      "Epoch 326 | Loss   0.37\n",
      "\tEpoch 327 | Batch 0 | Loss   0.37\n",
      "\tEpoch 327 | Batch 0 | Acc   0.95\n",
      "Epoch 327 | Loss   0.37\n",
      "\tEpoch 328 | Batch 0 | Loss   0.38\n",
      "\tEpoch 328 | Batch 0 | Acc   0.95\n",
      "Epoch 328 | Loss   0.37\n",
      "\tEpoch 329 | Batch 0 | Loss   0.34\n",
      "\tEpoch 329 | Batch 0 | Acc   0.98\n",
      "Epoch 329 | Loss   0.38\n",
      "\tEpoch 330 | Batch 0 | Loss   0.37\n",
      "\tEpoch 330 | Batch 0 | Acc   0.95\n",
      "Epoch 330 | Loss   0.37\n",
      "\tEpoch 331 | Batch 0 | Loss   0.38\n",
      "\tEpoch 331 | Batch 0 | Acc   0.95\n",
      "Epoch 331 | Loss   0.37\n",
      "\tEpoch 332 | Batch 0 | Loss   0.36\n",
      "\tEpoch 332 | Batch 0 | Acc   0.97\n",
      "Epoch 332 | Loss   0.37\n",
      "\tEpoch 333 | Batch 0 | Loss   0.42\n",
      "\tEpoch 333 | Batch 0 | Acc   0.91\n",
      "Epoch 333 | Loss   0.37\n",
      "\tEpoch 334 | Batch 0 | Loss   0.37\n",
      "\tEpoch 334 | Batch 0 | Acc   0.95\n",
      "Epoch 334 | Loss   0.37\n",
      "\tEpoch 335 | Batch 0 | Loss   0.36\n",
      "\tEpoch 335 | Batch 0 | Acc   0.97\n",
      "Epoch 335 | Loss   0.37\n",
      "\tEpoch 336 | Batch 0 | Loss   0.37\n",
      "\tEpoch 336 | Batch 0 | Acc   0.95\n",
      "Epoch 336 | Loss   0.38\n",
      "\tEpoch 337 | Batch 0 | Loss   0.37\n",
      "\tEpoch 337 | Batch 0 | Acc   0.98\n",
      "Epoch 337 | Loss   0.38\n",
      "\tEpoch 338 | Batch 0 | Loss   0.38\n",
      "\tEpoch 338 | Batch 0 | Acc   0.94\n",
      "Epoch 338 | Loss   0.38\n",
      "\tEpoch 339 | Batch 0 | Loss   0.34\n",
      "\tEpoch 339 | Batch 0 | Acc   0.98\n",
      "Epoch 339 | Loss   0.38\n",
      "\tEpoch 340 | Batch 0 | Loss   0.36\n",
      "\tEpoch 340 | Batch 0 | Acc   0.97\n",
      "Epoch 340 | Loss   0.37\n",
      "\tEpoch 341 | Batch 0 | Loss   0.36\n",
      "\tEpoch 341 | Batch 0 | Acc   0.97\n",
      "Epoch 341 | Loss   0.38\n",
      "\tEpoch 342 | Batch 0 | Loss   0.40\n",
      "\tEpoch 342 | Batch 0 | Acc   0.92\n",
      "Epoch 342 | Loss   0.38\n",
      "\tEpoch 343 | Batch 0 | Loss   0.37\n",
      "\tEpoch 343 | Batch 0 | Acc   0.97\n",
      "Epoch 343 | Loss   0.37\n",
      "\tEpoch 344 | Batch 0 | Loss   0.39\n",
      "\tEpoch 344 | Batch 0 | Acc   0.91\n",
      "Epoch 344 | Loss   0.38\n",
      "\tEpoch 345 | Batch 0 | Loss   0.35\n",
      "\tEpoch 345 | Batch 0 | Acc   0.98\n",
      "Epoch 345 | Loss   0.37\n",
      "\tEpoch 346 | Batch 0 | Loss   0.35\n",
      "\tEpoch 346 | Batch 0 | Acc   0.98\n",
      "Epoch 346 | Loss   0.38\n",
      "\tEpoch 347 | Batch 0 | Loss   0.35\n",
      "\tEpoch 347 | Batch 0 | Acc   0.97\n",
      "Epoch 347 | Loss   0.37\n",
      "\tEpoch 348 | Batch 0 | Loss   0.36\n",
      "\tEpoch 348 | Batch 0 | Acc   0.95\n",
      "Epoch 348 | Loss   0.37\n",
      "\tEpoch 349 | Batch 0 | Loss   0.34\n",
      "\tEpoch 349 | Batch 0 | Acc   1.00\n",
      "Epoch 349 | Loss   0.37\n",
      "\tEpoch 350 | Batch 0 | Loss   0.36\n",
      "\tEpoch 350 | Batch 0 | Acc   0.95\n",
      "Epoch 350 | Loss   0.37\n",
      "\tEpoch 351 | Batch 0 | Loss   0.38\n",
      "\tEpoch 351 | Batch 0 | Acc   0.95\n",
      "Epoch 351 | Loss   0.37\n",
      "\tEpoch 352 | Batch 0 | Loss   0.40\n",
      "\tEpoch 352 | Batch 0 | Acc   0.92\n",
      "Epoch 352 | Loss   0.37\n",
      "\tEpoch 353 | Batch 0 | Loss   0.38\n",
      "\tEpoch 353 | Batch 0 | Acc   0.95\n",
      "Epoch 353 | Loss   0.37\n",
      "\tEpoch 354 | Batch 0 | Loss   0.38\n",
      "\tEpoch 354 | Batch 0 | Acc   0.95\n",
      "Epoch 354 | Loss   0.37\n",
      "\tEpoch 355 | Batch 0 | Loss   0.37\n",
      "\tEpoch 355 | Batch 0 | Acc   0.95\n",
      "Epoch 355 | Loss   0.38\n",
      "\tEpoch 356 | Batch 0 | Loss   0.38\n",
      "\tEpoch 356 | Batch 0 | Acc   0.95\n",
      "Epoch 356 | Loss   0.38\n",
      "\tEpoch 357 | Batch 0 | Loss   0.39\n",
      "\tEpoch 357 | Batch 0 | Acc   0.92\n",
      "Epoch 357 | Loss   0.38\n",
      "\tEpoch 358 | Batch 0 | Loss   0.38\n",
      "\tEpoch 358 | Batch 0 | Acc   0.92\n",
      "Epoch 358 | Loss   0.38\n",
      "\tEpoch 359 | Batch 0 | Loss   0.39\n",
      "\tEpoch 359 | Batch 0 | Acc   0.92\n",
      "Epoch 359 | Loss   0.37\n",
      "\tEpoch 360 | Batch 0 | Loss   0.36\n",
      "\tEpoch 360 | Batch 0 | Acc   0.98\n",
      "Epoch 360 | Loss   0.37\n",
      "\tEpoch 361 | Batch 0 | Loss   0.36\n",
      "\tEpoch 361 | Batch 0 | Acc   0.95\n",
      "Epoch 361 | Loss   0.38\n",
      "\tEpoch 362 | Batch 0 | Loss   0.38\n",
      "\tEpoch 362 | Batch 0 | Acc   0.94\n",
      "Epoch 362 | Loss   0.38\n",
      "\tEpoch 363 | Batch 0 | Loss   0.36\n",
      "\tEpoch 363 | Batch 0 | Acc   0.97\n",
      "Epoch 363 | Loss   0.37\n",
      "\tEpoch 364 | Batch 0 | Loss   0.38\n",
      "\tEpoch 364 | Batch 0 | Acc   0.94\n",
      "Epoch 364 | Loss   0.37\n",
      "\tEpoch 365 | Batch 0 | Loss   0.36\n",
      "\tEpoch 365 | Batch 0 | Acc   0.97\n",
      "Epoch 365 | Loss   0.38\n",
      "\tEpoch 366 | Batch 0 | Loss   0.38\n",
      "\tEpoch 366 | Batch 0 | Acc   0.92\n",
      "Epoch 366 | Loss   0.38\n",
      "\tEpoch 367 | Batch 0 | Loss   0.35\n",
      "\tEpoch 367 | Batch 0 | Acc   0.97\n",
      "Epoch 367 | Loss   0.37\n",
      "\tEpoch 368 | Batch 0 | Loss   0.37\n",
      "\tEpoch 368 | Batch 0 | Acc   0.95\n",
      "Epoch 368 | Loss   0.37\n",
      "\tEpoch 369 | Batch 0 | Loss   0.36\n",
      "\tEpoch 369 | Batch 0 | Acc   0.98\n",
      "Epoch 369 | Loss   0.37\n",
      "\tEpoch 370 | Batch 0 | Loss   0.40\n",
      "\tEpoch 370 | Batch 0 | Acc   0.92\n",
      "Epoch 370 | Loss   0.37\n",
      "\tEpoch 371 | Batch 0 | Loss   0.36\n",
      "\tEpoch 371 | Batch 0 | Acc   0.95\n",
      "Epoch 371 | Loss   0.37\n",
      "\tEpoch 372 | Batch 0 | Loss   0.35\n",
      "\tEpoch 372 | Batch 0 | Acc   0.98\n",
      "Epoch 372 | Loss   0.37\n",
      "\tEpoch 373 | Batch 0 | Loss   0.38\n",
      "\tEpoch 373 | Batch 0 | Acc   0.94\n",
      "Epoch 373 | Loss   0.37\n",
      "\tEpoch 374 | Batch 0 | Loss   0.40\n",
      "\tEpoch 374 | Batch 0 | Acc   0.91\n",
      "Epoch 374 | Loss   0.37\n",
      "\tEpoch 375 | Batch 0 | Loss   0.36\n",
      "\tEpoch 375 | Batch 0 | Acc   0.97\n",
      "Epoch 375 | Loss   0.37\n",
      "\tEpoch 376 | Batch 0 | Loss   0.36\n",
      "\tEpoch 376 | Batch 0 | Acc   0.97\n",
      "Epoch 376 | Loss   0.37\n",
      "\tEpoch 377 | Batch 0 | Loss   0.36\n",
      "\tEpoch 377 | Batch 0 | Acc   0.95\n",
      "Epoch 377 | Loss   0.37\n",
      "\tEpoch 378 | Batch 0 | Loss   0.36\n",
      "\tEpoch 378 | Batch 0 | Acc   0.97\n",
      "Epoch 378 | Loss   0.37\n",
      "\tEpoch 379 | Batch 0 | Loss   0.36\n",
      "\tEpoch 379 | Batch 0 | Acc   0.97\n",
      "Epoch 379 | Loss   0.37\n",
      "\tEpoch 380 | Batch 0 | Loss   0.40\n",
      "\tEpoch 380 | Batch 0 | Acc   0.92\n",
      "Epoch 380 | Loss   0.37\n",
      "\tEpoch 381 | Batch 0 | Loss   0.35\n",
      "\tEpoch 381 | Batch 0 | Acc   0.97\n",
      "Epoch 381 | Loss   0.38\n",
      "\tEpoch 382 | Batch 0 | Loss   0.34\n",
      "\tEpoch 382 | Batch 0 | Acc   0.98\n",
      "Epoch 382 | Loss   0.37\n",
      "\tEpoch 383 | Batch 0 | Loss   0.40\n",
      "\tEpoch 383 | Batch 0 | Acc   0.94\n",
      "Epoch 383 | Loss   0.37\n",
      "\tEpoch 384 | Batch 0 | Loss   0.35\n",
      "\tEpoch 384 | Batch 0 | Acc   0.98\n",
      "Epoch 384 | Loss   0.37\n",
      "\tEpoch 385 | Batch 0 | Loss   0.36\n",
      "\tEpoch 385 | Batch 0 | Acc   0.95\n",
      "Epoch 385 | Loss   0.37\n",
      "\tEpoch 386 | Batch 0 | Loss   0.34\n",
      "\tEpoch 386 | Batch 0 | Acc   0.98\n",
      "Epoch 386 | Loss   0.37\n",
      "\tEpoch 387 | Batch 0 | Loss   0.34\n",
      "\tEpoch 387 | Batch 0 | Acc   0.98\n",
      "Epoch 387 | Loss   0.37\n",
      "\tEpoch 388 | Batch 0 | Loss   0.37\n",
      "\tEpoch 388 | Batch 0 | Acc   0.95\n",
      "Epoch 388 | Loss   0.37\n",
      "\tEpoch 389 | Batch 0 | Loss   0.38\n",
      "\tEpoch 389 | Batch 0 | Acc   0.94\n",
      "Epoch 389 | Loss   0.37\n",
      "\tEpoch 390 | Batch 0 | Loss   0.39\n",
      "\tEpoch 390 | Batch 0 | Acc   0.94\n",
      "Epoch 390 | Loss   0.37\n",
      "\tEpoch 391 | Batch 0 | Loss   0.36\n",
      "\tEpoch 391 | Batch 0 | Acc   0.97\n",
      "Epoch 391 | Loss   0.37\n",
      "\tEpoch 392 | Batch 0 | Loss   0.37\n",
      "\tEpoch 392 | Batch 0 | Acc   0.97\n",
      "Epoch 392 | Loss   0.37\n",
      "\tEpoch 393 | Batch 0 | Loss   0.36\n",
      "\tEpoch 393 | Batch 0 | Acc   0.94\n",
      "Epoch 393 | Loss   0.38\n",
      "\tEpoch 394 | Batch 0 | Loss   0.36\n",
      "\tEpoch 394 | Batch 0 | Acc   0.97\n",
      "Epoch 394 | Loss   0.37\n",
      "\tEpoch 395 | Batch 0 | Loss   0.36\n",
      "\tEpoch 395 | Batch 0 | Acc   0.97\n",
      "Epoch 395 | Loss   0.39\n",
      "\tEpoch 396 | Batch 0 | Loss   0.34\n",
      "\tEpoch 396 | Batch 0 | Acc   0.98\n",
      "Epoch 396 | Loss   0.37\n",
      "\tEpoch 397 | Batch 0 | Loss   0.37\n",
      "\tEpoch 397 | Batch 0 | Acc   0.97\n",
      "Epoch 397 | Loss   0.37\n",
      "\tEpoch 398 | Batch 0 | Loss   0.36\n",
      "\tEpoch 398 | Batch 0 | Acc   0.97\n",
      "Epoch 398 | Loss   0.37\n",
      "\tEpoch 399 | Batch 0 | Loss   0.39\n",
      "\tEpoch 399 | Batch 0 | Acc   0.94\n",
      "Epoch 399 | Loss   0.37\n",
      "\tEpoch 400 | Batch 0 | Loss   0.36\n",
      "\tEpoch 400 | Batch 0 | Acc   0.95\n",
      "Epoch 400 | Loss   0.37\n",
      "\tEpoch 401 | Batch 0 | Loss   0.40\n",
      "\tEpoch 401 | Batch 0 | Acc   0.92\n",
      "Epoch 401 | Loss   0.37\n",
      "\tEpoch 402 | Batch 0 | Loss   0.35\n",
      "\tEpoch 402 | Batch 0 | Acc   0.97\n",
      "Epoch 402 | Loss   0.37\n",
      "\tEpoch 403 | Batch 0 | Loss   0.41\n",
      "\tEpoch 403 | Batch 0 | Acc   0.89\n",
      "Epoch 403 | Loss   0.38\n",
      "\tEpoch 404 | Batch 0 | Loss   0.38\n",
      "\tEpoch 404 | Batch 0 | Acc   0.95\n",
      "Epoch 404 | Loss   0.38\n",
      "\tEpoch 405 | Batch 0 | Loss   0.36\n",
      "\tEpoch 405 | Batch 0 | Acc   0.98\n",
      "Epoch 405 | Loss   0.38\n",
      "\tEpoch 406 | Batch 0 | Loss   0.39\n",
      "\tEpoch 406 | Batch 0 | Acc   0.94\n",
      "Epoch 406 | Loss   0.37\n",
      "\tEpoch 407 | Batch 0 | Loss   0.38\n",
      "\tEpoch 407 | Batch 0 | Acc   0.92\n",
      "Epoch 407 | Loss   0.38\n",
      "\tEpoch 408 | Batch 0 | Loss   0.39\n",
      "\tEpoch 408 | Batch 0 | Acc   0.94\n",
      "Epoch 408 | Loss   0.38\n",
      "\tEpoch 409 | Batch 0 | Loss   0.40\n",
      "\tEpoch 409 | Batch 0 | Acc   0.92\n",
      "Epoch 409 | Loss   0.37\n",
      "\tEpoch 410 | Batch 0 | Loss   0.39\n",
      "\tEpoch 410 | Batch 0 | Acc   0.95\n",
      "Epoch 410 | Loss   0.36\n",
      "\tEpoch 411 | Batch 0 | Loss   0.34\n",
      "\tEpoch 411 | Batch 0 | Acc   0.98\n",
      "Epoch 411 | Loss   0.37\n",
      "\tEpoch 412 | Batch 0 | Loss   0.35\n",
      "\tEpoch 412 | Batch 0 | Acc   0.98\n",
      "Epoch 412 | Loss   0.37\n",
      "\tEpoch 413 | Batch 0 | Loss   0.35\n",
      "\tEpoch 413 | Batch 0 | Acc   0.97\n",
      "Epoch 413 | Loss   0.37\n",
      "\tEpoch 414 | Batch 0 | Loss   0.36\n",
      "\tEpoch 414 | Batch 0 | Acc   0.95\n",
      "Epoch 414 | Loss   0.37\n",
      "\tEpoch 415 | Batch 0 | Loss   0.37\n",
      "\tEpoch 415 | Batch 0 | Acc   0.95\n",
      "Epoch 415 | Loss   0.36\n",
      "\tEpoch 416 | Batch 0 | Loss   0.38\n",
      "\tEpoch 416 | Batch 0 | Acc   0.95\n",
      "Epoch 416 | Loss   0.37\n",
      "\tEpoch 417 | Batch 0 | Loss   0.39\n",
      "\tEpoch 417 | Batch 0 | Acc   0.94\n",
      "Epoch 417 | Loss   0.38\n",
      "\tEpoch 418 | Batch 0 | Loss   0.34\n",
      "\tEpoch 418 | Batch 0 | Acc   0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 418 | Loss   0.37\n",
      "\tEpoch 419 | Batch 0 | Loss   0.35\n",
      "\tEpoch 419 | Batch 0 | Acc   0.97\n",
      "Epoch 419 | Loss   0.38\n",
      "\tEpoch 420 | Batch 0 | Loss   0.35\n",
      "\tEpoch 420 | Batch 0 | Acc   0.97\n",
      "Epoch 420 | Loss   0.37\n",
      "\tEpoch 421 | Batch 0 | Loss   0.36\n",
      "\tEpoch 421 | Batch 0 | Acc   0.95\n",
      "Epoch 421 | Loss   0.36\n",
      "\tEpoch 422 | Batch 0 | Loss   0.37\n",
      "\tEpoch 422 | Batch 0 | Acc   0.95\n",
      "Epoch 422 | Loss   0.36\n",
      "\tEpoch 423 | Batch 0 | Loss   0.38\n",
      "\tEpoch 423 | Batch 0 | Acc   0.94\n",
      "Epoch 423 | Loss   0.38\n",
      "\tEpoch 424 | Batch 0 | Loss   0.38\n",
      "\tEpoch 424 | Batch 0 | Acc   0.95\n",
      "Epoch 424 | Loss   0.37\n",
      "\tEpoch 425 | Batch 0 | Loss   0.40\n",
      "\tEpoch 425 | Batch 0 | Acc   0.92\n",
      "Epoch 425 | Loss   0.36\n",
      "\tEpoch 426 | Batch 0 | Loss   0.36\n",
      "\tEpoch 426 | Batch 0 | Acc   0.97\n",
      "Epoch 426 | Loss   0.36\n",
      "\tEpoch 427 | Batch 0 | Loss   0.34\n",
      "\tEpoch 427 | Batch 0 | Acc   0.98\n",
      "Epoch 427 | Loss   0.36\n",
      "\tEpoch 428 | Batch 0 | Loss   0.38\n",
      "\tEpoch 428 | Batch 0 | Acc   0.95\n",
      "Epoch 428 | Loss   0.36\n",
      "\tEpoch 429 | Batch 0 | Loss   0.36\n",
      "\tEpoch 429 | Batch 0 | Acc   0.97\n",
      "Epoch 429 | Loss   0.37\n",
      "\tEpoch 430 | Batch 0 | Loss   0.34\n",
      "\tEpoch 430 | Batch 0 | Acc   0.98\n",
      "Epoch 430 | Loss   0.36\n",
      "\tEpoch 431 | Batch 0 | Loss   0.35\n",
      "\tEpoch 431 | Batch 0 | Acc   0.98\n",
      "Epoch 431 | Loss   0.37\n",
      "\tEpoch 432 | Batch 0 | Loss   0.35\n",
      "\tEpoch 432 | Batch 0 | Acc   0.97\n",
      "Epoch 432 | Loss   0.37\n",
      "\tEpoch 433 | Batch 0 | Loss   0.36\n",
      "\tEpoch 433 | Batch 0 | Acc   0.95\n",
      "Epoch 433 | Loss   0.37\n",
      "\tEpoch 434 | Batch 0 | Loss   0.37\n",
      "\tEpoch 434 | Batch 0 | Acc   0.95\n",
      "Epoch 434 | Loss   0.36\n",
      "\tEpoch 435 | Batch 0 | Loss   0.36\n",
      "\tEpoch 435 | Batch 0 | Acc   0.95\n",
      "Epoch 435 | Loss   0.37\n",
      "\tEpoch 436 | Batch 0 | Loss   0.36\n",
      "\tEpoch 436 | Batch 0 | Acc   0.97\n",
      "Epoch 436 | Loss   0.36\n",
      "\tEpoch 437 | Batch 0 | Loss   0.36\n",
      "\tEpoch 437 | Batch 0 | Acc   0.95\n",
      "Epoch 437 | Loss   0.36\n",
      "\tEpoch 438 | Batch 0 | Loss   0.38\n",
      "\tEpoch 438 | Batch 0 | Acc   0.94\n",
      "Epoch 438 | Loss   0.36\n",
      "\tEpoch 439 | Batch 0 | Loss   0.40\n",
      "\tEpoch 439 | Batch 0 | Acc   0.92\n",
      "Epoch 439 | Loss   0.37\n",
      "\tEpoch 440 | Batch 0 | Loss   0.34\n",
      "\tEpoch 440 | Batch 0 | Acc   0.98\n",
      "Epoch 440 | Loss   0.36\n",
      "\tEpoch 441 | Batch 0 | Loss   0.36\n",
      "\tEpoch 441 | Batch 0 | Acc   0.95\n",
      "Epoch 441 | Loss   0.38\n",
      "\tEpoch 442 | Batch 0 | Loss   0.38\n",
      "\tEpoch 442 | Batch 0 | Acc   0.94\n",
      "Epoch 442 | Loss   0.37\n",
      "\tEpoch 443 | Batch 0 | Loss   0.37\n",
      "\tEpoch 443 | Batch 0 | Acc   0.95\n",
      "Epoch 443 | Loss   0.38\n",
      "\tEpoch 444 | Batch 0 | Loss   0.33\n",
      "\tEpoch 444 | Batch 0 | Acc   0.98\n",
      "Epoch 444 | Loss   0.37\n",
      "\tEpoch 445 | Batch 0 | Loss   0.36\n",
      "\tEpoch 445 | Batch 0 | Acc   0.97\n",
      "Epoch 445 | Loss   0.36\n",
      "\tEpoch 446 | Batch 0 | Loss   0.37\n",
      "\tEpoch 446 | Batch 0 | Acc   0.95\n",
      "Epoch 446 | Loss   0.37\n",
      "\tEpoch 447 | Batch 0 | Loss   0.39\n",
      "\tEpoch 447 | Batch 0 | Acc   0.92\n",
      "Epoch 447 | Loss   0.38\n",
      "\tEpoch 448 | Batch 0 | Loss   0.34\n",
      "\tEpoch 448 | Batch 0 | Acc   1.00\n",
      "Epoch 448 | Loss   0.36\n",
      "\tEpoch 449 | Batch 0 | Loss   0.36\n",
      "\tEpoch 449 | Batch 0 | Acc   0.94\n",
      "Epoch 449 | Loss   0.36\n",
      "\tEpoch 450 | Batch 0 | Loss   0.37\n",
      "\tEpoch 450 | Batch 0 | Acc   0.95\n",
      "Epoch 450 | Loss   0.36\n",
      "\tEpoch 451 | Batch 0 | Loss   0.38\n",
      "\tEpoch 451 | Batch 0 | Acc   0.94\n",
      "Epoch 451 | Loss   0.37\n",
      "\tEpoch 452 | Batch 0 | Loss   0.34\n",
      "\tEpoch 452 | Batch 0 | Acc   0.98\n",
      "Epoch 452 | Loss   0.36\n",
      "\tEpoch 453 | Batch 0 | Loss   0.36\n",
      "\tEpoch 453 | Batch 0 | Acc   0.97\n",
      "Epoch 453 | Loss   0.37\n",
      "\tEpoch 454 | Batch 0 | Loss   0.35\n",
      "\tEpoch 454 | Batch 0 | Acc   0.97\n",
      "Epoch 454 | Loss   0.38\n",
      "\tEpoch 455 | Batch 0 | Loss   0.37\n",
      "\tEpoch 455 | Batch 0 | Acc   0.94\n",
      "Epoch 455 | Loss   0.37\n",
      "\tEpoch 456 | Batch 0 | Loss   0.36\n",
      "\tEpoch 456 | Batch 0 | Acc   0.97\n",
      "Epoch 456 | Loss   0.37\n",
      "\tEpoch 457 | Batch 0 | Loss   0.33\n",
      "\tEpoch 457 | Batch 0 | Acc   1.00\n",
      "Epoch 457 | Loss   0.37\n",
      "\tEpoch 458 | Batch 0 | Loss   0.36\n",
      "\tEpoch 458 | Batch 0 | Acc   0.97\n",
      "Epoch 458 | Loss   0.38\n",
      "\tEpoch 459 | Batch 0 | Loss   0.36\n",
      "\tEpoch 459 | Batch 0 | Acc   0.95\n",
      "Epoch 459 | Loss   0.37\n",
      "\tEpoch 460 | Batch 0 | Loss   0.36\n",
      "\tEpoch 460 | Batch 0 | Acc   0.97\n",
      "Epoch 460 | Loss   0.38\n",
      "\tEpoch 461 | Batch 0 | Loss   0.35\n",
      "\tEpoch 461 | Batch 0 | Acc   1.00\n",
      "Epoch 461 | Loss   0.36\n",
      "\tEpoch 462 | Batch 0 | Loss   0.35\n",
      "\tEpoch 462 | Batch 0 | Acc   0.98\n",
      "Epoch 462 | Loss   0.37\n",
      "\tEpoch 463 | Batch 0 | Loss   0.37\n",
      "\tEpoch 463 | Batch 0 | Acc   0.94\n",
      "Epoch 463 | Loss   0.36\n",
      "\tEpoch 464 | Batch 0 | Loss   0.39\n",
      "\tEpoch 464 | Batch 0 | Acc   0.91\n",
      "Epoch 464 | Loss   0.37\n",
      "\tEpoch 465 | Batch 0 | Loss   0.39\n",
      "\tEpoch 465 | Batch 0 | Acc   0.94\n",
      "Epoch 465 | Loss   0.37\n",
      "\tEpoch 466 | Batch 0 | Loss   0.36\n",
      "\tEpoch 466 | Batch 0 | Acc   0.95\n",
      "Epoch 466 | Loss   0.36\n",
      "\tEpoch 467 | Batch 0 | Loss   0.34\n",
      "\tEpoch 467 | Batch 0 | Acc   0.98\n",
      "Epoch 467 | Loss   0.36\n",
      "\tEpoch 468 | Batch 0 | Loss   0.36\n",
      "\tEpoch 468 | Batch 0 | Acc   0.97\n",
      "Epoch 468 | Loss   0.37\n",
      "\tEpoch 469 | Batch 0 | Loss   0.35\n",
      "\tEpoch 469 | Batch 0 | Acc   0.97\n",
      "Epoch 469 | Loss   0.36\n",
      "\tEpoch 470 | Batch 0 | Loss   0.37\n",
      "\tEpoch 470 | Batch 0 | Acc   0.94\n",
      "Epoch 470 | Loss   0.37\n",
      "\tEpoch 471 | Batch 0 | Loss   0.40\n",
      "\tEpoch 471 | Batch 0 | Acc   0.91\n",
      "Epoch 471 | Loss   0.36\n",
      "\tEpoch 472 | Batch 0 | Loss   0.34\n",
      "\tEpoch 472 | Batch 0 | Acc   0.98\n",
      "Epoch 472 | Loss   0.36\n",
      "\tEpoch 473 | Batch 0 | Loss   0.40\n",
      "\tEpoch 473 | Batch 0 | Acc   0.91\n",
      "Epoch 473 | Loss   0.37\n",
      "\tEpoch 474 | Batch 0 | Loss   0.34\n",
      "\tEpoch 474 | Batch 0 | Acc   0.98\n",
      "Epoch 474 | Loss   0.37\n",
      "\tEpoch 475 | Batch 0 | Loss   0.35\n",
      "\tEpoch 475 | Batch 0 | Acc   0.97\n",
      "Epoch 475 | Loss   0.36\n",
      "\tEpoch 476 | Batch 0 | Loss   0.39\n",
      "\tEpoch 476 | Batch 0 | Acc   0.94\n",
      "Epoch 476 | Loss   0.36\n",
      "\tEpoch 477 | Batch 0 | Loss   0.35\n",
      "\tEpoch 477 | Batch 0 | Acc   0.97\n",
      "Epoch 477 | Loss   0.37\n",
      "\tEpoch 478 | Batch 0 | Loss   0.38\n",
      "\tEpoch 478 | Batch 0 | Acc   0.95\n",
      "Epoch 478 | Loss   0.36\n",
      "\tEpoch 479 | Batch 0 | Loss   0.35\n",
      "\tEpoch 479 | Batch 0 | Acc   0.98\n",
      "Epoch 479 | Loss   0.37\n",
      "\tEpoch 480 | Batch 0 | Loss   0.37\n",
      "\tEpoch 480 | Batch 0 | Acc   0.95\n",
      "Epoch 480 | Loss   0.36\n",
      "\tEpoch 481 | Batch 0 | Loss   0.37\n",
      "\tEpoch 481 | Batch 0 | Acc   0.95\n",
      "Epoch 481 | Loss   0.36\n",
      "\tEpoch 482 | Batch 0 | Loss   0.37\n",
      "\tEpoch 482 | Batch 0 | Acc   0.95\n",
      "Epoch 482 | Loss   0.38\n",
      "\tEpoch 483 | Batch 0 | Loss   0.39\n",
      "\tEpoch 483 | Batch 0 | Acc   0.92\n",
      "Epoch 483 | Loss   0.36\n",
      "\tEpoch 484 | Batch 0 | Loss   0.34\n",
      "\tEpoch 484 | Batch 0 | Acc   0.98\n",
      "Epoch 484 | Loss   0.37\n",
      "\tEpoch 485 | Batch 0 | Loss   0.38\n",
      "\tEpoch 485 | Batch 0 | Acc   0.94\n",
      "Epoch 485 | Loss   0.36\n",
      "\tEpoch 486 | Batch 0 | Loss   0.32\n",
      "\tEpoch 486 | Batch 0 | Acc   1.00\n",
      "Epoch 486 | Loss   0.36\n",
      "\tEpoch 487 | Batch 0 | Loss   0.35\n",
      "\tEpoch 487 | Batch 0 | Acc   0.98\n",
      "Epoch 487 | Loss   0.38\n",
      "\tEpoch 488 | Batch 0 | Loss   0.35\n",
      "\tEpoch 488 | Batch 0 | Acc   0.97\n",
      "Epoch 488 | Loss   0.36\n",
      "\tEpoch 489 | Batch 0 | Loss   0.38\n",
      "\tEpoch 489 | Batch 0 | Acc   0.92\n",
      "Epoch 489 | Loss   0.36\n",
      "\tEpoch 490 | Batch 0 | Loss   0.36\n",
      "\tEpoch 490 | Batch 0 | Acc   0.97\n",
      "Epoch 490 | Loss   0.36\n",
      "\tEpoch 491 | Batch 0 | Loss   0.36\n",
      "\tEpoch 491 | Batch 0 | Acc   0.97\n",
      "Epoch 491 | Loss   0.36\n",
      "\tEpoch 492 | Batch 0 | Loss   0.37\n",
      "\tEpoch 492 | Batch 0 | Acc   0.95\n",
      "Epoch 492 | Loss   0.38\n",
      "\tEpoch 493 | Batch 0 | Loss   0.39\n",
      "\tEpoch 493 | Batch 0 | Acc   0.92\n",
      "Epoch 493 | Loss   0.38\n",
      "\tEpoch 494 | Batch 0 | Loss   0.40\n",
      "\tEpoch 494 | Batch 0 | Acc   0.92\n",
      "Epoch 494 | Loss   0.36\n",
      "\tEpoch 495 | Batch 0 | Loss   0.36\n",
      "\tEpoch 495 | Batch 0 | Acc   0.95\n",
      "Epoch 495 | Loss   0.36\n",
      "\tEpoch 496 | Batch 0 | Loss   0.34\n",
      "\tEpoch 496 | Batch 0 | Acc   0.98\n",
      "Epoch 496 | Loss   0.36\n",
      "\tEpoch 497 | Batch 0 | Loss   0.39\n",
      "\tEpoch 497 | Batch 0 | Acc   0.92\n",
      "Epoch 497 | Loss   0.39\n",
      "\tEpoch 498 | Batch 0 | Loss   0.37\n",
      "\tEpoch 498 | Batch 0 | Acc   0.94\n",
      "Epoch 498 | Loss   0.37\n",
      "\tEpoch 499 | Batch 0 | Loss   0.34\n",
      "\tEpoch 499 | Batch 0 | Acc   0.98\n",
      "Epoch 499 | Loss   0.36\n",
      "\tEpoch 500 | Batch 0 | Loss   0.37\n",
      "\tEpoch 500 | Batch 0 | Acc   0.95\n",
      "Epoch 500 | Loss   0.38\n",
      "\tEpoch 501 | Batch 0 | Loss   0.36\n",
      "\tEpoch 501 | Batch 0 | Acc   0.95\n",
      "Epoch 501 | Loss   0.37\n",
      "\tEpoch 502 | Batch 0 | Loss   0.36\n",
      "\tEpoch 502 | Batch 0 | Acc   0.95\n",
      "Epoch 502 | Loss   0.36\n",
      "\tEpoch 503 | Batch 0 | Loss   0.36\n",
      "\tEpoch 503 | Batch 0 | Acc   0.95\n",
      "Epoch 503 | Loss   0.37\n",
      "\tEpoch 504 | Batch 0 | Loss   0.34\n",
      "\tEpoch 504 | Batch 0 | Acc   0.98\n",
      "Epoch 504 | Loss   0.37\n",
      "\tEpoch 505 | Batch 0 | Loss   0.40\n",
      "\tEpoch 505 | Batch 0 | Acc   0.94\n",
      "Epoch 505 | Loss   0.36\n",
      "\tEpoch 506 | Batch 0 | Loss   0.35\n",
      "\tEpoch 506 | Batch 0 | Acc   0.97\n",
      "Epoch 506 | Loss   0.36\n",
      "\tEpoch 507 | Batch 0 | Loss   0.38\n",
      "\tEpoch 507 | Batch 0 | Acc   0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 507 | Loss   0.36\n",
      "\tEpoch 508 | Batch 0 | Loss   0.35\n",
      "\tEpoch 508 | Batch 0 | Acc   0.97\n",
      "Epoch 508 | Loss   0.37\n",
      "\tEpoch 509 | Batch 0 | Loss   0.38\n",
      "\tEpoch 509 | Batch 0 | Acc   0.94\n",
      "Epoch 509 | Loss   0.36\n",
      "\tEpoch 510 | Batch 0 | Loss   0.36\n",
      "\tEpoch 510 | Batch 0 | Acc   0.95\n",
      "Epoch 510 | Loss   0.36\n",
      "\tEpoch 511 | Batch 0 | Loss   0.35\n",
      "\tEpoch 511 | Batch 0 | Acc   0.97\n",
      "Epoch 511 | Loss   0.36\n",
      "\tEpoch 512 | Batch 0 | Loss   0.39\n",
      "\tEpoch 512 | Batch 0 | Acc   0.92\n",
      "Epoch 512 | Loss   0.36\n",
      "\tEpoch 513 | Batch 0 | Loss   0.35\n",
      "\tEpoch 513 | Batch 0 | Acc   0.95\n",
      "Epoch 513 | Loss   0.36\n",
      "\tEpoch 514 | Batch 0 | Loss   0.39\n",
      "\tEpoch 514 | Batch 0 | Acc   0.94\n",
      "Epoch 514 | Loss   0.37\n",
      "\tEpoch 515 | Batch 0 | Loss   0.39\n",
      "\tEpoch 515 | Batch 0 | Acc   0.92\n",
      "Epoch 515 | Loss   0.37\n",
      "\tEpoch 516 | Batch 0 | Loss   0.39\n",
      "\tEpoch 516 | Batch 0 | Acc   0.92\n",
      "Epoch 516 | Loss   0.36\n",
      "\tEpoch 517 | Batch 0 | Loss   0.34\n",
      "\tEpoch 517 | Batch 0 | Acc   0.98\n",
      "Epoch 517 | Loss   0.36\n",
      "\tEpoch 518 | Batch 0 | Loss   0.40\n",
      "\tEpoch 518 | Batch 0 | Acc   0.92\n",
      "Epoch 518 | Loss   0.37\n",
      "\tEpoch 519 | Batch 0 | Loss   0.37\n",
      "\tEpoch 519 | Batch 0 | Acc   0.94\n",
      "Epoch 519 | Loss   0.37\n",
      "\tEpoch 520 | Batch 0 | Loss   0.39\n",
      "\tEpoch 520 | Batch 0 | Acc   0.92\n",
      "Epoch 520 | Loss   0.36\n",
      "\tEpoch 521 | Batch 0 | Loss   0.39\n",
      "\tEpoch 521 | Batch 0 | Acc   0.92\n",
      "Epoch 521 | Loss   0.36\n",
      "\tEpoch 522 | Batch 0 | Loss   0.35\n",
      "\tEpoch 522 | Batch 0 | Acc   0.97\n",
      "Epoch 522 | Loss   0.36\n",
      "\tEpoch 523 | Batch 0 | Loss   0.37\n",
      "\tEpoch 523 | Batch 0 | Acc   0.97\n",
      "Epoch 523 | Loss   0.36\n",
      "\tEpoch 524 | Batch 0 | Loss   0.36\n",
      "\tEpoch 524 | Batch 0 | Acc   0.98\n",
      "Epoch 524 | Loss   0.36\n",
      "\tEpoch 525 | Batch 0 | Loss   0.34\n",
      "\tEpoch 525 | Batch 0 | Acc   0.98\n",
      "Epoch 525 | Loss   0.37\n",
      "\tEpoch 526 | Batch 0 | Loss   0.34\n",
      "\tEpoch 526 | Batch 0 | Acc   0.98\n",
      "Epoch 526 | Loss   0.38\n",
      "\tEpoch 527 | Batch 0 | Loss   0.42\n",
      "\tEpoch 527 | Batch 0 | Acc   0.89\n",
      "Epoch 527 | Loss   0.36\n",
      "\tEpoch 528 | Batch 0 | Loss   0.36\n",
      "\tEpoch 528 | Batch 0 | Acc   0.98\n",
      "Epoch 528 | Loss   0.37\n",
      "\tEpoch 529 | Batch 0 | Loss   0.37\n",
      "\tEpoch 529 | Batch 0 | Acc   0.97\n",
      "Epoch 529 | Loss   0.38\n",
      "\tEpoch 530 | Batch 0 | Loss   0.36\n",
      "\tEpoch 530 | Batch 0 | Acc   0.95\n",
      "Epoch 530 | Loss   0.36\n",
      "\tEpoch 531 | Batch 0 | Loss   0.32\n",
      "\tEpoch 531 | Batch 0 | Acc   1.00\n",
      "Epoch 531 | Loss   0.36\n",
      "\tEpoch 532 | Batch 0 | Loss   0.36\n",
      "\tEpoch 532 | Batch 0 | Acc   0.97\n",
      "Epoch 532 | Loss   0.36\n",
      "\tEpoch 533 | Batch 0 | Loss   0.35\n",
      "\tEpoch 533 | Batch 0 | Acc   0.97\n",
      "Epoch 533 | Loss   0.36\n",
      "\tEpoch 534 | Batch 0 | Loss   0.37\n",
      "\tEpoch 534 | Batch 0 | Acc   0.95\n",
      "Epoch 534 | Loss   0.36\n",
      "\tEpoch 535 | Batch 0 | Loss   0.34\n",
      "\tEpoch 535 | Batch 0 | Acc   0.98\n",
      "Epoch 535 | Loss   0.36\n",
      "\tEpoch 536 | Batch 0 | Loss   0.36\n",
      "\tEpoch 536 | Batch 0 | Acc   0.97\n",
      "Epoch 536 | Loss   0.37\n",
      "\tEpoch 537 | Batch 0 | Loss   0.39\n",
      "\tEpoch 537 | Batch 0 | Acc   0.92\n",
      "Epoch 537 | Loss   0.37\n",
      "\tEpoch 538 | Batch 0 | Loss   0.32\n",
      "\tEpoch 538 | Batch 0 | Acc   1.00\n",
      "Epoch 538 | Loss   0.36\n",
      "\tEpoch 539 | Batch 0 | Loss   0.36\n",
      "\tEpoch 539 | Batch 0 | Acc   0.97\n",
      "Epoch 539 | Loss   0.38\n",
      "\tEpoch 540 | Batch 0 | Loss   0.36\n",
      "\tEpoch 540 | Batch 0 | Acc   0.95\n",
      "Epoch 540 | Loss   0.36\n",
      "\tEpoch 541 | Batch 0 | Loss   0.37\n",
      "\tEpoch 541 | Batch 0 | Acc   0.94\n",
      "Epoch 541 | Loss   0.36\n",
      "\tEpoch 542 | Batch 0 | Loss   0.38\n",
      "\tEpoch 542 | Batch 0 | Acc   0.95\n",
      "Epoch 542 | Loss   0.36\n",
      "\tEpoch 543 | Batch 0 | Loss   0.38\n",
      "\tEpoch 543 | Batch 0 | Acc   0.94\n",
      "Epoch 543 | Loss   0.38\n",
      "\tEpoch 544 | Batch 0 | Loss   0.34\n",
      "\tEpoch 544 | Batch 0 | Acc   0.98\n",
      "Epoch 544 | Loss   0.36\n",
      "\tEpoch 545 | Batch 0 | Loss   0.37\n",
      "\tEpoch 545 | Batch 0 | Acc   0.95\n",
      "Epoch 545 | Loss   0.36\n",
      "\tEpoch 546 | Batch 0 | Loss   0.38\n",
      "\tEpoch 546 | Batch 0 | Acc   0.94\n",
      "Epoch 546 | Loss   0.37\n",
      "\tEpoch 547 | Batch 0 | Loss   0.40\n",
      "\tEpoch 547 | Batch 0 | Acc   0.92\n",
      "Epoch 547 | Loss   0.36\n",
      "\tEpoch 548 | Batch 0 | Loss   0.40\n",
      "\tEpoch 548 | Batch 0 | Acc   0.92\n",
      "Epoch 548 | Loss   0.36\n",
      "\tEpoch 549 | Batch 0 | Loss   0.37\n",
      "\tEpoch 549 | Batch 0 | Acc   0.94\n",
      "Epoch 549 | Loss   0.36\n",
      "\tEpoch 550 | Batch 0 | Loss   0.34\n",
      "\tEpoch 550 | Batch 0 | Acc   0.98\n",
      "Epoch 550 | Loss   0.37\n",
      "\tEpoch 551 | Batch 0 | Loss   0.35\n",
      "\tEpoch 551 | Batch 0 | Acc   0.98\n",
      "Epoch 551 | Loss   0.36\n",
      "\tEpoch 552 | Batch 0 | Loss   0.42\n",
      "\tEpoch 552 | Batch 0 | Acc   0.88\n",
      "Epoch 552 | Loss   0.36\n",
      "\tEpoch 553 | Batch 0 | Loss   0.38\n",
      "\tEpoch 553 | Batch 0 | Acc   0.94\n",
      "Epoch 553 | Loss   0.37\n",
      "\tEpoch 554 | Batch 0 | Loss   0.40\n",
      "\tEpoch 554 | Batch 0 | Acc   0.91\n",
      "Epoch 554 | Loss   0.36\n",
      "\tEpoch 555 | Batch 0 | Loss   0.36\n",
      "\tEpoch 555 | Batch 0 | Acc   0.95\n",
      "Epoch 555 | Loss   0.36\n",
      "\tEpoch 556 | Batch 0 | Loss   0.37\n",
      "\tEpoch 556 | Batch 0 | Acc   0.94\n",
      "Epoch 556 | Loss   0.36\n",
      "\tEpoch 557 | Batch 0 | Loss   0.36\n",
      "\tEpoch 557 | Batch 0 | Acc   0.95\n",
      "Epoch 557 | Loss   0.37\n",
      "\tEpoch 558 | Batch 0 | Loss   0.34\n",
      "\tEpoch 558 | Batch 0 | Acc   0.98\n",
      "Epoch 558 | Loss   0.36\n",
      "\tEpoch 559 | Batch 0 | Loss   0.40\n",
      "\tEpoch 559 | Batch 0 | Acc   0.91\n",
      "Epoch 559 | Loss   0.37\n",
      "\tEpoch 560 | Batch 0 | Loss   0.34\n",
      "\tEpoch 560 | Batch 0 | Acc   0.98\n",
      "Epoch 560 | Loss   0.36\n",
      "\tEpoch 561 | Batch 0 | Loss   0.39\n",
      "\tEpoch 561 | Batch 0 | Acc   0.92\n",
      "Epoch 561 | Loss   0.37\n",
      "\tEpoch 562 | Batch 0 | Loss   0.39\n",
      "\tEpoch 562 | Batch 0 | Acc   0.94\n",
      "Epoch 562 | Loss   0.37\n",
      "\tEpoch 563 | Batch 0 | Loss   0.35\n",
      "\tEpoch 563 | Batch 0 | Acc   0.97\n",
      "Epoch 563 | Loss   0.36\n",
      "\tEpoch 564 | Batch 0 | Loss   0.34\n",
      "\tEpoch 564 | Batch 0 | Acc   0.98\n",
      "Epoch 564 | Loss   0.36\n",
      "\tEpoch 565 | Batch 0 | Loss   0.33\n",
      "\tEpoch 565 | Batch 0 | Acc   1.00\n",
      "Epoch 565 | Loss   0.36\n",
      "\tEpoch 566 | Batch 0 | Loss   0.39\n",
      "\tEpoch 566 | Batch 0 | Acc   0.92\n",
      "Epoch 566 | Loss   0.36\n",
      "\tEpoch 567 | Batch 0 | Loss   0.37\n",
      "\tEpoch 567 | Batch 0 | Acc   0.94\n",
      "Epoch 567 | Loss   0.36\n",
      "\tEpoch 568 | Batch 0 | Loss   0.39\n",
      "\tEpoch 568 | Batch 0 | Acc   0.92\n",
      "Epoch 568 | Loss   0.37\n",
      "\tEpoch 569 | Batch 0 | Loss   0.38\n",
      "\tEpoch 569 | Batch 0 | Acc   0.92\n",
      "Epoch 569 | Loss   0.37\n",
      "\tEpoch 570 | Batch 0 | Loss   0.35\n",
      "\tEpoch 570 | Batch 0 | Acc   0.97\n",
      "Epoch 570 | Loss   0.36\n",
      "\tEpoch 571 | Batch 0 | Loss   0.37\n",
      "\tEpoch 571 | Batch 0 | Acc   0.95\n",
      "Epoch 571 | Loss   0.37\n",
      "\tEpoch 572 | Batch 0 | Loss   0.36\n",
      "\tEpoch 572 | Batch 0 | Acc   0.97\n",
      "Epoch 572 | Loss   0.37\n",
      "\tEpoch 573 | Batch 0 | Loss   0.36\n",
      "\tEpoch 573 | Batch 0 | Acc   0.97\n",
      "Epoch 573 | Loss   0.36\n",
      "\tEpoch 574 | Batch 0 | Loss   0.36\n",
      "\tEpoch 574 | Batch 0 | Acc   0.97\n",
      "Epoch 574 | Loss   0.36\n",
      "\tEpoch 575 | Batch 0 | Loss   0.36\n",
      "\tEpoch 575 | Batch 0 | Acc   0.95\n",
      "Epoch 575 | Loss   0.37\n",
      "\tEpoch 576 | Batch 0 | Loss   0.33\n",
      "\tEpoch 576 | Batch 0 | Acc   0.98\n",
      "Epoch 576 | Loss   0.36\n",
      "\tEpoch 577 | Batch 0 | Loss   0.38\n",
      "\tEpoch 577 | Batch 0 | Acc   0.94\n",
      "Epoch 577 | Loss   0.36\n",
      "\tEpoch 578 | Batch 0 | Loss   0.34\n",
      "\tEpoch 578 | Batch 0 | Acc   0.97\n",
      "Epoch 578 | Loss   0.36\n",
      "\tEpoch 579 | Batch 0 | Loss   0.35\n",
      "\tEpoch 579 | Batch 0 | Acc   0.97\n",
      "Epoch 579 | Loss   0.36\n",
      "\tEpoch 580 | Batch 0 | Loss   0.34\n",
      "\tEpoch 580 | Batch 0 | Acc   0.98\n",
      "Epoch 580 | Loss   0.36\n",
      "\tEpoch 581 | Batch 0 | Loss   0.35\n",
      "\tEpoch 581 | Batch 0 | Acc   0.98\n",
      "Epoch 581 | Loss   0.36\n",
      "\tEpoch 582 | Batch 0 | Loss   0.37\n",
      "\tEpoch 582 | Batch 0 | Acc   0.95\n",
      "Epoch 582 | Loss   0.36\n",
      "\tEpoch 583 | Batch 0 | Loss   0.36\n",
      "\tEpoch 583 | Batch 0 | Acc   0.95\n",
      "Epoch 583 | Loss   0.36\n",
      "\tEpoch 584 | Batch 0 | Loss   0.37\n",
      "\tEpoch 584 | Batch 0 | Acc   0.95\n",
      "Epoch 584 | Loss   0.36\n",
      "\tEpoch 585 | Batch 0 | Loss   0.36\n",
      "\tEpoch 585 | Batch 0 | Acc   0.95\n",
      "Epoch 585 | Loss   0.36\n",
      "\tEpoch 586 | Batch 0 | Loss   0.37\n",
      "\tEpoch 586 | Batch 0 | Acc   0.94\n",
      "Epoch 586 | Loss   0.36\n",
      "\tEpoch 587 | Batch 0 | Loss   0.35\n",
      "\tEpoch 587 | Batch 0 | Acc   0.98\n",
      "Epoch 587 | Loss   0.37\n",
      "\tEpoch 588 | Batch 0 | Loss   0.34\n",
      "\tEpoch 588 | Batch 0 | Acc   0.98\n",
      "Epoch 588 | Loss   0.37\n",
      "\tEpoch 589 | Batch 0 | Loss   0.34\n",
      "\tEpoch 589 | Batch 0 | Acc   0.98\n",
      "Epoch 589 | Loss   0.36\n",
      "\tEpoch 590 | Batch 0 | Loss   0.33\n",
      "\tEpoch 590 | Batch 0 | Acc   0.98\n",
      "Epoch 590 | Loss   0.36\n",
      "\tEpoch 591 | Batch 0 | Loss   0.37\n",
      "\tEpoch 591 | Batch 0 | Acc   0.95\n",
      "Epoch 591 | Loss   0.38\n",
      "\tEpoch 592 | Batch 0 | Loss   0.35\n",
      "\tEpoch 592 | Batch 0 | Acc   0.98\n",
      "Epoch 592 | Loss   0.36\n",
      "\tEpoch 593 | Batch 0 | Loss   0.36\n",
      "\tEpoch 593 | Batch 0 | Acc   0.95\n",
      "Epoch 593 | Loss   0.36\n",
      "\tEpoch 594 | Batch 0 | Loss   0.36\n",
      "\tEpoch 594 | Batch 0 | Acc   0.97\n",
      "Epoch 594 | Loss   0.36\n",
      "\tEpoch 595 | Batch 0 | Loss   0.36\n",
      "\tEpoch 595 | Batch 0 | Acc   0.95\n",
      "Epoch 595 | Loss   0.37\n",
      "\tEpoch 596 | Batch 0 | Loss   0.37\n",
      "\tEpoch 596 | Batch 0 | Acc   0.94\n",
      "Epoch 596 | Loss   0.36\n",
      "\tEpoch 597 | Batch 0 | Loss   0.36\n",
      "\tEpoch 597 | Batch 0 | Acc   0.95\n",
      "Epoch 597 | Loss   0.37\n",
      "\tEpoch 598 | Batch 0 | Loss   0.35\n",
      "\tEpoch 598 | Batch 0 | Acc   0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 598 | Loss   0.36\n",
      "\tEpoch 599 | Batch 0 | Loss   0.35\n",
      "\tEpoch 599 | Batch 0 | Acc   0.97\n",
      "Epoch 599 | Loss   0.36\n",
      "\tEpoch 600 | Batch 0 | Loss   0.40\n",
      "\tEpoch 600 | Batch 0 | Acc   0.92\n",
      "Epoch 600 | Loss   0.37\n",
      "\tEpoch 601 | Batch 0 | Loss   0.37\n",
      "\tEpoch 601 | Batch 0 | Acc   0.94\n",
      "Epoch 601 | Loss   0.36\n",
      "\tEpoch 602 | Batch 0 | Loss   0.38\n",
      "\tEpoch 602 | Batch 0 | Acc   0.94\n",
      "Epoch 602 | Loss   0.36\n",
      "\tEpoch 603 | Batch 0 | Loss   0.37\n",
      "\tEpoch 603 | Batch 0 | Acc   0.94\n",
      "Epoch 603 | Loss   0.36\n",
      "\tEpoch 604 | Batch 0 | Loss   0.38\n",
      "\tEpoch 604 | Batch 0 | Acc   0.94\n",
      "Epoch 604 | Loss   0.36\n",
      "\tEpoch 605 | Batch 0 | Loss   0.36\n",
      "\tEpoch 605 | Batch 0 | Acc   0.95\n",
      "Epoch 605 | Loss   0.36\n",
      "\tEpoch 606 | Batch 0 | Loss   0.37\n",
      "\tEpoch 606 | Batch 0 | Acc   0.94\n",
      "Epoch 606 | Loss   0.36\n",
      "\tEpoch 607 | Batch 0 | Loss   0.40\n",
      "\tEpoch 607 | Batch 0 | Acc   0.91\n",
      "Epoch 607 | Loss   0.36\n",
      "\tEpoch 608 | Batch 0 | Loss   0.40\n",
      "\tEpoch 608 | Batch 0 | Acc   0.92\n",
      "Epoch 608 | Loss   0.36\n",
      "\tEpoch 609 | Batch 0 | Loss   0.38\n",
      "\tEpoch 609 | Batch 0 | Acc   0.94\n",
      "Epoch 609 | Loss   0.36\n",
      "\tEpoch 610 | Batch 0 | Loss   0.36\n",
      "\tEpoch 610 | Batch 0 | Acc   0.97\n",
      "Epoch 610 | Loss   0.36\n",
      "\tEpoch 611 | Batch 0 | Loss   0.34\n",
      "\tEpoch 611 | Batch 0 | Acc   0.98\n",
      "Epoch 611 | Loss   0.37\n",
      "\tEpoch 612 | Batch 0 | Loss   0.39\n",
      "\tEpoch 612 | Batch 0 | Acc   0.94\n",
      "Epoch 612 | Loss   0.37\n",
      "\tEpoch 613 | Batch 0 | Loss   0.35\n",
      "\tEpoch 613 | Batch 0 | Acc   0.95\n",
      "Epoch 613 | Loss   0.38\n",
      "\tEpoch 614 | Batch 0 | Loss   0.35\n",
      "\tEpoch 614 | Batch 0 | Acc   0.97\n",
      "Epoch 614 | Loss   0.36\n",
      "\tEpoch 615 | Batch 0 | Loss   0.34\n",
      "\tEpoch 615 | Batch 0 | Acc   1.00\n",
      "Epoch 615 | Loss   0.36\n",
      "\tEpoch 616 | Batch 0 | Loss   0.34\n",
      "\tEpoch 616 | Batch 0 | Acc   0.98\n",
      "Epoch 616 | Loss   0.36\n",
      "\tEpoch 617 | Batch 0 | Loss   0.35\n",
      "\tEpoch 617 | Batch 0 | Acc   0.97\n",
      "Epoch 617 | Loss   0.37\n",
      "\tEpoch 618 | Batch 0 | Loss   0.39\n",
      "\tEpoch 618 | Batch 0 | Acc   0.91\n",
      "Epoch 618 | Loss   0.36\n",
      "\tEpoch 619 | Batch 0 | Loss   0.40\n",
      "\tEpoch 619 | Batch 0 | Acc   0.92\n",
      "Epoch 619 | Loss   0.36\n",
      "\tEpoch 620 | Batch 0 | Loss   0.36\n",
      "\tEpoch 620 | Batch 0 | Acc   0.95\n",
      "Epoch 620 | Loss   0.36\n",
      "\tEpoch 621 | Batch 0 | Loss   0.34\n",
      "\tEpoch 621 | Batch 0 | Acc   0.97\n",
      "Epoch 621 | Loss   0.37\n",
      "\tEpoch 622 | Batch 0 | Loss   0.35\n",
      "\tEpoch 622 | Batch 0 | Acc   0.97\n",
      "Epoch 622 | Loss   0.36\n",
      "\tEpoch 623 | Batch 0 | Loss   0.38\n",
      "\tEpoch 623 | Batch 0 | Acc   0.91\n",
      "Epoch 623 | Loss   0.36\n",
      "\tEpoch 624 | Batch 0 | Loss   0.36\n",
      "\tEpoch 624 | Batch 0 | Acc   0.95\n",
      "Epoch 624 | Loss   0.37\n",
      "\tEpoch 625 | Batch 0 | Loss   0.33\n",
      "\tEpoch 625 | Batch 0 | Acc   0.98\n",
      "Epoch 625 | Loss   0.36\n",
      "\tEpoch 626 | Batch 0 | Loss   0.39\n",
      "\tEpoch 626 | Batch 0 | Acc   0.94\n",
      "Epoch 626 | Loss   0.36\n",
      "\tEpoch 627 | Batch 0 | Loss   0.36\n",
      "\tEpoch 627 | Batch 0 | Acc   0.97\n",
      "Epoch 627 | Loss   0.37\n",
      "\tEpoch 628 | Batch 0 | Loss   0.35\n",
      "\tEpoch 628 | Batch 0 | Acc   0.97\n",
      "Epoch 628 | Loss   0.36\n",
      "\tEpoch 629 | Batch 0 | Loss   0.41\n",
      "\tEpoch 629 | Batch 0 | Acc   0.91\n",
      "Epoch 629 | Loss   0.36\n",
      "\tEpoch 630 | Batch 0 | Loss   0.36\n",
      "\tEpoch 630 | Batch 0 | Acc   0.94\n",
      "Epoch 630 | Loss   0.37\n",
      "\tEpoch 631 | Batch 0 | Loss   0.39\n",
      "\tEpoch 631 | Batch 0 | Acc   0.92\n",
      "Epoch 631 | Loss   0.36\n",
      "\tEpoch 632 | Batch 0 | Loss   0.34\n",
      "\tEpoch 632 | Batch 0 | Acc   0.98\n",
      "Epoch 632 | Loss   0.36\n",
      "\tEpoch 633 | Batch 0 | Loss   0.39\n",
      "\tEpoch 633 | Batch 0 | Acc   0.94\n",
      "Epoch 633 | Loss   0.36\n",
      "\tEpoch 634 | Batch 0 | Loss   0.39\n",
      "\tEpoch 634 | Batch 0 | Acc   0.94\n",
      "Epoch 634 | Loss   0.37\n",
      "\tEpoch 635 | Batch 0 | Loss   0.38\n",
      "\tEpoch 635 | Batch 0 | Acc   0.94\n",
      "Epoch 635 | Loss   0.36\n",
      "\tEpoch 636 | Batch 0 | Loss   0.32\n",
      "\tEpoch 636 | Batch 0 | Acc   1.00\n",
      "Epoch 636 | Loss   0.38\n",
      "\tEpoch 637 | Batch 0 | Loss   0.37\n",
      "\tEpoch 637 | Batch 0 | Acc   0.95\n",
      "Epoch 637 | Loss   0.37\n",
      "\tEpoch 638 | Batch 0 | Loss   0.36\n",
      "\tEpoch 638 | Batch 0 | Acc   0.95\n",
      "Epoch 638 | Loss   0.37\n",
      "\tEpoch 639 | Batch 0 | Loss   0.32\n",
      "\tEpoch 639 | Batch 0 | Acc   1.00\n",
      "Epoch 639 | Loss   0.36\n",
      "\tEpoch 640 | Batch 0 | Loss   0.32\n",
      "\tEpoch 640 | Batch 0 | Acc   1.00\n",
      "Epoch 640 | Loss   0.36\n",
      "\tEpoch 641 | Batch 0 | Loss   0.36\n",
      "\tEpoch 641 | Batch 0 | Acc   0.95\n",
      "Epoch 641 | Loss   0.36\n",
      "\tEpoch 642 | Batch 0 | Loss   0.39\n",
      "\tEpoch 642 | Batch 0 | Acc   0.92\n",
      "Epoch 642 | Loss   0.36\n",
      "\tEpoch 643 | Batch 0 | Loss   0.33\n",
      "\tEpoch 643 | Batch 0 | Acc   1.00\n",
      "Epoch 643 | Loss   0.36\n",
      "\tEpoch 644 | Batch 0 | Loss   0.35\n",
      "\tEpoch 644 | Batch 0 | Acc   0.98\n",
      "Epoch 644 | Loss   0.39\n",
      "\tEpoch 645 | Batch 0 | Loss   0.36\n",
      "\tEpoch 645 | Batch 0 | Acc   0.95\n",
      "Epoch 645 | Loss   0.36\n",
      "\tEpoch 646 | Batch 0 | Loss   0.37\n",
      "\tEpoch 646 | Batch 0 | Acc   0.94\n",
      "Epoch 646 | Loss   0.36\n",
      "\tEpoch 647 | Batch 0 | Loss   0.37\n",
      "\tEpoch 647 | Batch 0 | Acc   0.94\n",
      "Epoch 647 | Loss   0.36\n",
      "\tEpoch 648 | Batch 0 | Loss   0.34\n",
      "\tEpoch 648 | Batch 0 | Acc   0.95\n",
      "Epoch 648 | Loss   0.37\n",
      "\tEpoch 649 | Batch 0 | Loss   0.34\n",
      "\tEpoch 649 | Batch 0 | Acc   0.97\n",
      "Epoch 649 | Loss   0.36\n",
      "\tEpoch 650 | Batch 0 | Loss   0.37\n",
      "\tEpoch 650 | Batch 0 | Acc   0.95\n",
      "Epoch 650 | Loss   0.36\n",
      "\tEpoch 651 | Batch 0 | Loss   0.36\n",
      "\tEpoch 651 | Batch 0 | Acc   0.95\n",
      "Epoch 651 | Loss   0.36\n",
      "\tEpoch 652 | Batch 0 | Loss   0.34\n",
      "\tEpoch 652 | Batch 0 | Acc   0.98\n",
      "Epoch 652 | Loss   0.37\n",
      "\tEpoch 653 | Batch 0 | Loss   0.32\n",
      "\tEpoch 653 | Batch 0 | Acc   1.00\n",
      "Epoch 653 | Loss   0.36\n",
      "\tEpoch 654 | Batch 0 | Loss   0.41\n",
      "\tEpoch 654 | Batch 0 | Acc   0.89\n",
      "Epoch 654 | Loss   0.36\n",
      "\tEpoch 655 | Batch 0 | Loss   0.37\n",
      "\tEpoch 655 | Batch 0 | Acc   0.95\n",
      "Epoch 655 | Loss   0.37\n",
      "\tEpoch 656 | Batch 0 | Loss   0.38\n",
      "\tEpoch 656 | Batch 0 | Acc   0.94\n",
      "Epoch 656 | Loss   0.36\n",
      "\tEpoch 657 | Batch 0 | Loss   0.38\n",
      "\tEpoch 657 | Batch 0 | Acc   0.94\n",
      "Epoch 657 | Loss   0.36\n",
      "\tEpoch 658 | Batch 0 | Loss   0.37\n",
      "\tEpoch 658 | Batch 0 | Acc   0.94\n",
      "Epoch 658 | Loss   0.36\n",
      "\tEpoch 659 | Batch 0 | Loss   0.37\n",
      "\tEpoch 659 | Batch 0 | Acc   0.92\n",
      "Epoch 659 | Loss   0.37\n",
      "\tEpoch 660 | Batch 0 | Loss   0.39\n",
      "\tEpoch 660 | Batch 0 | Acc   0.95\n",
      "Epoch 660 | Loss   0.36\n",
      "\tEpoch 661 | Batch 0 | Loss   0.34\n",
      "\tEpoch 661 | Batch 0 | Acc   0.97\n",
      "Epoch 661 | Loss   0.36\n",
      "\tEpoch 662 | Batch 0 | Loss   0.36\n",
      "\tEpoch 662 | Batch 0 | Acc   0.97\n",
      "Epoch 662 | Loss   0.37\n",
      "\tEpoch 663 | Batch 0 | Loss   0.39\n",
      "\tEpoch 663 | Batch 0 | Acc   0.92\n",
      "Epoch 663 | Loss   0.37\n",
      "\tEpoch 664 | Batch 0 | Loss   0.35\n",
      "\tEpoch 664 | Batch 0 | Acc   0.97\n",
      "Epoch 664 | Loss   0.36\n",
      "\tEpoch 665 | Batch 0 | Loss   0.36\n",
      "\tEpoch 665 | Batch 0 | Acc   0.95\n",
      "Epoch 665 | Loss   0.36\n",
      "\tEpoch 666 | Batch 0 | Loss   0.37\n",
      "\tEpoch 666 | Batch 0 | Acc   0.95\n",
      "Epoch 666 | Loss   0.36\n",
      "\tEpoch 667 | Batch 0 | Loss   0.38\n",
      "\tEpoch 667 | Batch 0 | Acc   0.94\n",
      "Epoch 667 | Loss   0.37\n",
      "\tEpoch 668 | Batch 0 | Loss   0.38\n",
      "\tEpoch 668 | Batch 0 | Acc   0.94\n",
      "Epoch 668 | Loss   0.36\n",
      "\tEpoch 669 | Batch 0 | Loss   0.35\n",
      "\tEpoch 669 | Batch 0 | Acc   0.97\n",
      "Epoch 669 | Loss   0.38\n",
      "\tEpoch 670 | Batch 0 | Loss   0.40\n",
      "\tEpoch 670 | Batch 0 | Acc   0.92\n",
      "Epoch 670 | Loss   0.36\n",
      "\tEpoch 671 | Batch 0 | Loss   0.38\n",
      "\tEpoch 671 | Batch 0 | Acc   0.92\n",
      "Epoch 671 | Loss   0.36\n",
      "\tEpoch 672 | Batch 0 | Loss   0.34\n",
      "\tEpoch 672 | Batch 0 | Acc   0.98\n",
      "Epoch 672 | Loss   0.36\n",
      "\tEpoch 673 | Batch 0 | Loss   0.36\n",
      "\tEpoch 673 | Batch 0 | Acc   0.95\n",
      "Epoch 673 | Loss   0.37\n",
      "\tEpoch 674 | Batch 0 | Loss   0.36\n",
      "\tEpoch 674 | Batch 0 | Acc   0.95\n",
      "Epoch 674 | Loss   0.36\n",
      "\tEpoch 675 | Batch 0 | Loss   0.36\n",
      "\tEpoch 675 | Batch 0 | Acc   0.95\n",
      "Epoch 675 | Loss   0.36\n",
      "\tEpoch 676 | Batch 0 | Loss   0.37\n",
      "\tEpoch 676 | Batch 0 | Acc   0.95\n",
      "Epoch 676 | Loss   0.36\n",
      "\tEpoch 677 | Batch 0 | Loss   0.36\n",
      "\tEpoch 677 | Batch 0 | Acc   0.94\n",
      "Epoch 677 | Loss   0.36\n",
      "\tEpoch 678 | Batch 0 | Loss   0.39\n",
      "\tEpoch 678 | Batch 0 | Acc   0.94\n",
      "Epoch 678 | Loss   0.36\n",
      "\tEpoch 679 | Batch 0 | Loss   0.39\n",
      "\tEpoch 679 | Batch 0 | Acc   0.91\n",
      "Epoch 679 | Loss   0.37\n",
      "\tEpoch 680 | Batch 0 | Loss   0.36\n",
      "\tEpoch 680 | Batch 0 | Acc   0.97\n",
      "Epoch 680 | Loss   0.36\n",
      "\tEpoch 681 | Batch 0 | Loss   0.38\n",
      "\tEpoch 681 | Batch 0 | Acc   0.94\n",
      "Epoch 681 | Loss   0.36\n",
      "\tEpoch 682 | Batch 0 | Loss   0.36\n",
      "\tEpoch 682 | Batch 0 | Acc   0.97\n",
      "Epoch 682 | Loss   0.36\n",
      "\tEpoch 683 | Batch 0 | Loss   0.38\n",
      "\tEpoch 683 | Batch 0 | Acc   0.94\n",
      "Epoch 683 | Loss   0.36\n",
      "\tEpoch 684 | Batch 0 | Loss   0.37\n",
      "\tEpoch 684 | Batch 0 | Acc   0.94\n",
      "Epoch 684 | Loss   0.37\n",
      "\tEpoch 685 | Batch 0 | Loss   0.39\n",
      "\tEpoch 685 | Batch 0 | Acc   0.94\n",
      "Epoch 685 | Loss   0.36\n",
      "\tEpoch 686 | Batch 0 | Loss   0.32\n",
      "\tEpoch 686 | Batch 0 | Acc   1.00\n",
      "Epoch 686 | Loss   0.36\n",
      "\tEpoch 687 | Batch 0 | Loss   0.39\n",
      "\tEpoch 687 | Batch 0 | Acc   0.92\n",
      "Epoch 687 | Loss   0.36\n",
      "\tEpoch 688 | Batch 0 | Loss   0.40\n",
      "\tEpoch 688 | Batch 0 | Acc   0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 688 | Loss   0.36\n",
      "\tEpoch 689 | Batch 0 | Loss   0.34\n",
      "\tEpoch 689 | Batch 0 | Acc   0.98\n",
      "Epoch 689 | Loss   0.37\n",
      "\tEpoch 690 | Batch 0 | Loss   0.34\n",
      "\tEpoch 690 | Batch 0 | Acc   0.98\n",
      "Epoch 690 | Loss   0.37\n",
      "\tEpoch 691 | Batch 0 | Loss   0.35\n",
      "\tEpoch 691 | Batch 0 | Acc   0.97\n",
      "Epoch 691 | Loss   0.37\n",
      "\tEpoch 692 | Batch 0 | Loss   0.33\n",
      "\tEpoch 692 | Batch 0 | Acc   1.00\n",
      "Epoch 692 | Loss   0.36\n",
      "\tEpoch 693 | Batch 0 | Loss   0.36\n",
      "\tEpoch 693 | Batch 0 | Acc   0.95\n",
      "Epoch 693 | Loss   0.36\n",
      "\tEpoch 694 | Batch 0 | Loss   0.33\n",
      "\tEpoch 694 | Batch 0 | Acc   0.98\n",
      "Epoch 694 | Loss   0.37\n",
      "\tEpoch 695 | Batch 0 | Loss   0.36\n",
      "\tEpoch 695 | Batch 0 | Acc   0.97\n",
      "Epoch 695 | Loss   0.36\n",
      "\tEpoch 696 | Batch 0 | Loss   0.36\n",
      "\tEpoch 696 | Batch 0 | Acc   0.95\n",
      "Epoch 696 | Loss   0.38\n",
      "\tEpoch 697 | Batch 0 | Loss   0.35\n",
      "\tEpoch 697 | Batch 0 | Acc   0.97\n",
      "Epoch 697 | Loss   0.36\n",
      "\tEpoch 698 | Batch 0 | Loss   0.34\n",
      "\tEpoch 698 | Batch 0 | Acc   0.98\n",
      "Epoch 698 | Loss   0.36\n",
      "\tEpoch 699 | Batch 0 | Loss   0.39\n",
      "\tEpoch 699 | Batch 0 | Acc   0.92\n",
      "Epoch 699 | Loss   0.36\n",
      "\tEpoch 700 | Batch 0 | Loss   0.40\n",
      "\tEpoch 700 | Batch 0 | Acc   0.91\n",
      "Epoch 700 | Loss   0.36\n",
      "\tEpoch 701 | Batch 0 | Loss   0.34\n",
      "\tEpoch 701 | Batch 0 | Acc   0.98\n",
      "Epoch 701 | Loss   0.37\n",
      "\tEpoch 702 | Batch 0 | Loss   0.37\n",
      "\tEpoch 702 | Batch 0 | Acc   0.94\n",
      "Epoch 702 | Loss   0.37\n",
      "\tEpoch 703 | Batch 0 | Loss   0.35\n",
      "\tEpoch 703 | Batch 0 | Acc   0.97\n",
      "Epoch 703 | Loss   0.36\n",
      "\tEpoch 704 | Batch 0 | Loss   0.41\n",
      "\tEpoch 704 | Batch 0 | Acc   0.91\n",
      "Epoch 704 | Loss   0.36\n",
      "\tEpoch 705 | Batch 0 | Loss   0.38\n",
      "\tEpoch 705 | Batch 0 | Acc   0.92\n",
      "Epoch 705 | Loss   0.37\n",
      "\tEpoch 706 | Batch 0 | Loss   0.34\n",
      "\tEpoch 706 | Batch 0 | Acc   0.98\n",
      "Epoch 706 | Loss   0.36\n",
      "\tEpoch 707 | Batch 0 | Loss   0.36\n",
      "\tEpoch 707 | Batch 0 | Acc   0.95\n",
      "Epoch 707 | Loss   0.38\n",
      "\tEpoch 708 | Batch 0 | Loss   0.36\n",
      "\tEpoch 708 | Batch 0 | Acc   0.97\n",
      "Epoch 708 | Loss   0.36\n",
      "\tEpoch 709 | Batch 0 | Loss   0.34\n",
      "\tEpoch 709 | Batch 0 | Acc   0.98\n",
      "Epoch 709 | Loss   0.36\n",
      "\tEpoch 710 | Batch 0 | Loss   0.37\n",
      "\tEpoch 710 | Batch 0 | Acc   0.95\n",
      "Epoch 710 | Loss   0.37\n",
      "\tEpoch 711 | Batch 0 | Loss   0.34\n",
      "\tEpoch 711 | Batch 0 | Acc   0.98\n",
      "Epoch 711 | Loss   0.36\n",
      "\tEpoch 712 | Batch 0 | Loss   0.36\n",
      "\tEpoch 712 | Batch 0 | Acc   0.95\n",
      "Epoch 712 | Loss   0.36\n",
      "\tEpoch 713 | Batch 0 | Loss   0.38\n",
      "\tEpoch 713 | Batch 0 | Acc   0.92\n",
      "Epoch 713 | Loss   0.36\n",
      "\tEpoch 714 | Batch 0 | Loss   0.38\n",
      "\tEpoch 714 | Batch 0 | Acc   0.94\n",
      "Epoch 714 | Loss   0.36\n",
      "\tEpoch 715 | Batch 0 | Loss   0.33\n",
      "\tEpoch 715 | Batch 0 | Acc   0.98\n",
      "Epoch 715 | Loss   0.37\n",
      "\tEpoch 716 | Batch 0 | Loss   0.38\n",
      "\tEpoch 716 | Batch 0 | Acc   0.94\n",
      "Epoch 716 | Loss   0.36\n",
      "\tEpoch 717 | Batch 0 | Loss   0.38\n",
      "\tEpoch 717 | Batch 0 | Acc   0.94\n",
      "Epoch 717 | Loss   0.36\n",
      "\tEpoch 718 | Batch 0 | Loss   0.39\n",
      "\tEpoch 718 | Batch 0 | Acc   0.92\n",
      "Epoch 718 | Loss   0.36\n",
      "\tEpoch 719 | Batch 0 | Loss   0.36\n",
      "\tEpoch 719 | Batch 0 | Acc   0.95\n",
      "Epoch 719 | Loss   0.36\n",
      "\tEpoch 720 | Batch 0 | Loss   0.36\n",
      "\tEpoch 720 | Batch 0 | Acc   0.97\n",
      "Epoch 720 | Loss   0.36\n",
      "\tEpoch 721 | Batch 0 | Loss   0.36\n",
      "\tEpoch 721 | Batch 0 | Acc   0.95\n",
      "Epoch 721 | Loss   0.36\n",
      "\tEpoch 722 | Batch 0 | Loss   0.38\n",
      "\tEpoch 722 | Batch 0 | Acc   0.94\n",
      "Epoch 722 | Loss   0.36\n",
      "\tEpoch 723 | Batch 0 | Loss   0.38\n",
      "\tEpoch 723 | Batch 0 | Acc   0.92\n",
      "Epoch 723 | Loss   0.37\n",
      "\tEpoch 724 | Batch 0 | Loss   0.38\n",
      "\tEpoch 724 | Batch 0 | Acc   0.94\n",
      "Epoch 724 | Loss   0.39\n",
      "\tEpoch 725 | Batch 0 | Loss   0.33\n",
      "\tEpoch 725 | Batch 0 | Acc   0.98\n",
      "Epoch 725 | Loss   0.36\n",
      "\tEpoch 726 | Batch 0 | Loss   0.37\n",
      "\tEpoch 726 | Batch 0 | Acc   0.94\n",
      "Epoch 726 | Loss   0.36\n",
      "\tEpoch 727 | Batch 0 | Loss   0.35\n",
      "\tEpoch 727 | Batch 0 | Acc   0.97\n",
      "Epoch 727 | Loss   0.37\n",
      "\tEpoch 728 | Batch 0 | Loss   0.38\n",
      "\tEpoch 728 | Batch 0 | Acc   0.92\n",
      "Epoch 728 | Loss   0.36\n",
      "\tEpoch 729 | Batch 0 | Loss   0.38\n",
      "\tEpoch 729 | Batch 0 | Acc   0.94\n",
      "Epoch 729 | Loss   0.36\n",
      "\tEpoch 730 | Batch 0 | Loss   0.35\n",
      "\tEpoch 730 | Batch 0 | Acc   0.98\n",
      "Epoch 730 | Loss   0.37\n",
      "\tEpoch 731 | Batch 0 | Loss   0.37\n",
      "\tEpoch 731 | Batch 0 | Acc   0.97\n",
      "Epoch 731 | Loss   0.36\n",
      "\tEpoch 732 | Batch 0 | Loss   0.34\n",
      "\tEpoch 732 | Batch 0 | Acc   0.98\n",
      "Epoch 732 | Loss   0.37\n",
      "\tEpoch 733 | Batch 0 | Loss   0.35\n",
      "\tEpoch 733 | Batch 0 | Acc   0.97\n",
      "Epoch 733 | Loss   0.36\n",
      "\tEpoch 734 | Batch 0 | Loss   0.37\n",
      "\tEpoch 734 | Batch 0 | Acc   0.95\n",
      "Epoch 734 | Loss   0.37\n",
      "\tEpoch 735 | Batch 0 | Loss   0.42\n",
      "\tEpoch 735 | Batch 0 | Acc   0.91\n",
      "Epoch 735 | Loss   0.36\n",
      "\tEpoch 736 | Batch 0 | Loss   0.34\n",
      "\tEpoch 736 | Batch 0 | Acc   0.98\n",
      "Epoch 736 | Loss   0.36\n",
      "\tEpoch 737 | Batch 0 | Loss   0.35\n",
      "\tEpoch 737 | Batch 0 | Acc   0.97\n",
      "Epoch 737 | Loss   0.36\n",
      "\tEpoch 738 | Batch 0 | Loss   0.36\n",
      "\tEpoch 738 | Batch 0 | Acc   0.95\n",
      "Epoch 738 | Loss   0.36\n",
      "\tEpoch 739 | Batch 0 | Loss   0.39\n",
      "\tEpoch 739 | Batch 0 | Acc   0.92\n",
      "Epoch 739 | Loss   0.36\n",
      "\tEpoch 740 | Batch 0 | Loss   0.34\n",
      "\tEpoch 740 | Batch 0 | Acc   1.00\n",
      "Epoch 740 | Loss   0.36\n",
      "\tEpoch 741 | Batch 0 | Loss   0.36\n",
      "\tEpoch 741 | Batch 0 | Acc   0.97\n",
      "Epoch 741 | Loss   0.38\n",
      "\tEpoch 742 | Batch 0 | Loss   0.34\n",
      "\tEpoch 742 | Batch 0 | Acc   0.97\n",
      "Epoch 742 | Loss   0.36\n",
      "\tEpoch 743 | Batch 0 | Loss   0.37\n",
      "\tEpoch 743 | Batch 0 | Acc   0.95\n",
      "Epoch 743 | Loss   0.36\n",
      "\tEpoch 744 | Batch 0 | Loss   0.37\n",
      "\tEpoch 744 | Batch 0 | Acc   0.95\n",
      "Epoch 744 | Loss   0.36\n",
      "\tEpoch 745 | Batch 0 | Loss   0.32\n",
      "\tEpoch 745 | Batch 0 | Acc   1.00\n",
      "Epoch 745 | Loss   0.39\n",
      "\tEpoch 746 | Batch 0 | Loss   0.34\n",
      "\tEpoch 746 | Batch 0 | Acc   0.98\n",
      "Epoch 746 | Loss   0.36\n",
      "\tEpoch 747 | Batch 0 | Loss   0.34\n",
      "\tEpoch 747 | Batch 0 | Acc   0.97\n",
      "Epoch 747 | Loss   0.38\n",
      "\tEpoch 748 | Batch 0 | Loss   0.38\n",
      "\tEpoch 748 | Batch 0 | Acc   0.92\n",
      "Epoch 748 | Loss   0.37\n",
      "\tEpoch 749 | Batch 0 | Loss   0.39\n",
      "\tEpoch 749 | Batch 0 | Acc   0.92\n",
      "Epoch 749 | Loss   0.36\n",
      "\tEpoch 750 | Batch 0 | Loss   0.37\n",
      "\tEpoch 750 | Batch 0 | Acc   0.95\n",
      "Epoch 750 | Loss   0.36\n",
      "\tEpoch 751 | Batch 0 | Loss   0.36\n",
      "\tEpoch 751 | Batch 0 | Acc   0.95\n",
      "Epoch 751 | Loss   0.36\n",
      "\tEpoch 752 | Batch 0 | Loss   0.33\n",
      "\tEpoch 752 | Batch 0 | Acc   0.98\n",
      "Epoch 752 | Loss   0.37\n",
      "\tEpoch 753 | Batch 0 | Loss   0.36\n",
      "\tEpoch 753 | Batch 0 | Acc   0.95\n",
      "Epoch 753 | Loss   0.36\n",
      "\tEpoch 754 | Batch 0 | Loss   0.35\n",
      "\tEpoch 754 | Batch 0 | Acc   0.95\n",
      "Epoch 754 | Loss   0.36\n",
      "\tEpoch 755 | Batch 0 | Loss   0.40\n",
      "\tEpoch 755 | Batch 0 | Acc   0.91\n",
      "Epoch 755 | Loss   0.36\n",
      "\tEpoch 756 | Batch 0 | Loss   0.35\n",
      "\tEpoch 756 | Batch 0 | Acc   0.97\n",
      "Epoch 756 | Loss   0.36\n",
      "\tEpoch 757 | Batch 0 | Loss   0.38\n",
      "\tEpoch 757 | Batch 0 | Acc   0.94\n",
      "Epoch 757 | Loss   0.36\n",
      "\tEpoch 758 | Batch 0 | Loss   0.32\n",
      "\tEpoch 758 | Batch 0 | Acc   1.00\n",
      "Epoch 758 | Loss   0.37\n",
      "\tEpoch 759 | Batch 0 | Loss   0.36\n",
      "\tEpoch 759 | Batch 0 | Acc   0.95\n",
      "Epoch 759 | Loss   0.37\n",
      "\tEpoch 760 | Batch 0 | Loss   0.36\n",
      "\tEpoch 760 | Batch 0 | Acc   0.95\n",
      "Epoch 760 | Loss   0.36\n",
      "\tEpoch 761 | Batch 0 | Loss   0.36\n",
      "\tEpoch 761 | Batch 0 | Acc   0.95\n",
      "Epoch 761 | Loss   0.36\n",
      "\tEpoch 762 | Batch 0 | Loss   0.36\n",
      "\tEpoch 762 | Batch 0 | Acc   0.95\n",
      "Epoch 762 | Loss   0.36\n",
      "\tEpoch 763 | Batch 0 | Loss   0.35\n",
      "\tEpoch 763 | Batch 0 | Acc   0.97\n",
      "Epoch 763 | Loss   0.37\n",
      "\tEpoch 764 | Batch 0 | Loss   0.35\n",
      "\tEpoch 764 | Batch 0 | Acc   0.97\n",
      "Epoch 764 | Loss   0.37\n",
      "\tEpoch 765 | Batch 0 | Loss   0.36\n",
      "\tEpoch 765 | Batch 0 | Acc   0.95\n",
      "Epoch 765 | Loss   0.37\n",
      "\tEpoch 766 | Batch 0 | Loss   0.36\n",
      "\tEpoch 766 | Batch 0 | Acc   0.95\n",
      "Epoch 766 | Loss   0.36\n",
      "\tEpoch 767 | Batch 0 | Loss   0.37\n",
      "\tEpoch 767 | Batch 0 | Acc   0.94\n",
      "Epoch 767 | Loss   0.36\n",
      "\tEpoch 768 | Batch 0 | Loss   0.36\n",
      "\tEpoch 768 | Batch 0 | Acc   0.97\n",
      "Epoch 768 | Loss   0.37\n",
      "\tEpoch 769 | Batch 0 | Loss   0.35\n",
      "\tEpoch 769 | Batch 0 | Acc   0.97\n",
      "Epoch 769 | Loss   0.37\n",
      "\tEpoch 770 | Batch 0 | Loss   0.38\n",
      "\tEpoch 770 | Batch 0 | Acc   0.92\n",
      "Epoch 770 | Loss   0.36\n",
      "\tEpoch 771 | Batch 0 | Loss   0.33\n",
      "\tEpoch 771 | Batch 0 | Acc   0.98\n",
      "Epoch 771 | Loss   0.36\n",
      "\tEpoch 772 | Batch 0 | Loss   0.34\n",
      "\tEpoch 772 | Batch 0 | Acc   0.98\n",
      "Epoch 772 | Loss   0.37\n",
      "\tEpoch 773 | Batch 0 | Loss   0.33\n",
      "\tEpoch 773 | Batch 0 | Acc   0.98\n",
      "Epoch 773 | Loss   0.37\n",
      "\tEpoch 774 | Batch 0 | Loss   0.38\n",
      "\tEpoch 774 | Batch 0 | Acc   0.94\n",
      "Epoch 774 | Loss   0.36\n",
      "\tEpoch 775 | Batch 0 | Loss   0.35\n",
      "\tEpoch 775 | Batch 0 | Acc   0.97\n",
      "Epoch 775 | Loss   0.36\n",
      "\tEpoch 776 | Batch 0 | Loss   0.37\n",
      "\tEpoch 776 | Batch 0 | Acc   0.95\n",
      "Epoch 776 | Loss   0.36\n",
      "\tEpoch 777 | Batch 0 | Loss   0.36\n",
      "\tEpoch 777 | Batch 0 | Acc   0.95\n",
      "Epoch 777 | Loss   0.37\n",
      "\tEpoch 778 | Batch 0 | Loss   0.36\n",
      "\tEpoch 778 | Batch 0 | Acc   0.95\n",
      "Epoch 778 | Loss   0.36\n",
      "\tEpoch 779 | Batch 0 | Loss   0.37\n",
      "\tEpoch 779 | Batch 0 | Acc   0.95\n",
      "Epoch 779 | Loss   0.37\n",
      "\tEpoch 780 | Batch 0 | Loss   0.38\n",
      "\tEpoch 780 | Batch 0 | Acc   0.94\n",
      "Epoch 780 | Loss   0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 781 | Batch 0 | Loss   0.35\n",
      "\tEpoch 781 | Batch 0 | Acc   0.98\n",
      "Epoch 781 | Loss   0.36\n",
      "\tEpoch 782 | Batch 0 | Loss   0.36\n",
      "\tEpoch 782 | Batch 0 | Acc   0.94\n",
      "Epoch 782 | Loss   0.36\n",
      "\tEpoch 783 | Batch 0 | Loss   0.36\n",
      "\tEpoch 783 | Batch 0 | Acc   0.95\n",
      "Epoch 783 | Loss   0.37\n",
      "\tEpoch 784 | Batch 0 | Loss   0.35\n",
      "\tEpoch 784 | Batch 0 | Acc   0.98\n",
      "Epoch 784 | Loss   0.37\n",
      "\tEpoch 785 | Batch 0 | Loss   0.40\n",
      "\tEpoch 785 | Batch 0 | Acc   0.91\n",
      "Epoch 785 | Loss   0.36\n",
      "\tEpoch 786 | Batch 0 | Loss   0.38\n",
      "\tEpoch 786 | Batch 0 | Acc   0.94\n",
      "Epoch 786 | Loss   0.36\n",
      "\tEpoch 787 | Batch 0 | Loss   0.35\n",
      "\tEpoch 787 | Batch 0 | Acc   0.97\n",
      "Epoch 787 | Loss   0.36\n",
      "\tEpoch 788 | Batch 0 | Loss   0.38\n",
      "\tEpoch 788 | Batch 0 | Acc   0.94\n",
      "Epoch 788 | Loss   0.36\n",
      "\tEpoch 789 | Batch 0 | Loss   0.38\n",
      "\tEpoch 789 | Batch 0 | Acc   0.94\n",
      "Epoch 789 | Loss   0.36\n",
      "\tEpoch 790 | Batch 0 | Loss   0.36\n",
      "\tEpoch 790 | Batch 0 | Acc   0.97\n",
      "Epoch 790 | Loss   0.37\n",
      "\tEpoch 791 | Batch 0 | Loss   0.38\n",
      "\tEpoch 791 | Batch 0 | Acc   0.94\n",
      "Epoch 791 | Loss   0.36\n",
      "\tEpoch 792 | Batch 0 | Loss   0.37\n",
      "\tEpoch 792 | Batch 0 | Acc   0.95\n",
      "Epoch 792 | Loss   0.37\n",
      "\tEpoch 793 | Batch 0 | Loss   0.35\n",
      "\tEpoch 793 | Batch 0 | Acc   0.97\n",
      "Epoch 793 | Loss   0.37\n",
      "\tEpoch 794 | Batch 0 | Loss   0.37\n",
      "\tEpoch 794 | Batch 0 | Acc   0.95\n",
      "Epoch 794 | Loss   0.37\n",
      "\tEpoch 795 | Batch 0 | Loss   0.36\n",
      "\tEpoch 795 | Batch 0 | Acc   0.97\n",
      "Epoch 795 | Loss   0.37\n",
      "\tEpoch 796 | Batch 0 | Loss   0.35\n",
      "\tEpoch 796 | Batch 0 | Acc   0.97\n",
      "Epoch 796 | Loss   0.36\n",
      "\tEpoch 797 | Batch 0 | Loss   0.36\n",
      "\tEpoch 797 | Batch 0 | Acc   0.97\n",
      "Epoch 797 | Loss   0.36\n",
      "\tEpoch 798 | Batch 0 | Loss   0.38\n",
      "\tEpoch 798 | Batch 0 | Acc   0.94\n",
      "Epoch 798 | Loss   0.36\n",
      "\tEpoch 799 | Batch 0 | Loss   0.39\n",
      "\tEpoch 799 | Batch 0 | Acc   0.94\n",
      "Epoch 799 | Loss   0.36\n",
      "\tEpoch 800 | Batch 0 | Loss   0.36\n",
      "\tEpoch 800 | Batch 0 | Acc   0.97\n",
      "Epoch 800 | Loss   0.36\n",
      "\tEpoch 801 | Batch 0 | Loss   0.34\n",
      "\tEpoch 801 | Batch 0 | Acc   0.98\n",
      "Epoch 801 | Loss   0.36\n",
      "\tEpoch 802 | Batch 0 | Loss   0.35\n",
      "\tEpoch 802 | Batch 0 | Acc   0.98\n",
      "Epoch 802 | Loss   0.36\n",
      "\tEpoch 803 | Batch 0 | Loss   0.38\n",
      "\tEpoch 803 | Batch 0 | Acc   0.94\n",
      "Epoch 803 | Loss   0.36\n",
      "\tEpoch 804 | Batch 0 | Loss   0.34\n",
      "\tEpoch 804 | Batch 0 | Acc   0.98\n",
      "Epoch 804 | Loss   0.36\n",
      "\tEpoch 805 | Batch 0 | Loss   0.37\n",
      "\tEpoch 805 | Batch 0 | Acc   0.95\n",
      "Epoch 805 | Loss   0.37\n",
      "\tEpoch 806 | Batch 0 | Loss   0.37\n",
      "\tEpoch 806 | Batch 0 | Acc   0.95\n",
      "Epoch 806 | Loss   0.36\n",
      "\tEpoch 807 | Batch 0 | Loss   0.38\n",
      "\tEpoch 807 | Batch 0 | Acc   0.94\n",
      "Epoch 807 | Loss   0.37\n",
      "\tEpoch 808 | Batch 0 | Loss   0.34\n",
      "\tEpoch 808 | Batch 0 | Acc   0.97\n",
      "Epoch 808 | Loss   0.37\n",
      "\tEpoch 809 | Batch 0 | Loss   0.36\n",
      "\tEpoch 809 | Batch 0 | Acc   0.95\n",
      "Epoch 809 | Loss   0.37\n",
      "\tEpoch 810 | Batch 0 | Loss   0.36\n",
      "\tEpoch 810 | Batch 0 | Acc   0.95\n",
      "Epoch 810 | Loss   0.37\n",
      "\tEpoch 811 | Batch 0 | Loss   0.35\n",
      "\tEpoch 811 | Batch 0 | Acc   0.97\n",
      "Epoch 811 | Loss   0.36\n",
      "\tEpoch 812 | Batch 0 | Loss   0.40\n",
      "\tEpoch 812 | Batch 0 | Acc   0.92\n",
      "Epoch 812 | Loss   0.36\n",
      "\tEpoch 813 | Batch 0 | Loss   0.35\n",
      "\tEpoch 813 | Batch 0 | Acc   0.97\n",
      "Epoch 813 | Loss   0.36\n",
      "\tEpoch 814 | Batch 0 | Loss   0.36\n",
      "\tEpoch 814 | Batch 0 | Acc   0.95\n",
      "Epoch 814 | Loss   0.37\n",
      "\tEpoch 815 | Batch 0 | Loss   0.37\n",
      "\tEpoch 815 | Batch 0 | Acc   0.97\n",
      "Epoch 815 | Loss   0.36\n",
      "\tEpoch 816 | Batch 0 | Loss   0.33\n",
      "\tEpoch 816 | Batch 0 | Acc   0.98\n",
      "Epoch 816 | Loss   0.36\n",
      "\tEpoch 817 | Batch 0 | Loss   0.39\n",
      "\tEpoch 817 | Batch 0 | Acc   0.92\n",
      "Epoch 817 | Loss   0.36\n",
      "\tEpoch 818 | Batch 0 | Loss   0.36\n",
      "\tEpoch 818 | Batch 0 | Acc   0.95\n",
      "Epoch 818 | Loss   0.37\n",
      "\tEpoch 819 | Batch 0 | Loss   0.40\n",
      "\tEpoch 819 | Batch 0 | Acc   0.91\n",
      "Epoch 819 | Loss   0.37\n",
      "\tEpoch 820 | Batch 0 | Loss   0.34\n",
      "\tEpoch 820 | Batch 0 | Acc   0.98\n",
      "Epoch 820 | Loss   0.36\n",
      "\tEpoch 821 | Batch 0 | Loss   0.38\n",
      "\tEpoch 821 | Batch 0 | Acc   0.94\n",
      "Epoch 821 | Loss   0.36\n",
      "\tEpoch 822 | Batch 0 | Loss   0.37\n",
      "\tEpoch 822 | Batch 0 | Acc   0.94\n",
      "Epoch 822 | Loss   0.36\n",
      "\tEpoch 823 | Batch 0 | Loss   0.33\n",
      "\tEpoch 823 | Batch 0 | Acc   1.00\n",
      "Epoch 823 | Loss   0.37\n",
      "\tEpoch 824 | Batch 0 | Loss   0.35\n",
      "\tEpoch 824 | Batch 0 | Acc   0.97\n",
      "Epoch 824 | Loss   0.36\n",
      "\tEpoch 825 | Batch 0 | Loss   0.36\n",
      "\tEpoch 825 | Batch 0 | Acc   0.95\n",
      "Epoch 825 | Loss   0.38\n",
      "\tEpoch 826 | Batch 0 | Loss   0.34\n",
      "\tEpoch 826 | Batch 0 | Acc   0.98\n",
      "Epoch 826 | Loss   0.37\n",
      "\tEpoch 827 | Batch 0 | Loss   0.38\n",
      "\tEpoch 827 | Batch 0 | Acc   0.95\n",
      "Epoch 827 | Loss   0.36\n",
      "\tEpoch 828 | Batch 0 | Loss   0.36\n",
      "\tEpoch 828 | Batch 0 | Acc   0.95\n",
      "Epoch 828 | Loss   0.36\n",
      "\tEpoch 829 | Batch 0 | Loss   0.33\n",
      "\tEpoch 829 | Batch 0 | Acc   1.00\n",
      "Epoch 829 | Loss   0.37\n",
      "\tEpoch 830 | Batch 0 | Loss   0.32\n",
      "\tEpoch 830 | Batch 0 | Acc   1.00\n",
      "Epoch 830 | Loss   0.38\n",
      "\tEpoch 831 | Batch 0 | Loss   0.33\n",
      "\tEpoch 831 | Batch 0 | Acc   0.98\n",
      "Epoch 831 | Loss   0.36\n",
      "\tEpoch 832 | Batch 0 | Loss   0.36\n",
      "\tEpoch 832 | Batch 0 | Acc   0.95\n",
      "Epoch 832 | Loss   0.36\n",
      "\tEpoch 833 | Batch 0 | Loss   0.33\n",
      "\tEpoch 833 | Batch 0 | Acc   0.98\n",
      "Epoch 833 | Loss   0.36\n",
      "\tEpoch 834 | Batch 0 | Loss   0.35\n",
      "\tEpoch 834 | Batch 0 | Acc   0.97\n",
      "Epoch 834 | Loss   0.36\n",
      "\tEpoch 835 | Batch 0 | Loss   0.35\n",
      "\tEpoch 835 | Batch 0 | Acc   0.95\n",
      "Epoch 835 | Loss   0.37\n",
      "\tEpoch 836 | Batch 0 | Loss   0.34\n",
      "\tEpoch 836 | Batch 0 | Acc   0.97\n",
      "Epoch 836 | Loss   0.37\n",
      "\tEpoch 837 | Batch 0 | Loss   0.39\n",
      "\tEpoch 837 | Batch 0 | Acc   0.92\n",
      "Epoch 837 | Loss   0.37\n",
      "\tEpoch 838 | Batch 0 | Loss   0.37\n",
      "\tEpoch 838 | Batch 0 | Acc   0.95\n",
      "Epoch 838 | Loss   0.37\n",
      "\tEpoch 839 | Batch 0 | Loss   0.34\n",
      "\tEpoch 839 | Batch 0 | Acc   0.97\n",
      "Epoch 839 | Loss   0.36\n",
      "\tEpoch 840 | Batch 0 | Loss   0.35\n",
      "\tEpoch 840 | Batch 0 | Acc   0.97\n",
      "Epoch 840 | Loss   0.36\n",
      "\tEpoch 841 | Batch 0 | Loss   0.32\n",
      "\tEpoch 841 | Batch 0 | Acc   1.00\n",
      "Epoch 841 | Loss   0.36\n",
      "\tEpoch 842 | Batch 0 | Loss   0.35\n",
      "\tEpoch 842 | Batch 0 | Acc   0.97\n",
      "Epoch 842 | Loss   0.36\n",
      "\tEpoch 843 | Batch 0 | Loss   0.32\n",
      "\tEpoch 843 | Batch 0 | Acc   1.00\n",
      "Epoch 843 | Loss   0.36\n",
      "\tEpoch 844 | Batch 0 | Loss   0.34\n",
      "\tEpoch 844 | Batch 0 | Acc   0.98\n",
      "Epoch 844 | Loss   0.36\n",
      "\tEpoch 845 | Batch 0 | Loss   0.33\n",
      "\tEpoch 845 | Batch 0 | Acc   1.00\n",
      "Epoch 845 | Loss   0.37\n",
      "\tEpoch 846 | Batch 0 | Loss   0.35\n",
      "\tEpoch 846 | Batch 0 | Acc   0.97\n",
      "Epoch 846 | Loss   0.36\n",
      "\tEpoch 847 | Batch 0 | Loss   0.36\n",
      "\tEpoch 847 | Batch 0 | Acc   0.95\n",
      "Epoch 847 | Loss   0.36\n",
      "\tEpoch 848 | Batch 0 | Loss   0.39\n",
      "\tEpoch 848 | Batch 0 | Acc   0.92\n",
      "Epoch 848 | Loss   0.36\n",
      "\tEpoch 849 | Batch 0 | Loss   0.34\n",
      "\tEpoch 849 | Batch 0 | Acc   0.98\n",
      "Epoch 849 | Loss   0.37\n",
      "\tEpoch 850 | Batch 0 | Loss   0.35\n",
      "\tEpoch 850 | Batch 0 | Acc   0.97\n",
      "Epoch 850 | Loss   0.36\n",
      "\tEpoch 851 | Batch 0 | Loss   0.38\n",
      "\tEpoch 851 | Batch 0 | Acc   0.94\n",
      "Epoch 851 | Loss   0.36\n",
      "\tEpoch 852 | Batch 0 | Loss   0.34\n",
      "\tEpoch 852 | Batch 0 | Acc   0.98\n",
      "Epoch 852 | Loss   0.36\n",
      "\tEpoch 853 | Batch 0 | Loss   0.36\n",
      "\tEpoch 853 | Batch 0 | Acc   0.97\n",
      "Epoch 853 | Loss   0.37\n",
      "\tEpoch 854 | Batch 0 | Loss   0.33\n",
      "\tEpoch 854 | Batch 0 | Acc   0.98\n",
      "Epoch 854 | Loss   0.36\n",
      "\tEpoch 855 | Batch 0 | Loss   0.37\n",
      "\tEpoch 855 | Batch 0 | Acc   0.95\n",
      "Epoch 855 | Loss   0.36\n",
      "\tEpoch 856 | Batch 0 | Loss   0.37\n",
      "\tEpoch 856 | Batch 0 | Acc   0.94\n",
      "Epoch 856 | Loss   0.37\n",
      "\tEpoch 857 | Batch 0 | Loss   0.35\n",
      "\tEpoch 857 | Batch 0 | Acc   0.97\n",
      "Epoch 857 | Loss   0.36\n",
      "\tEpoch 858 | Batch 0 | Loss   0.40\n",
      "\tEpoch 858 | Batch 0 | Acc   0.92\n",
      "Epoch 858 | Loss   0.36\n",
      "\tEpoch 859 | Batch 0 | Loss   0.39\n",
      "\tEpoch 859 | Batch 0 | Acc   0.94\n",
      "Epoch 859 | Loss   0.36\n",
      "\tEpoch 860 | Batch 0 | Loss   0.36\n",
      "\tEpoch 860 | Batch 0 | Acc   0.95\n",
      "Epoch 860 | Loss   0.36\n",
      "\tEpoch 861 | Batch 0 | Loss   0.40\n",
      "\tEpoch 861 | Batch 0 | Acc   0.92\n",
      "Epoch 861 | Loss   0.38\n",
      "\tEpoch 862 | Batch 0 | Loss   0.36\n",
      "\tEpoch 862 | Batch 0 | Acc   0.97\n",
      "Epoch 862 | Loss   0.36\n",
      "\tEpoch 863 | Batch 0 | Loss   0.35\n",
      "\tEpoch 863 | Batch 0 | Acc   0.97\n",
      "Epoch 863 | Loss   0.37\n",
      "\tEpoch 864 | Batch 0 | Loss   0.34\n",
      "\tEpoch 864 | Batch 0 | Acc   0.98\n",
      "Epoch 864 | Loss   0.37\n",
      "\tEpoch 865 | Batch 0 | Loss   0.39\n",
      "\tEpoch 865 | Batch 0 | Acc   0.91\n",
      "Epoch 865 | Loss   0.36\n",
      "\tEpoch 866 | Batch 0 | Loss   0.36\n",
      "\tEpoch 866 | Batch 0 | Acc   0.97\n",
      "Epoch 866 | Loss   0.36\n",
      "\tEpoch 867 | Batch 0 | Loss   0.34\n",
      "\tEpoch 867 | Batch 0 | Acc   0.97\n",
      "Epoch 867 | Loss   0.36\n",
      "\tEpoch 868 | Batch 0 | Loss   0.34\n",
      "\tEpoch 868 | Batch 0 | Acc   0.98\n",
      "Epoch 868 | Loss   0.36\n",
      "\tEpoch 869 | Batch 0 | Loss   0.37\n",
      "\tEpoch 869 | Batch 0 | Acc   0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 869 | Loss   0.36\n",
      "\tEpoch 870 | Batch 0 | Loss   0.36\n",
      "\tEpoch 870 | Batch 0 | Acc   0.97\n",
      "Epoch 870 | Loss   0.36\n",
      "\tEpoch 871 | Batch 0 | Loss   0.37\n",
      "\tEpoch 871 | Batch 0 | Acc   0.94\n",
      "Epoch 871 | Loss   0.36\n",
      "\tEpoch 872 | Batch 0 | Loss   0.36\n",
      "\tEpoch 872 | Batch 0 | Acc   0.95\n",
      "Epoch 872 | Loss   0.36\n",
      "\tEpoch 873 | Batch 0 | Loss   0.36\n",
      "\tEpoch 873 | Batch 0 | Acc   0.95\n",
      "Epoch 873 | Loss   0.36\n",
      "\tEpoch 874 | Batch 0 | Loss   0.36\n",
      "\tEpoch 874 | Batch 0 | Acc   0.95\n",
      "Epoch 874 | Loss   0.36\n",
      "\tEpoch 875 | Batch 0 | Loss   0.38\n",
      "\tEpoch 875 | Batch 0 | Acc   0.94\n",
      "Epoch 875 | Loss   0.36\n",
      "\tEpoch 876 | Batch 0 | Loss   0.33\n",
      "\tEpoch 876 | Batch 0 | Acc   0.98\n",
      "Epoch 876 | Loss   0.36\n",
      "\tEpoch 877 | Batch 0 | Loss   0.35\n",
      "\tEpoch 877 | Batch 0 | Acc   0.97\n",
      "Epoch 877 | Loss   0.36\n",
      "\tEpoch 878 | Batch 0 | Loss   0.33\n",
      "\tEpoch 878 | Batch 0 | Acc   0.98\n",
      "Epoch 878 | Loss   0.36\n",
      "\tEpoch 879 | Batch 0 | Loss   0.38\n",
      "\tEpoch 879 | Batch 0 | Acc   0.92\n",
      "Epoch 879 | Loss   0.36\n",
      "\tEpoch 880 | Batch 0 | Loss   0.39\n",
      "\tEpoch 880 | Batch 0 | Acc   0.94\n",
      "Epoch 880 | Loss   0.36\n",
      "\tEpoch 881 | Batch 0 | Loss   0.36\n",
      "\tEpoch 881 | Batch 0 | Acc   0.95\n",
      "Epoch 881 | Loss   0.36\n",
      "\tEpoch 882 | Batch 0 | Loss   0.36\n",
      "\tEpoch 882 | Batch 0 | Acc   0.95\n",
      "Epoch 882 | Loss   0.37\n",
      "\tEpoch 883 | Batch 0 | Loss   0.36\n",
      "\tEpoch 883 | Batch 0 | Acc   0.95\n",
      "Epoch 883 | Loss   0.36\n",
      "\tEpoch 884 | Batch 0 | Loss   0.39\n",
      "\tEpoch 884 | Batch 0 | Acc   0.92\n",
      "Epoch 884 | Loss   0.37\n",
      "\tEpoch 885 | Batch 0 | Loss   0.35\n",
      "\tEpoch 885 | Batch 0 | Acc   0.97\n",
      "Epoch 885 | Loss   0.37\n",
      "\tEpoch 886 | Batch 0 | Loss   0.37\n",
      "\tEpoch 886 | Batch 0 | Acc   0.94\n",
      "Epoch 886 | Loss   0.36\n",
      "\tEpoch 887 | Batch 0 | Loss   0.38\n",
      "\tEpoch 887 | Batch 0 | Acc   0.94\n",
      "Epoch 887 | Loss   0.36\n",
      "\tEpoch 888 | Batch 0 | Loss   0.35\n",
      "\tEpoch 888 | Batch 0 | Acc   0.97\n",
      "Epoch 888 | Loss   0.36\n",
      "\tEpoch 889 | Batch 0 | Loss   0.35\n",
      "\tEpoch 889 | Batch 0 | Acc   0.97\n",
      "Epoch 889 | Loss   0.36\n",
      "\tEpoch 890 | Batch 0 | Loss   0.33\n",
      "\tEpoch 890 | Batch 0 | Acc   1.00\n",
      "Epoch 890 | Loss   0.36\n",
      "\tEpoch 891 | Batch 0 | Loss   0.36\n",
      "\tEpoch 891 | Batch 0 | Acc   0.95\n",
      "Epoch 891 | Loss   0.36\n",
      "\tEpoch 892 | Batch 0 | Loss   0.36\n",
      "\tEpoch 892 | Batch 0 | Acc   0.95\n",
      "Epoch 892 | Loss   0.36\n",
      "\tEpoch 893 | Batch 0 | Loss   0.38\n",
      "\tEpoch 893 | Batch 0 | Acc   0.94\n",
      "Epoch 893 | Loss   0.37\n",
      "\tEpoch 894 | Batch 0 | Loss   0.36\n",
      "\tEpoch 894 | Batch 0 | Acc   0.95\n",
      "Epoch 894 | Loss   0.36\n",
      "\tEpoch 895 | Batch 0 | Loss   0.34\n",
      "\tEpoch 895 | Batch 0 | Acc   0.98\n",
      "Epoch 895 | Loss   0.37\n",
      "\tEpoch 896 | Batch 0 | Loss   0.34\n",
      "\tEpoch 896 | Batch 0 | Acc   0.98\n",
      "Epoch 896 | Loss   0.37\n",
      "\tEpoch 897 | Batch 0 | Loss   0.36\n",
      "\tEpoch 897 | Batch 0 | Acc   0.97\n",
      "Epoch 897 | Loss   0.37\n",
      "\tEpoch 898 | Batch 0 | Loss   0.36\n",
      "\tEpoch 898 | Batch 0 | Acc   0.95\n",
      "Epoch 898 | Loss   0.36\n",
      "\tEpoch 899 | Batch 0 | Loss   0.35\n",
      "\tEpoch 899 | Batch 0 | Acc   0.97\n",
      "Epoch 899 | Loss   0.36\n",
      "\tEpoch 900 | Batch 0 | Loss   0.35\n",
      "\tEpoch 900 | Batch 0 | Acc   0.97\n",
      "Epoch 900 | Loss   0.37\n",
      "\tEpoch 901 | Batch 0 | Loss   0.34\n",
      "\tEpoch 901 | Batch 0 | Acc   0.98\n",
      "Epoch 901 | Loss   0.36\n",
      "\tEpoch 902 | Batch 0 | Loss   0.35\n",
      "\tEpoch 902 | Batch 0 | Acc   0.97\n",
      "Epoch 902 | Loss   0.37\n",
      "\tEpoch 903 | Batch 0 | Loss   0.39\n",
      "\tEpoch 903 | Batch 0 | Acc   0.92\n",
      "Epoch 903 | Loss   0.36\n",
      "\tEpoch 904 | Batch 0 | Loss   0.40\n",
      "\tEpoch 904 | Batch 0 | Acc   0.92\n",
      "Epoch 904 | Loss   0.36\n",
      "\tEpoch 905 | Batch 0 | Loss   0.36\n",
      "\tEpoch 905 | Batch 0 | Acc   0.97\n",
      "Epoch 905 | Loss   0.36\n",
      "\tEpoch 906 | Batch 0 | Loss   0.35\n",
      "\tEpoch 906 | Batch 0 | Acc   0.97\n",
      "Epoch 906 | Loss   0.36\n",
      "\tEpoch 907 | Batch 0 | Loss   0.37\n",
      "\tEpoch 907 | Batch 0 | Acc   0.95\n",
      "Epoch 907 | Loss   0.37\n",
      "\tEpoch 908 | Batch 0 | Loss   0.34\n",
      "\tEpoch 908 | Batch 0 | Acc   0.98\n",
      "Epoch 908 | Loss   0.36\n",
      "\tEpoch 909 | Batch 0 | Loss   0.35\n",
      "\tEpoch 909 | Batch 0 | Acc   0.98\n",
      "Epoch 909 | Loss   0.36\n",
      "\tEpoch 910 | Batch 0 | Loss   0.36\n",
      "\tEpoch 910 | Batch 0 | Acc   0.95\n",
      "Epoch 910 | Loss   0.36\n",
      "\tEpoch 911 | Batch 0 | Loss   0.38\n",
      "\tEpoch 911 | Batch 0 | Acc   0.92\n",
      "Epoch 911 | Loss   0.36\n",
      "\tEpoch 912 | Batch 0 | Loss   0.36\n",
      "\tEpoch 912 | Batch 0 | Acc   0.95\n",
      "Epoch 912 | Loss   0.36\n",
      "\tEpoch 913 | Batch 0 | Loss   0.33\n",
      "\tEpoch 913 | Batch 0 | Acc   0.98\n",
      "Epoch 913 | Loss   0.37\n",
      "\tEpoch 914 | Batch 0 | Loss   0.32\n",
      "\tEpoch 914 | Batch 0 | Acc   1.00\n",
      "Epoch 914 | Loss   0.36\n",
      "\tEpoch 915 | Batch 0 | Loss   0.34\n",
      "\tEpoch 915 | Batch 0 | Acc   0.98\n",
      "Epoch 915 | Loss   0.37\n",
      "\tEpoch 916 | Batch 0 | Loss   0.37\n",
      "\tEpoch 916 | Batch 0 | Acc   0.94\n",
      "Epoch 916 | Loss   0.36\n",
      "\tEpoch 917 | Batch 0 | Loss   0.35\n",
      "\tEpoch 917 | Batch 0 | Acc   0.97\n",
      "Epoch 917 | Loss   0.36\n",
      "\tEpoch 918 | Batch 0 | Loss   0.40\n",
      "\tEpoch 918 | Batch 0 | Acc   0.91\n",
      "Epoch 918 | Loss   0.36\n",
      "\tEpoch 919 | Batch 0 | Loss   0.34\n",
      "\tEpoch 919 | Batch 0 | Acc   0.97\n",
      "Epoch 919 | Loss   0.36\n",
      "\tEpoch 920 | Batch 0 | Loss   0.36\n",
      "\tEpoch 920 | Batch 0 | Acc   0.95\n",
      "Epoch 920 | Loss   0.36\n",
      "\tEpoch 921 | Batch 0 | Loss   0.36\n",
      "\tEpoch 921 | Batch 0 | Acc   0.95\n",
      "Epoch 921 | Loss   0.36\n",
      "\tEpoch 922 | Batch 0 | Loss   0.34\n",
      "\tEpoch 922 | Batch 0 | Acc   0.98\n",
      "Epoch 922 | Loss   0.36\n",
      "\tEpoch 923 | Batch 0 | Loss   0.35\n",
      "\tEpoch 923 | Batch 0 | Acc   0.95\n",
      "Epoch 923 | Loss   0.36\n",
      "\tEpoch 924 | Batch 0 | Loss   0.35\n",
      "\tEpoch 924 | Batch 0 | Acc   0.97\n",
      "Epoch 924 | Loss   0.37\n",
      "\tEpoch 925 | Batch 0 | Loss   0.37\n",
      "\tEpoch 925 | Batch 0 | Acc   0.94\n",
      "Epoch 925 | Loss   0.37\n",
      "\tEpoch 926 | Batch 0 | Loss   0.39\n",
      "\tEpoch 926 | Batch 0 | Acc   0.92\n",
      "Epoch 926 | Loss   0.36\n",
      "\tEpoch 927 | Batch 0 | Loss   0.37\n",
      "\tEpoch 927 | Batch 0 | Acc   0.95\n",
      "Epoch 927 | Loss   0.36\n",
      "\tEpoch 928 | Batch 0 | Loss   0.34\n",
      "\tEpoch 928 | Batch 0 | Acc   0.98\n",
      "Epoch 928 | Loss   0.37\n",
      "\tEpoch 929 | Batch 0 | Loss   0.39\n",
      "\tEpoch 929 | Batch 0 | Acc   0.92\n",
      "Epoch 929 | Loss   0.36\n",
      "\tEpoch 930 | Batch 0 | Loss   0.36\n",
      "\tEpoch 930 | Batch 0 | Acc   0.95\n",
      "Epoch 930 | Loss   0.36\n",
      "\tEpoch 931 | Batch 0 | Loss   0.40\n",
      "\tEpoch 931 | Batch 0 | Acc   0.91\n",
      "Epoch 931 | Loss   0.36\n",
      "\tEpoch 932 | Batch 0 | Loss   0.36\n",
      "\tEpoch 932 | Batch 0 | Acc   0.95\n",
      "Epoch 932 | Loss   0.36\n",
      "\tEpoch 933 | Batch 0 | Loss   0.35\n",
      "\tEpoch 933 | Batch 0 | Acc   0.97\n",
      "Epoch 933 | Loss   0.37\n",
      "\tEpoch 934 | Batch 0 | Loss   0.37\n",
      "\tEpoch 934 | Batch 0 | Acc   0.95\n",
      "Epoch 934 | Loss   0.36\n",
      "\tEpoch 935 | Batch 0 | Loss   0.35\n",
      "\tEpoch 935 | Batch 0 | Acc   0.97\n",
      "Epoch 935 | Loss   0.37\n",
      "\tEpoch 936 | Batch 0 | Loss   0.34\n",
      "\tEpoch 936 | Batch 0 | Acc   0.97\n",
      "Epoch 936 | Loss   0.37\n",
      "\tEpoch 937 | Batch 0 | Loss   0.33\n",
      "\tEpoch 937 | Batch 0 | Acc   1.00\n",
      "Epoch 937 | Loss   0.36\n",
      "\tEpoch 938 | Batch 0 | Loss   0.33\n",
      "\tEpoch 938 | Batch 0 | Acc   0.98\n",
      "Epoch 938 | Loss   0.37\n",
      "\tEpoch 939 | Batch 0 | Loss   0.37\n",
      "\tEpoch 939 | Batch 0 | Acc   0.94\n",
      "Epoch 939 | Loss   0.37\n",
      "\tEpoch 940 | Batch 0 | Loss   0.35\n",
      "\tEpoch 940 | Batch 0 | Acc   0.97\n",
      "Epoch 940 | Loss   0.36\n",
      "\tEpoch 941 | Batch 0 | Loss   0.35\n",
      "\tEpoch 941 | Batch 0 | Acc   0.97\n",
      "Epoch 941 | Loss   0.36\n",
      "\tEpoch 942 | Batch 0 | Loss   0.36\n",
      "\tEpoch 942 | Batch 0 | Acc   0.95\n",
      "Epoch 942 | Loss   0.36\n",
      "\tEpoch 943 | Batch 0 | Loss   0.34\n",
      "\tEpoch 943 | Batch 0 | Acc   0.98\n",
      "Epoch 943 | Loss   0.36\n",
      "\tEpoch 944 | Batch 0 | Loss   0.35\n",
      "\tEpoch 944 | Batch 0 | Acc   0.95\n",
      "Epoch 944 | Loss   0.37\n",
      "\tEpoch 945 | Batch 0 | Loss   0.39\n",
      "\tEpoch 945 | Batch 0 | Acc   0.92\n",
      "Epoch 945 | Loss   0.37\n",
      "\tEpoch 946 | Batch 0 | Loss   0.37\n",
      "\tEpoch 946 | Batch 0 | Acc   0.95\n",
      "Epoch 946 | Loss   0.36\n",
      "\tEpoch 947 | Batch 0 | Loss   0.35\n",
      "\tEpoch 947 | Batch 0 | Acc   0.95\n",
      "Epoch 947 | Loss   0.36\n",
      "\tEpoch 948 | Batch 0 | Loss   0.38\n",
      "\tEpoch 948 | Batch 0 | Acc   0.94\n",
      "Epoch 948 | Loss   0.36\n",
      "\tEpoch 949 | Batch 0 | Loss   0.33\n",
      "\tEpoch 949 | Batch 0 | Acc   0.98\n",
      "Epoch 949 | Loss   0.36\n",
      "\tEpoch 950 | Batch 0 | Loss   0.35\n",
      "\tEpoch 950 | Batch 0 | Acc   0.97\n",
      "Epoch 950 | Loss   0.36\n",
      "\tEpoch 951 | Batch 0 | Loss   0.35\n",
      "\tEpoch 951 | Batch 0 | Acc   0.98\n",
      "Epoch 951 | Loss   0.36\n",
      "\tEpoch 952 | Batch 0 | Loss   0.38\n",
      "\tEpoch 952 | Batch 0 | Acc   0.92\n",
      "Epoch 952 | Loss   0.36\n",
      "\tEpoch 953 | Batch 0 | Loss   0.38\n",
      "\tEpoch 953 | Batch 0 | Acc   0.92\n",
      "Epoch 953 | Loss   0.36\n",
      "\tEpoch 954 | Batch 0 | Loss   0.35\n",
      "\tEpoch 954 | Batch 0 | Acc   0.97\n",
      "Epoch 954 | Loss   0.37\n",
      "\tEpoch 955 | Batch 0 | Loss   0.37\n",
      "\tEpoch 955 | Batch 0 | Acc   0.92\n",
      "Epoch 955 | Loss   0.37\n",
      "\tEpoch 956 | Batch 0 | Loss   0.35\n",
      "\tEpoch 956 | Batch 0 | Acc   0.97\n",
      "Epoch 956 | Loss   0.36\n",
      "\tEpoch 957 | Batch 0 | Loss   0.34\n",
      "\tEpoch 957 | Batch 0 | Acc   0.98\n",
      "Epoch 957 | Loss   0.37\n",
      "\tEpoch 958 | Batch 0 | Loss   0.36\n",
      "\tEpoch 958 | Batch 0 | Acc   0.97\n",
      "Epoch 958 | Loss   0.36\n",
      "\tEpoch 959 | Batch 0 | Loss   0.38\n",
      "\tEpoch 959 | Batch 0 | Acc   0.94\n",
      "Epoch 959 | Loss   0.38\n",
      "\tEpoch 960 | Batch 0 | Loss   0.40\n",
      "\tEpoch 960 | Batch 0 | Acc   0.91\n",
      "Epoch 960 | Loss   0.36\n",
      "\tEpoch 961 | Batch 0 | Loss   0.34\n",
      "\tEpoch 961 | Batch 0 | Acc   0.98\n",
      "Epoch 961 | Loss   0.36\n",
      "\tEpoch 962 | Batch 0 | Loss   0.37\n",
      "\tEpoch 962 | Batch 0 | Acc   0.95\n",
      "Epoch 962 | Loss   0.37\n",
      "\tEpoch 963 | Batch 0 | Loss   0.33\n",
      "\tEpoch 963 | Batch 0 | Acc   0.98\n",
      "Epoch 963 | Loss   0.36\n",
      "\tEpoch 964 | Batch 0 | Loss   0.36\n",
      "\tEpoch 964 | Batch 0 | Acc   0.95\n",
      "Epoch 964 | Loss   0.37\n",
      "\tEpoch 965 | Batch 0 | Loss   0.33\n",
      "\tEpoch 965 | Batch 0 | Acc   0.98\n",
      "Epoch 965 | Loss   0.37\n",
      "\tEpoch 966 | Batch 0 | Loss   0.35\n",
      "\tEpoch 966 | Batch 0 | Acc   0.97\n",
      "Epoch 966 | Loss   0.37\n",
      "\tEpoch 967 | Batch 0 | Loss   0.37\n",
      "\tEpoch 967 | Batch 0 | Acc   0.92\n",
      "Epoch 967 | Loss   0.37\n",
      "\tEpoch 968 | Batch 0 | Loss   0.38\n",
      "\tEpoch 968 | Batch 0 | Acc   0.94\n",
      "Epoch 968 | Loss   0.37\n",
      "\tEpoch 969 | Batch 0 | Loss   0.36\n",
      "\tEpoch 969 | Batch 0 | Acc   0.95\n",
      "Epoch 969 | Loss   0.37\n",
      "\tEpoch 970 | Batch 0 | Loss   0.39\n",
      "\tEpoch 970 | Batch 0 | Acc   0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 970 | Loss   0.36\n",
      "\tEpoch 971 | Batch 0 | Loss   0.36\n",
      "\tEpoch 971 | Batch 0 | Acc   0.94\n",
      "Epoch 971 | Loss   0.37\n",
      "\tEpoch 972 | Batch 0 | Loss   0.35\n",
      "\tEpoch 972 | Batch 0 | Acc   0.97\n",
      "Epoch 972 | Loss   0.37\n",
      "\tEpoch 973 | Batch 0 | Loss   0.34\n",
      "\tEpoch 973 | Batch 0 | Acc   0.98\n",
      "Epoch 973 | Loss   0.36\n",
      "\tEpoch 974 | Batch 0 | Loss   0.36\n",
      "\tEpoch 974 | Batch 0 | Acc   0.95\n",
      "Epoch 974 | Loss   0.37\n",
      "\tEpoch 975 | Batch 0 | Loss   0.36\n",
      "\tEpoch 975 | Batch 0 | Acc   0.95\n",
      "Epoch 975 | Loss   0.36\n",
      "\tEpoch 976 | Batch 0 | Loss   0.38\n",
      "\tEpoch 976 | Batch 0 | Acc   0.94\n",
      "Epoch 976 | Loss   0.37\n",
      "\tEpoch 977 | Batch 0 | Loss   0.35\n",
      "\tEpoch 977 | Batch 0 | Acc   0.97\n",
      "Epoch 977 | Loss   0.36\n",
      "\tEpoch 978 | Batch 0 | Loss   0.38\n",
      "\tEpoch 978 | Batch 0 | Acc   0.94\n",
      "Epoch 978 | Loss   0.36\n",
      "\tEpoch 979 | Batch 0 | Loss   0.37\n",
      "\tEpoch 979 | Batch 0 | Acc   0.94\n",
      "Epoch 979 | Loss   0.36\n",
      "\tEpoch 980 | Batch 0 | Loss   0.38\n",
      "\tEpoch 980 | Batch 0 | Acc   0.94\n",
      "Epoch 980 | Loss   0.36\n",
      "\tEpoch 981 | Batch 0 | Loss   0.39\n",
      "\tEpoch 981 | Batch 0 | Acc   0.94\n",
      "Epoch 981 | Loss   0.37\n",
      "\tEpoch 982 | Batch 0 | Loss   0.35\n",
      "\tEpoch 982 | Batch 0 | Acc   0.97\n",
      "Epoch 982 | Loss   0.36\n",
      "\tEpoch 983 | Batch 0 | Loss   0.34\n",
      "\tEpoch 983 | Batch 0 | Acc   0.98\n",
      "Epoch 983 | Loss   0.36\n",
      "\tEpoch 984 | Batch 0 | Loss   0.34\n",
      "\tEpoch 984 | Batch 0 | Acc   0.98\n",
      "Epoch 984 | Loss   0.36\n",
      "\tEpoch 985 | Batch 0 | Loss   0.33\n",
      "\tEpoch 985 | Batch 0 | Acc   1.00\n",
      "Epoch 985 | Loss   0.36\n",
      "\tEpoch 986 | Batch 0 | Loss   0.33\n",
      "\tEpoch 986 | Batch 0 | Acc   0.98\n",
      "Epoch 986 | Loss   0.37\n",
      "\tEpoch 987 | Batch 0 | Loss   0.37\n",
      "\tEpoch 987 | Batch 0 | Acc   0.95\n",
      "Epoch 987 | Loss   0.36\n",
      "\tEpoch 988 | Batch 0 | Loss   0.34\n",
      "\tEpoch 988 | Batch 0 | Acc   0.97\n",
      "Epoch 988 | Loss   0.37\n",
      "\tEpoch 989 | Batch 0 | Loss   0.40\n",
      "\tEpoch 989 | Batch 0 | Acc   0.92\n",
      "Epoch 989 | Loss   0.36\n",
      "\tEpoch 990 | Batch 0 | Loss   0.38\n",
      "\tEpoch 990 | Batch 0 | Acc   0.94\n",
      "Epoch 990 | Loss   0.36\n",
      "\tEpoch 991 | Batch 0 | Loss   0.34\n",
      "\tEpoch 991 | Batch 0 | Acc   0.98\n",
      "Epoch 991 | Loss   0.37\n",
      "\tEpoch 992 | Batch 0 | Loss   0.36\n",
      "\tEpoch 992 | Batch 0 | Acc   0.94\n",
      "Epoch 992 | Loss   0.37\n",
      "\tEpoch 993 | Batch 0 | Loss   0.41\n",
      "\tEpoch 993 | Batch 0 | Acc   0.89\n",
      "Epoch 993 | Loss   0.37\n",
      "\tEpoch 994 | Batch 0 | Loss   0.35\n",
      "\tEpoch 994 | Batch 0 | Acc   0.97\n",
      "Epoch 994 | Loss   0.37\n",
      "\tEpoch 995 | Batch 0 | Loss   0.37\n",
      "\tEpoch 995 | Batch 0 | Acc   0.94\n",
      "Epoch 995 | Loss   0.36\n",
      "\tEpoch 996 | Batch 0 | Loss   0.39\n",
      "\tEpoch 996 | Batch 0 | Acc   0.94\n",
      "Epoch 996 | Loss   0.36\n",
      "\tEpoch 997 | Batch 0 | Loss   0.38\n",
      "\tEpoch 997 | Batch 0 | Acc   0.92\n",
      "Epoch 997 | Loss   0.36\n",
      "\tEpoch 998 | Batch 0 | Loss   0.37\n",
      "\tEpoch 998 | Batch 0 | Acc   0.94\n",
      "Epoch 998 | Loss   0.37\n",
      "\tEpoch 999 | Batch 0 | Loss   0.36\n",
      "\tEpoch 999 | Batch 0 | Acc   0.97\n",
      "Epoch 999 | Loss   0.37\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = []\n",
    "    for batch_num, input_data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = input_data\n",
    "        x = x.to(device).float()\n",
    "        y = y.to(device)\n",
    "\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_num % 40 == 0:\n",
    "            print('\\tEpoch %d | Batch %d | Loss %6.2f' % (epoch, batch_num, loss.item()))\n",
    "            y_pred = output.detach().cpu().argmax(dim=1).numpy()\n",
    "            y_score = output.detach().cpu().max(dim=1).values\n",
    "            print('\\tEpoch %d | Batch %d | Acc %6.2f' % (epoch, batch_num, metrics.accuracy_score(y.detach().cpu().numpy(),y_pred)))\n",
    "    print('Epoch %d | Loss %6.2f' % (epoch, sum(losses)/len(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity I choose 3 layers with the same number of neurons as there are features in my data set\n",
    "mlp_hidden_layer_sizes = [(3,3)]\n",
    "mlp_activation = ['tanh', 'relu', 'logistic']\n",
    "mlp_solver = ['lbfgs', 'sgd', 'adam']\n",
    "mlp_alpha = np.linspace(0.0001,0.1,10)\n",
    "mlp_eta = ['constant','invscaling','adaptive']\n",
    "mlp_grid = {'mlpclassifier__hidden_layer_sizes': mlp_hidden_layer_sizes,\n",
    "            'mlpclassifier__activation': mlp_activation,\n",
    "            'mlpclassifier__solver': mlp_solver,\n",
    "            'mlpclassifier__alpha': mlp_alpha,\n",
    "            'mlpclassifier__learning_rate': mlp_eta\n",
    "}\n",
    "\n",
    "mlp = MLPClassifier(random_state = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(PowerTransformer(method='yeo-johnson',standardize = True),\n",
    "                                    PCA(n_components = 0.99),\n",
    "                                    mlp)\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_mlp = RandomizedSearchCV(estimator=pipe_lr, param_distributions = mlp_grid, cv = cv, n_jobs=-1, verbose=True, scoring = 'roc_auc', refit = True, n_iter = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=4, random_state=32, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('powertransformer',\n",
       "                                              PowerTransformer()),\n",
       "                                             ('pca', PCA(n_components=0.99)),\n",
       "                                             ('mlpclassifier',\n",
       "                                              MLPClassifier(random_state=32))]),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'mlpclassifier__activation': ['tanh',\n",
       "                                                                      'relu',\n",
       "                                                                      'logistic'],\n",
       "                                        'mlpclassifier__alpha': array([0.0001, 0.0112, 0.0223, 0.0334, 0.0445, 0.0556, 0.0667, 0.0778,\n",
       "       0.0889, 0.1   ]),\n",
       "                                        'mlpclassifier__hidden_layer_sizes': [(3,\n",
       "                                                                               3)],\n",
       "                                        'mlpclassifier__learning_rate': ['constant',\n",
       "                                                                         'invscaling',\n",
       "                                                                         'adaptive'],\n",
       "                                        'mlpclassifier__solver': ['lbfgs',\n",
       "                                                                  'sgd',\n",
       "                                                                  'adam']},\n",
       "                   scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to dataset\n",
    "\n",
    "raw_data_path = './original_score_all.csv'\n",
    "\n",
    "df = pd.read_csv(raw_data_path)\n",
    "\n",
    "df.columns=['ID', 'score_max', 'score_mean', 'score_median', 'class']\n",
    "\n",
    "# split X (features), y (PD stage label) from the dataframe\n",
    "features = ['score_max', 'score_mean', 'score_median']\n",
    "\n",
    "X = df[features]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=rand_state)\n",
    "\n",
    "# fit a standardScaler to normalize all input to zero mean and unit variance\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(torch.from_numpy(X_test.values).to(device).float()).detach().cpu().argmax(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the evaluation result of MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def evaluate_clustering(lst_true_label, lst_pred_label, encoding=None, affinity=None, eval_method=range(1,10),**kwargs):\n",
    "    '''\n",
    "    Output: a dict with 'method_name'-'score' as key-value pairs.\n",
    "\n",
    "    - `eval_method` (list) indicates which method to use.\n",
    "        Nine types of evaluation methods available :\n",
    "            0. Accuracy, Precision, Recall, F1-Score\n",
    "            1. Rand Score (Index) and Adjusted Rand Score (Index)\n",
    "            2. Mutual Information based scores (NMI score and AMI score)\n",
    "            3. Homogeneity, completeness and V-measure\n",
    "            4. Fowlkes-Mallows scores\n",
    "            5. Silhouette Coefficient\n",
    "            6. Calinski-Harabasz Index\n",
    "            7. Davies-Bouldin Index\n",
    "            8. Contingency Matrix\n",
    "            9. Pair Confusion Matrix\n",
    "\n",
    "    - The input `encoding` is a must-have input only for \n",
    "        `5. Silhouette Coefficient`, \n",
    "        `6. Calinski-Harabasz Index`, \n",
    "        `7. Davies-Bouldin Index`. \n",
    "\n",
    "    - The input `affinity` is an optional input only for `5. Silhouette Coefficient` method   \n",
    "    \n",
    "    Note that :\n",
    "    1/ the following clustering evaluation methods are symmetric: swapping the argument does not change the scores.\n",
    "        1. Rand Index (RI), Adjusted Rand Index (ARI)\n",
    "        2. Mutual Information (MI), Adjusted Mutual Information (AMI), Normalised Mutual Information (NMI)\n",
    "        3. V-measure Score\n",
    "\n",
    "    2/ For Precision, Recall, F1-Score, Fbeta in method 0, by default the `1`s in the true/pred label are regraded as `Positive Examlpe`, but a `pos_label` parameter can be fed to them to indicate the positive labels.\n",
    "\n",
    "    More details at: https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "    '''\n",
    "    res = {}\n",
    "    # 0. Accuracy, Precision, Recall, F1-Score, Fbeta\n",
    "    if 0 in eval_method:\n",
    "        res[\"0_accuracy\"] = metrics.accuracy_score(lst_true_label, lst_pred_label)\n",
    "        pos_label = kwargs[\"pos_label\"] if \"pos_label\" in kwargs.keys() else 1\n",
    "        res[\"0_precision\"],res[\"0_sensitivity\"],res[\"0_f1_score\"] = \\\n",
    "            metrics.precision_score(lst_true_label, lst_pred_label,pos_label=pos_label), metrics.recall_score(lst_true_label, lst_pred_label,pos_label=pos_label), metrics.f1_score(lst_true_label, lst_pred_label,pos_label=pos_label)\n",
    "        cm = metrics.confusion_matrix(lst_true_label, lst_pred_label)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        res[\"0_specificity\"] = tn/(tn+fp) if tn+fp!=0 else tn        \n",
    "        \n",
    "        if \"fbeta\":\n",
    "            if \"beta_fbeta\" in kwargs.keys():\n",
    "                beta_fbeta = kwargs[\"beta_fbeta\"]\n",
    "            else:\n",
    "                beta_fbeta = 2\n",
    "            res[f\"0_f{beta_fbeta}_score\"] = metrics.fbeta_score(lst_true_label, lst_pred_label,pos_label=pos_label, beta=beta_fbeta)\n",
    "    # 1. Rand Score\n",
    "    if 1 in eval_method:\n",
    "        res[\"1_rand score\"] = metrics.rand_score(lst_true_label, lst_pred_label)\n",
    "        res[\"1_adjusted_rand_score\"] = metrics.adjusted_rand_score(lst_true_label, lst_pred_label)\n",
    "    # 2. Mutual Information based scores\n",
    "    if 2 in eval_method:\n",
    "        res[\"2_NMI score\"] = metrics.normalized_mutual_info_score(lst_true_label, lst_pred_label)\n",
    "        res[\"2_AMI score\"] = metrics.adjusted_mutual_info_score(lst_true_label, lst_pred_label)\n",
    "    # 3. Homogeneity, completeness and V-measure\n",
    "    if 3 in eval_method:\n",
    "        res[\"3_homogeneity_score\"] = metrics.homogeneity_score(lst_true_label, lst_pred_label)\n",
    "        res[\"3_completeness_score\"] = metrics.completeness_score(lst_true_label, lst_pred_label)\n",
    "        res[\"3_v_measure_score\"] = metrics.v_measure_score(lst_true_label, lst_pred_label, beta=1.0)\n",
    "    # 4. Fowlkes-Mallows scores\n",
    "    if 4 in eval_method:\n",
    "        res[\"4_fowlkes_mallows_score\"] = metrics.fowlkes_mallows_score(lst_true_label, lst_pred_label)\n",
    "    # 5. Silhouette Coefficient\n",
    "    if 5 in eval_method and encoding is not None :\n",
    "        try:\n",
    "            if affinity is not None:\n",
    "                res[\"5_silhouette_score\"] = metrics.silhouette_score(encoding, lst_pred_label, metric=affinity)\n",
    "            else:\n",
    "                res[\"5_silhouette_score\"] = metrics.silhouette_score(encoding, lst_pred_label)\n",
    "        except:\n",
    "            res[\"5_silhouette_score\"] = None\n",
    "    # 6. Calinski-Harabasz Index\n",
    "    if 6 in eval_method and encoding is not None:\n",
    "        try:\n",
    "            res[\"6_calinski_harabasz_score\"] = metrics.calinski_harabasz_score(encoding, lst_pred_label)\n",
    "        except:\n",
    "            res[\"6_calinski_harabasz_score\"] = None\n",
    "    # 7. Davies-Bouldin Index\n",
    "    if 7 in eval_method and encoding is not None:\n",
    "        try:\n",
    "            res[\"7_davies_bouldin_score\"] = metrics.davies_bouldin_score(encoding, lst_pred_label)\n",
    "        except:\n",
    "            res[\"7_davies_bouldin_score\"] = None\n",
    "    # 8. Contingency Matrix\n",
    "    if 8 in eval_method:\n",
    "        res[\"8_Contingency Matrix\"] = metrics.cluster.contingency_matrix(lst_true_label, lst_pred_label)\n",
    "    # 9. Pair Confusion Matrix\n",
    "    if 9 in eval_method:\n",
    "        res[\"9_pair_confusion_matrix\"] = metrics.cluster.pair_confusion_matrix(lst_true_label, lst_pred_label)\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0_accuracy': 0.9377431906614786,\n",
       " '0_precision': 0.7692307692307693,\n",
       " '0_sensitivity': 0.43478260869565216,\n",
       " '0_f1_score': 0.5555555555555555,\n",
       " '0_specificity': 0.9871794871794872,\n",
       " '0_f2_score': 0.47619047619047616}"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_mlp.fit(X_train, y_train)\n",
    "evaluate_clustering(y_test, search_mlp.predict(X_test), eval_method=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compared 3 kind of classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x26969a7a128>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#total = len(y_test)\n",
    "#class_1_count = np.sum(y_test)\n",
    "#class_0_count = total - class_1_count\n",
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "fpr_mlp, tpr_mlp, thresholds_mlp = roc_curve(y_test, predict_rdf)\n",
    "modelauc_mlp = roc_auc_score(y_test, y_pred)\n",
    "# mlp Model\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "# Perfect Model\n",
    "plt.plot(fpr_mlp, tpr_mlp, \n",
    "         c = 'r', \n",
    "         linewidth = 4, \n",
    "         label = 'MLP Model AUC:' + ' {0:.2f}'.format(modelauc_mlp))\n",
    "\n",
    "#th 75   \n",
    "modelauc_th75 = roc_auc_score(y_test, y_pred_quantile75)\n",
    "fpr_th75, tpr_th75, thresholds_th75 = roc_curve(y_test, y_pred_quantile75)    \n",
    "\n",
    "plt.plot(fpr_th75, tpr_th75, \n",
    "         c = 'b', \n",
    "         label = 'Threshold_Quantile.75 Model AUC:' + ' {0:.2f}'.format(modelauc_th75), \n",
    "         linewidth = 4)\n",
    "# ml Model\n",
    "svm_rbf.fit(X_train,y_train)\n",
    "y_pred_ml = svm_rbf.predict(X_test)\n",
    "probs = svm_rbf.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "fpr_0, tpr_0, thresholds_0 = roc_curve(y_test, probs)\n",
    "modelauc_ml = roc_auc_score(y_test, y_pred_ml)\n",
    "plt.plot(fpr_0, tpr_0, \n",
    "         c = 'g', \n",
    "         label = 'SVM_rbf Model AUC:' + ' {0:.2f}'.format(modelauc_ml), \n",
    "         linewidth = 4)          \n",
    "# Plot information\n",
    "plt.xlabel('False positive rate', fontsize = 20)\n",
    "plt.ylabel('True positive rate', fontsize = 20)\n",
    "plt.xlim(left=0,right=1)\n",
    "plt.ylim(bottom=0,top=1)\n",
    "#plt.title('Cumulative Accuracy Profile', fontsize = 16)\n",
    "plt.legend(loc = 'lower right', fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use DeLong to do the significant test between AUC of 3 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "\n",
    "# AUC comparison adapted from https://github.com/yandexdataschool/roc_comparison\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Parameters:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Parameters:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Parameters:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    p_val = np.log10(2) + norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "    \n",
    "    p_val = math.exp(p_val)\n",
    "    \n",
    "    return p_val\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight=None):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Parameters:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Parameters:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count, a = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack((predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return calc_pvalue(aucs, delongcov)\n",
    "\n",
    "def calc_auc_ci(y_true, y_pred, alpha=0.95):\n",
    "    auc, auc_cov = delong_roc_variance(y_true,y_pred)\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "    ci = norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "\n",
    "    ci[ci > 1] = 1\n",
    "    return ci\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.DeLong_Test as delong\n",
    "import ipynb.fs.full.Bootstrap_Test as bootstrap\n",
    "import numpy as np\n",
    "class Scoring(object):\n",
    "    def __init__(self, base_models):\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def delong_test(self, predict_df_all, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes p-value of DeLong Test with hypothesis that ROC AUCs of two classifiers are different\n",
    "        Parameters:\n",
    "            predict_df_all: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "            labels: test set; dataframe with dependent variable as column\n",
    "\n",
    "            return: \n",
    "                Test_df_sets: dataframe with results of paired test comparing ROC AUCs of different sets ('ALL' vs 'PDE' and 'ALL' vs 'PD') for all classifiers\n",
    "                Test_df_all: dataframe with results of paired test comparing ROC AUCs for all classifiers built based on set of all variables ('ALL')\n",
    "        \"\"\"\n",
    "\n",
    "        Test_df_sets = pd.DataFrame(np.zeros((2, len(self.base_models))), index=['ALL/PDE','ALL/PD'], columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        Test_df_sets.columns = pd.MultiIndex.from_product([['DeLong Test'], Test_df_sets.columns])\n",
    "        \n",
    "        Test_df_all = pd.DataFrame(list(combinations(Test_df_sets['DeLong Test'].columns,2)),columns = ['1st Algorithm', '2nd Algorithm'])\n",
    "        Test_df_all['score'] = 0\n",
    "        Test_df_all.columns = pd.MultiIndex.from_product([['DeLong Test'], Test_df_all.columns])    \n",
    "            \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "        \n",
    "            Test_df_sets['DeLong Test'].loc['ALL/PDE',str(clf).split('(')[0].lower()] = delong.delong_roc_test(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PDE'][str(clf).split('(')[0].lower()])\n",
    "            Test_df_sets['DeLong Test'].loc['ALL/PD',str(clf).split('(')[0].lower()] = delong.delong_roc_test(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PD'][str(clf).split('(')[0].lower()])\n",
    "        \n",
    "        for j in range(Test_df_all.shape[0]):\n",
    "            Test_df_all.loc[j, ('DeLong Test', 'score')]  = delong.delong_roc_test(labels.values.ravel(), predict_df_all['ALL'][Test_df_all.loc[j, ('DeLong Test', '1st Algorithm')]], predict_df_all['ALL'][Test_df_all.loc[j, ('DeLong Test', '2nd Algorithm')]])\n",
    "       \n",
    "        return Test_df_sets, Test_df_all\n",
    "    \n",
    "    def bootstrap_test(self, predict_df_all, labels):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes p-value of Bootstrap Test with hypothesis that ROC AUCs of two classifiers are different\n",
    "        Parameters:\n",
    "            predict_df_all: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "            labels: test set; dataframe with dependent variable as column\n",
    "\n",
    "            return: \n",
    "                Test_df_sets: dataframe with results of paired test comparing ROC AUCs of different sets ('ALL' vs 'PDE' and 'ALL' vs 'PD') for all classifiers\n",
    "                Test_df_all: dataframe with results of paired test comparing ROC AUCs for all classifiers built based on set of all variables ('ALL')\n",
    "        \"\"\"\n",
    "        \n",
    "        Test_df_sets = pd.DataFrame(np.zeros((2, len(self.base_models))), index=['ALL/PDE','ALL/PD'], columns=[str(i).split('(')[0].lower() for i in self.base_models])\n",
    "        Test_df_sets.columns = pd.MultiIndex.from_product([['Bootstrap Test'], Test_df_sets.columns])\n",
    "        \n",
    "        Test_df_all = pd.DataFrame(list(combinations(Test_df_sets['Bootstrap Test'].columns,2)),columns = ['1st Algorithm', '2nd Algorithm'])\n",
    "        Test_df_all['score'] = 0\n",
    "        Test_df_all.columns = pd.MultiIndex.from_product([['Bootstrap Test'], Test_df_all.columns])\n",
    "            \n",
    "        for i, clf in enumerate(self.base_models):\n",
    "        \n",
    "            Test_df_sets['Bootstrap Test'].loc['ALL/PDE',str(clf).split('(')[0].lower()] = bootstrap.pvalue(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PDE'][str(clf).split('(')[0].lower()], score_fun=roc_auc_score)\n",
    "            Test_df_sets['Bootstrap Test'].loc['ALL/PD',str(clf).split('(')[0].lower()] = bootstrap.pvalue(labels.values.ravel(), predict_df_all['ALL'][str(clf).split('(')[0].lower()], predict_df_all['PD'][str(clf).split('(')[0].lower()], score_fun=roc_auc_score)\n",
    "            \n",
    "        for j in range(Test_df_all.shape[0]):\n",
    "            Test_df_all.loc[j, ('Bootstrap Test', 'score')]  = bootstrap.pvalue(labels.values.ravel(), predict_df_all['ALL'][Test_df_all.loc[j, ('Bootstrap Test', '1st Algorithm')]], predict_df_all['ALL'][Test_df_all.loc[j, ('Bootstrap Test', '2nd Algorithm')]],score_fun=roc_auc_score)\n",
    "       \n",
    "        return Test_df_sets, Test_df_all\n",
    "    \n",
    "    def likelihood_RT(self, predict_df_all, estimators, x_train, y_test):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes p-value of Likelihood Ratio Test with hypothesis that ROC AUCs of Logistic Regression models built on different sets of variables are different\n",
    "        Parameters:\n",
    "            predict_df_all: dataframe with predictions of the classifiers for all three sets of predictors ('ALL', 'PDE', 'PD')\n",
    "            classifiers: list of best classifiers' instances fitted to the model\n",
    "            x_train: training set; dataframe with predictors as columns\n",
    "            labels: test set; dataframe with dependent variable as column\n",
    "\n",
    "            return: \n",
    "                Test_df_sets: dataframe with results of paired test comparing ROC AUCs of different sets ('ALL' vs 'PDE' and 'ALL' vs 'PD') for all classifiers\n",
    "        \"\"\"\n",
    "        \n",
    "        Test_df_sets = pd.DataFrame((np.zeros((2, 1))), index=['ALL/PDE','ALL/PD'], columns=[str(classifiers[2])])\n",
    "        Test_df_sets.columns = pd.MultiIndex.from_product([['LRT'], Test_df_sets.columns])\n",
    "\n",
    "        alt_log_likelihood = -log_loss(y_test,\n",
    "                                       predict_df_all['ALL'][str(classifiers[2])],\n",
    "                                       normalize=False)\n",
    "        null_log_likelihood = -log_loss(y_test,\n",
    "                                        predict_df_all['PDE'][str(classifiers[2])],\n",
    "                                        normalize=False)\n",
    "        G = 2 * (alt_log_likelihood - null_log_likelihood)\n",
    "        p_log_l = chi2.sf(G, x_train.shape[1])\n",
    "        \n",
    "        alt_log_likelihood = -log_loss(y_test,\n",
    "                                       predict_df_all['ALL'][str(classifiers[2])],\n",
    "                                       normalize=False)\n",
    "        null_log_likelihood = -log_loss(y_test,\n",
    "                                        predict_df_all['PD'][str(classifiers[2])],\n",
    "                                        normalize=False)\n",
    "        \n",
    "        G = 2 * (alt_log_likelihood - null_log_likelihood)\n",
    "        p_log_2 = chi2.sf(G, x_train.shape[1])\n",
    "        \n",
    "        Test_df_sets['LRT'].loc['ALL/PDE' ,str(classifiers[2])] = p_log_l\n",
    "        Test_df_sets['LRT'].loc['ALL/PD', str(classifiers[2])] = p_log_2\n",
    "\n",
    "        return Test_df_sets\n",
    "\n",
    "    \n",
    "def graph_ci_alternative(predict_df_all, y_test, base_models):\n",
    "\n",
    "    \"\"\"\n",
    "    Confidence Intervals for given set of predictors\n",
    "    Parameters:\n",
    "        predict_df_all: dataframe with predictions of the classifiers for all sets of predictors ('ALL', 'PDE', 'PD')\n",
    "        y_test: test set; dataframe with dependent variable as column\n",
    "        base_models: List with set of classifiers\n",
    "\n",
    "        return: Graph showing the Confidence Intervals of all classifiers for given set of predictors\n",
    "    \"\"\"\n",
    "    predict_df = predict_df_all.copy(deep=False)\n",
    "\n",
    "    result_table = pd.DataFrame(columns=['classifiers', 'delong','bootstrap'])\n",
    "    for i, (j, clf) in enumerate(predict_df):\n",
    "        yproba = predict_df[j][clf]\n",
    "\n",
    "        delong = delong.calc_auc_ci(y_test.values.ravel(),  yproba, alpha=0.95) \n",
    "        bootstrap = bootstrap.score_stat_ci(y_test.values.ravel(), yproba,  roc_auc_score)\n",
    "\n",
    "        result_table = result_table.append({'classifiers': [j,clf],\n",
    "                                            'delong':delong, \n",
    "                                            'bootstrap':bootstrap}, ignore_index=True)\n",
    "\n",
    "        result_table[['set', 'classifier']] = pd.DataFrame(result_table['classifiers'].tolist(), index=result_table.index)\n",
    "\n",
    "    for k,m in enumerate(result_table.set.unique()):\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "\n",
    "        SMALL_SIZE = 10\n",
    "        MEDIUM_SIZE = 12\n",
    "        BIGGER_SIZE = 14\n",
    "\n",
    "        plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "        plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "        plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "        plt.rc('xtick', labelsize=MEDIUM_SIZE)   # fontsize of the tick labels\n",
    "        plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "        plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "        plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "        x_ticks = [str(i).split('(')[0].lower() for i in base_models]\n",
    "\n",
    "        for n,l in enumerate(result_table.classifier.unique()):\n",
    "\n",
    "            eb_1 = plt.errorbar(x=n+1, \n",
    "                             y=(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][1] + result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][0])/2, \n",
    "                             yerr=[(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][1] - result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['bootstrap'].values[0][0])/2],\n",
    "                             fmt='ok',\n",
    "                             capsize = 10)\n",
    "\n",
    "            eb_2 = plt.errorbar(x=n+1.1, \n",
    "                             y=(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][1] + result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][0])/2, \n",
    "                             yerr=[(result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][1] - result_table.loc[(result_table.classifier == str(l)) & (result_table.set == str(m))]['delong'].values[0][0])/2],\n",
    "                             fmt='ok',\n",
    "                             capsize = 10)\n",
    "            eb_2[-1][0].set_linestyle('--')\n",
    "\n",
    "            # I need to manipulate 3rd parameter in arange, so the graph looks nice & I also need to do the same in errorbar(x)\n",
    "\n",
    "            plt.xticks(np.arange(1.05,len(x_ticks)+0.5,1), x_ticks, rotation=90)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.ylabel(\"ROC AUC Przedzia Ufnoci\", fontsize=15)\n",
    "            plt.tight_layout()\n",
    "\n",
    "        plt.savefig('plot'+str(m)+'ci.png', dpi=1200)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_prob = search.best_estimator_.predict_proba(X_test)[:,1].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12003852956721218"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delong_roc_test(np.array(y_test),y_pred_quantile5,y_pred_quantile75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23773628662316804"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delong_roc_test(np.array(y_test),SVM_probe,MLP_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006762222082778825"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delong_roc_test(np.array(y_test),MLP_probs,y_pred_quantile75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10981800355531782"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delong_roc_test(np.array(y_test),SVM_probe,y_pred_quantile75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screening for Potential Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_data(all_dataframe,clf):    \n",
    "    res=[]\n",
    "    M = np.empty(shape=[2, 2])\n",
    "    X=all_dataframe[features]\n",
    "    y=all_dataframe['class']\n",
    "\n",
    "    clf.fit(X,y)\n",
    "    y_pred=clf.predict(X)\n",
    "    print(y_pred)\n",
    "    index = np.arange(0,len(y))\n",
    "    bad_index=index[y != y_pred]\n",
    "    for i in bad_index:\n",
    "            bad=str(all_dataframe[\"class\"][i])+\"_\"+str(all_dataframe[\"ID\"][i])\n",
    "            res.append(bad)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_grid=SVC(C=451, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "   decision_function_shape=\"ovo\", degree=3, gamma=0.32222222222222224, kernel='rbf',\n",
    "   max_iter=-1, probability=False,\n",
    "   random_state=rand_state, shrinking=True,\n",
    "   tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1_2',\n",
       " '0_6',\n",
       " '0_30',\n",
       " '0_32',\n",
       " '0_34',\n",
       " '0_49',\n",
       " '0_62',\n",
       " '0_68',\n",
       " '1_75',\n",
       " '1_82',\n",
       " '0_85',\n",
       " '0_86',\n",
       " '0_126',\n",
       " '1_140',\n",
       " '1_151',\n",
       " '0_163',\n",
       " '0_235',\n",
       " '0_241',\n",
       " '0_274',\n",
       " '0_299',\n",
       " '0_314',\n",
       " '0_318',\n",
       " '0_329',\n",
       " '1_353',\n",
       " '0_356',\n",
       " '1_364',\n",
       " '1_374',\n",
       " '0_378',\n",
       " '0_382',\n",
       " '0_393',\n",
       " '0_417',\n",
       " '0_452',\n",
       " '0_458',\n",
       " '0_461',\n",
       " '0_463',\n",
       " '1_469',\n",
       " '0_493',\n",
       " '0_522',\n",
       " '0_561',\n",
       " '0_596',\n",
       " '1_608',\n",
       " '0_619',\n",
       " '1_634',\n",
       " '0_639',\n",
       " '0_641',\n",
       " '0_653',\n",
       " '0_681',\n",
       " '1_682',\n",
       " '0_683',\n",
       " '0_684',\n",
       " '0_688',\n",
       " '0_721',\n",
       " '0_726',\n",
       " '0_731',\n",
       " '0_738',\n",
       " '0_750',\n",
       " '0_762',\n",
       " '0_768',\n",
       " '0_776',\n",
       " '0_781',\n",
       " '0_812',\n",
       " '0_815',\n",
       " '0_846',\n",
       " '0_847',\n",
       " '0_848',\n",
       " '0_858',\n",
       " '1_885',\n",
       " '0_896',\n",
       " '0_901',\n",
       " '0_909',\n",
       " '0_913',\n",
       " '0_926',\n",
       " '0_951',\n",
       " '0_962',\n",
       " '0_965',\n",
       " '1_970',\n",
       " '0_977',\n",
       " '0_989',\n",
       " '1_992']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bad_data(df, svc_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_2',\n",
       " '0_6',\n",
       " '0_7',\n",
       " '0_24',\n",
       " '0_25',\n",
       " '0_28',\n",
       " '0_30',\n",
       " '0_32',\n",
       " '0_34',\n",
       " '0_38',\n",
       " '0_48',\n",
       " '0_49',\n",
       " '0_62',\n",
       " '0_68',\n",
       " '0_74',\n",
       " '1_75',\n",
       " '0_78',\n",
       " '0_80',\n",
       " '0_84',\n",
       " '0_85',\n",
       " '0_86',\n",
       " '0_114',\n",
       " '0_116',\n",
       " '0_126',\n",
       " '0_129',\n",
       " '1_140',\n",
       " '0_141',\n",
       " '0_144',\n",
       " '1_151',\n",
       " '0_160',\n",
       " '0_163',\n",
       " '0_166',\n",
       " '0_175',\n",
       " '0_203',\n",
       " '0_219',\n",
       " '0_221',\n",
       " '0_231',\n",
       " '0_235',\n",
       " '0_238',\n",
       " '0_241',\n",
       " '0_274',\n",
       " '0_288',\n",
       " '0_299',\n",
       " '0_303',\n",
       " '0_314',\n",
       " '0_318',\n",
       " '0_329',\n",
       " '0_336',\n",
       " '1_353',\n",
       " '0_354',\n",
       " '0_356',\n",
       " '0_358',\n",
       " '1_364',\n",
       " '0_371',\n",
       " '0_372',\n",
       " '0_378',\n",
       " '0_379',\n",
       " '0_382',\n",
       " '0_391',\n",
       " '0_393',\n",
       " '0_400',\n",
       " '0_402',\n",
       " '0_417',\n",
       " '0_452',\n",
       " '0_458',\n",
       " '0_461',\n",
       " '0_462',\n",
       " '0_463',\n",
       " '1_469',\n",
       " '0_474',\n",
       " '0_485',\n",
       " '0_489',\n",
       " '0_493',\n",
       " '0_494',\n",
       " '0_502',\n",
       " '0_503',\n",
       " '0_516',\n",
       " '0_522',\n",
       " '0_541',\n",
       " '0_555',\n",
       " '0_557',\n",
       " '0_561',\n",
       " '0_566',\n",
       " '0_574',\n",
       " '0_582',\n",
       " '0_592',\n",
       " '0_596',\n",
       " '0_602',\n",
       " '1_608',\n",
       " '0_612',\n",
       " '0_619',\n",
       " '0_630',\n",
       " '1_634',\n",
       " '0_639',\n",
       " '0_641',\n",
       " '0_648',\n",
       " '0_652',\n",
       " '0_653',\n",
       " '0_656',\n",
       " '0_675',\n",
       " '0_679',\n",
       " '0_681',\n",
       " '1_682',\n",
       " '0_683',\n",
       " '0_684',\n",
       " '0_688',\n",
       " '0_692',\n",
       " '0_698',\n",
       " '0_712',\n",
       " '0_721',\n",
       " '0_726',\n",
       " '0_731',\n",
       " '0_738',\n",
       " '0_750',\n",
       " '0_762',\n",
       " '0_765',\n",
       " '0_766',\n",
       " '0_767',\n",
       " '0_768',\n",
       " '0_776',\n",
       " '0_781',\n",
       " '0_785',\n",
       " '0_802',\n",
       " '0_812',\n",
       " '0_815',\n",
       " '0_826',\n",
       " '0_832',\n",
       " '0_846',\n",
       " '0_847',\n",
       " '0_848',\n",
       " '0_853',\n",
       " '0_855',\n",
       " '0_858',\n",
       " '0_863',\n",
       " '0_868',\n",
       " '0_873',\n",
       " '0_887',\n",
       " '0_895',\n",
       " '0_896',\n",
       " '0_897',\n",
       " '0_901',\n",
       " '0_908',\n",
       " '0_909',\n",
       " '0_911',\n",
       " '0_913',\n",
       " '0_914',\n",
       " '0_918',\n",
       " '0_921',\n",
       " '0_923',\n",
       " '0_926',\n",
       " '0_945',\n",
       " '0_951',\n",
       " '0_955',\n",
       " '0_962',\n",
       " '0_965',\n",
       " '1_970',\n",
       " '0_972',\n",
       " '0_977',\n",
       " '0_989',\n",
       " '1_992',\n",
       " '0_994',\n",
       " '0_997']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X=df[features]\n",
    "y=df['class']\n",
    "\n",
    "res=[]\n",
    "threshold_75=X['score_max'].quantile(q=.75)\n",
    "y_pred_quantile75 = np.where(X['score_max']>threshold_75, 1, 0)\n",
    "\n",
    "y_pred=y_pred_quantile75\n",
    "index = np.arange(0,len(y))\n",
    "bad_index=index[y != y_pred]\n",
    "for i in bad_index:\n",
    "        bad=str(df[\"class\"][i])+\"_\"+str(df[\"ID\"][i])\n",
    "        res.append(bad)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
